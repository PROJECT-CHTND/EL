"""Eager Learner (EL) - LLM-native interview agent with knowledge graph integration."""

from __future__ import annotations

import logging
import uuid
from typing import Any

from el_core.llm.client import LLMClient
from el_core.prompts import get_system_prompt
from el_core.schemas import (
    AgentResponse,
    ConsistencyIssue,
    ConsistencyIssueKind,
    ConversationTurn,
    DateType,
    DocumentChunk,
    DocumentExtractionResult,
    Domain,
    ExtractedFact,
    Insight,
    KnowledgeItem,
    Session,
    SessionSummary,
    Tag,
    TagSuggestion,
    TagSuggestionResult,
    TopicSummary,
)
from el_core.stores.kg_store import KnowledgeGraphStore
from el_core.tools import ALL_TOOLS, ToolExecutor

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """Detect if text is Japanese or English.

    Args:
        text: Text to analyze.

    Returns:
        "Japanese" or "English".
    """
    # Check for Japanese characters
    for char in text:
        if (
            "\u3040" <= char <= "\u309f"  # Hiragana
            or "\u30a0" <= char <= "\u30ff"  # Katakana
            or "\u4e00" <= char <= "\u9faf"  # Kanji
        ):
            return "Japanese"
    return "English"


def extract_dates_from_text(text: str) -> tuple[Any, Any]:
    """Extract date references from text for search filtering.

    Supports:
    - Japanese relative dates: ‰ªäÊó•, Êò®Êó•, ‰∏ÄÊò®Êó•, ÂÖàÈÄ±, ‰ªäÈÄ±, etc.
    - English relative dates: today, yesterday, last week, etc.
    - Japanese date formats: 2024Âπ¥5Êúà1Êó•, 5Êúà1Êó•, 5/1, 5Êúà
    - English date formats: May 1, 2024, May 1st, May 2024
    - ISO format: 2024-05-01

    Args:
        text: Text to extract dates from.

    Returns:
        Tuple of (start_date, end_date) as datetime objects, or (None, None) if no dates found.
    """
    import re
    from datetime import datetime, timedelta

    now = datetime.now()
    current_year = now.year
    today = now.replace(hour=0, minute=0, second=0, microsecond=0)

    # ===== RELATIVE DATES (Japanese) =====
    # Check for relative date references first (highest priority)
    text_lower = text.lower()
    
    # ‰∏ÄÊò®Êó• (day before yesterday)
    if "‰∏ÄÊò®Êó•" in text or "„Åä„Å®„Å®„ÅÑ" in text:
        target = today - timedelta(days=2)
        return target, target
    
    # Êò®Êó• (yesterday)
    if "Êò®Êó•" in text or "„Åç„ÅÆ„ÅÜ" in text:
        target = today - timedelta(days=1)
        return target, target
    
    # ‰ªäÊó• (today)
    if "‰ªäÊó•" in text or "„Åç„Çá„ÅÜ" in text or "Êú¨Êó•" in text:
        return today, today
    
    # ÊòéÊó• (tomorrow)
    if "ÊòéÊó•" in text or "„ÅÇ„Åó„Åü" in text or "„ÅÇ„Åô" in text:
        target = today + timedelta(days=1)
        return target, target
    
    # ÊòéÂæåÊó• (day after tomorrow)
    if "ÊòéÂæåÊó•" in text or "„ÅÇ„Åï„Å£„Å¶" in text:
        target = today + timedelta(days=2)
        return target, target
    
    # ÂÖàÈÄ± (last week)
    if "ÂÖàÈÄ±" in text:
        # Last week: Monday to Sunday of the previous week
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    # ‰ªäÈÄ± (this week)
    if "‰ªäÈÄ±" in text:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    # ÂÖàÊúà (last month)
    if "ÂÖàÊúà" in text:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # ‰ªäÊúà (this month)
    if "‰ªäÊúà" in text:
        first_of_this_month = today.replace(day=1)
        if today.month == 12:
            last_of_this_month = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
        else:
            last_of_this_month = today.replace(month=today.month + 1, day=1) - timedelta(days=1)
        return first_of_this_month, last_of_this_month
    
    # NÊó•Ââç (N days ago)
    n_days_ago_match = re.search(r"(\d+)Êó•Ââç", text)
    if n_days_ago_match:
        days = int(n_days_ago_match.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # NÈÄ±ÈñìÂâç (N weeks ago)
    n_weeks_ago_match = re.search(r"(\d+)ÈÄ±ÈñìÂâç", text)
    if n_weeks_ago_match:
        weeks = int(n_weeks_ago_match.group(1))
        target = today - timedelta(weeks=weeks)
        return target, target
    
    # ===== RELATIVE DATES (English) =====
    if "day before yesterday" in text_lower:
        target = today - timedelta(days=2)
        return target, target
    
    if "yesterday" in text_lower:
        target = today - timedelta(days=1)
        return target, target
    
    if "today" in text_lower:
        return today, today
    
    if "tomorrow" in text_lower:
        target = today + timedelta(days=1)
        return target, target
    
    if "day after tomorrow" in text_lower:
        target = today + timedelta(days=2)
        return target, target
    
    if "last week" in text_lower:
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    if "this week" in text_lower:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    if "last month" in text_lower:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # N days ago
    n_days_ago_en = re.search(r"(\d+)\s*days?\s*ago", text_lower)
    if n_days_ago_en:
        days = int(n_days_ago_en.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # ===== ABSOLUTE DATES =====
    # Japanese date patterns
    jp_full_date = re.search(r"(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•", text)
    jp_month_day = re.search(r"(\d{1,2})Êúà(\d{1,2})Êó•", text)
    jp_month_only = re.search(r"(\d{1,2})Êúà(?!Êó•)", text)

    # English date patterns
    en_full_date = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?,?\s*(\d{4})",
        text,
        re.IGNORECASE,
    )
    en_month_day = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?",
        text,
        re.IGNORECASE,
    )
    en_month_only = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*(\d{4})?",
        text,
        re.IGNORECASE,
    )

    # ISO format
    iso_date = re.search(r"(\d{4})-(\d{2})-(\d{2})", text)

    month_map = {
        "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
        "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12,
    }

    try:
        # Try Japanese full date first
        if jp_full_date:
            year, month, day = int(jp_full_date.group(1)), int(jp_full_date.group(2)), int(jp_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # ISO format
        if iso_date:
            year, month, day = int(iso_date.group(1)), int(iso_date.group(2)), int(iso_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # English full date
        if en_full_date:
            month = month_map[en_full_date.group(1).lower()[:3]]
            day = int(en_full_date.group(2))
            year = int(en_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # Japanese month + day
        if jp_month_day:
            month, day = int(jp_month_day.group(1)), int(jp_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # English month + day
        if en_month_day:
            month = month_map[en_month_day.group(1).lower()[:3]]
            day = int(en_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # Japanese month only - return range for the whole month
        if jp_month_only:
            month = int(jp_month_only.group(1))
            start_date = datetime(current_year, month, 1)
            # End of month
            if month == 12:
                end_date = datetime(current_year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(current_year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

        # English month only
        if en_month_only:
            month = month_map[en_month_only.group(1).lower()[:3]]
            year = int(en_month_only.group(2)) if en_month_only.group(2) else current_year
            start_date = datetime(year, month, 1)
            if month == 12:
                end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

    except (ValueError, AttributeError):
        pass

    return None, None


class ELAgent:
    """Eager Learner (EL) - A curious and empathetic interviewer powered by LLM.

    This agent uses GPT-5.2 with function calling to:
    - Search knowledge graph for relevant context
    - Save important insights from conversations
    - Dynamically detect conversation domains
    - Generate empathetic, insightful questions
    """

    def __init__(
        self,
        llm_client: LLMClient | None = None,
        kg_store: KnowledgeGraphStore | None = None,
    ) -> None:
        """Initialize the Eager Learner agent.

        Args:
            llm_client: LLM client for chat completions. Creates default if None.
            kg_store: Knowledge graph store. Tools will work in local-only mode if None.
        """
        self._llm = llm_client or LLMClient()
        self._kg_store = kg_store
        self._sessions: dict[str, Session] = {}

    async def start_session(
        self,
        user_id: str,
        topic: str,
        language: str | None = None,
    ) -> tuple[str, str]:
        """Start a new conversation session.

        Args:
            user_id: User identifier.
            topic: Conversation topic.
            language: Language preference. Auto-detected if None.

        Returns:
            Tuple of (session_id, opening_message).
        """
        detected_lang = language or detect_language(topic)
        session_id = str(uuid.uuid4())

        session = Session(
            id=session_id,
            user_id=user_id,
            topic=topic,
            language=detected_lang,
        )
        
        # Pre-fetch related knowledge from KG
        if self._kg_store:
            try:
                related_items = await self._kg_store.search(topic, limit=5)
                if related_items:
                    session.prior_knowledge = related_items
                    session.prior_context = self._format_prior_knowledge(related_items, detected_lang)
                    logger.info(f"Found {len(related_items)} related knowledge items for topic: {topic}")
            except Exception as e:
                logger.warning(f"Failed to pre-fetch knowledge: {e}")
        
        self._sessions[session_id] = session

        # Save session metadata to Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to save session metadata: {e}")

        # Generate natural opening response using LLM
        opening = await self._generate_opening_response(session, topic, detected_lang)
        logger.info(f"Started session {session_id} for user {user_id} on topic: {topic}")

        return session_id, opening
    
    async def _save_session_metadata(self, session: Session) -> None:
        """Save session metadata to Neo4j.
        
        Args:
            session: Session to save metadata for.
        """
        if self._kg_store is None:
            return
        
        await self._kg_store.save_session_metadata(
            session_id=session.id,
            user_id=session.user_id,
            topic=session.topic,
            domain=session.domain,
            turn_count=len(session.turns),
            insights_count=session.insights_count,
            created_at=session.created_at,
            updated_at=session.updated_at,
        )
    
    async def _generate_opening_response(
        self,
        session: Session,
        user_input: str,
        language: str,
    ) -> str:
        """Generate a natural opening response using LLM.
        
        Instead of using a template, we let the LLM respond naturally to
        whatever the user typed - whether it's a topic, a request, or a question.
        
        Args:
            session: The session object.
            user_input: What the user typed to start the conversation.
            language: Detected language.
            
        Returns:
            Natural opening response from the LLM.
        """
        # Use a simplified prompt for opening - no tool instructions
        if language.lower() in ("english", "en"):
            system_content = (
                "You are EL, a curious and empathetic interviewer. "
                "Your role is to deeply understand what people share and help them discover new insights.\n\n"
                "Guidelines:\n"
                "- Show genuine interest and empathy\n"
                "- Keep responses conversational and warm\n"
                "- Ask 1-2 open-ended questions to start the dialogue\n"
                "- Be concise but engaging\n"
                "- Do NOT output any JSON, tool calls, or code\n"
            )
        else:
            system_content = (
                "„ÅÇ„Å™„Åü„ÅØ„ÄåEL„Äç„Åß„Åô„ÄÇÂ•ΩÂ•áÂøÉÊó∫Áõõ„ÅßÂÖ±ÊÑüÊÄß„ÅÆÈ´ò„ÅÑ„Ç§„É≥„Çø„Éì„É•„ÉØ„Éº„Å®„Åó„Å¶„ÄÅ"
                "Áõ∏Êâã„ÅÆË©±„ÇíÊ∑±„ÅèÁêÜËß£„Åó„ÄÅÊñ∞„Åü„Å™Ê∞ó„Å•„Åç„ÇíÂºï„ÅçÂá∫„ÅôÂØæË©±„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ\n\n"
                "„Ç¨„Ç§„Éâ„É©„Ç§„É≥Ôºö\n"
                "- ÂÖ±ÊÑü„Å®Èñ¢ÂøÉ„ÇíÊåÅ„Å£„Å¶ÂøúÁ≠î„Åô„Çã\n"
                "- Ëá™ÁÑ∂„ÅßÊ∏©„Åã„ÅÑ‰ºöË©±Ë™ø„ÇíÂøÉ„Åå„Åë„Çã\n"
                "- ‰ºöË©±„ÇíÂßã„ÇÅ„Çã„Åü„ÇÅ„Å´1„Äú2ÂÄã„ÅÆË≥™Âïè„Çí„Åô„Çã\n"
                "- Á∞°ÊΩî„Å†„Åë„Å©È≠ÖÂäõÁöÑ„Å´\n"
                "- JSON„ÇÑ„ÉÑ„Éº„É´„Ç≥„Éº„É´„ÄÅ„Ç≥„Éº„Éâ„ÅØÁµ∂ÂØæ„Å´Âá∫Âäõ„Åó„Å™„ÅÑ\n"
            )
        
        # Add prior context if available
        if session.prior_context:
            system_content += session.prior_context
        
        # Add instruction for first response
        if language.lower() in ("english", "en"):
            system_content += (
                "\n\n### First Response Instructions\n"
                "This is the start of a new conversation. The user has just shared "
                "what they want to discuss. Respond naturally and warmly - acknowledge "
                "their input, show interest, and ask a relevant follow-up question to "
                "get the conversation started. Keep it conversational and friendly. "
                "Output ONLY natural conversational text."
            )
        else:
            system_content += (
                "\n\n### ÊúÄÂàù„ÅÆÂøúÁ≠î„Å´Èñ¢„Åô„ÇãÊåáÁ§∫\n"
                "„Åì„Çå„ÅØÊñ∞„Åó„ÅÑ‰ºöË©±„ÅÆÈñãÂßã„Åß„Åô„ÄÇ„É¶„Éº„Ç∂„Éº„ÅåË©±„Åó„Åü„ÅÑ„Åì„Å®„Çí‰ºù„Åà„Å¶„Åç„Åæ„Åó„Åü„ÄÇ"
                "Ëá™ÁÑ∂„ÅßÊ∏©„Åã„ÅèÂøúÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„É¶„Éº„Ç∂„Éº„ÅÆÂÖ•Âäõ„ÇíÂèó„ÅëÊ≠¢„ÇÅ„ÄÅÈñ¢ÂøÉ„ÇíÁ§∫„Åó„ÄÅ"
                "‰ºöË©±„ÇíÂßã„ÇÅ„Çã„Åü„ÇÅ„ÅÆÈÅ©Âàá„Å™„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„Éó„ÅÆË≥™Âïè„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                "‰ºöË©±Ë™ø„Åß„Éï„É¨„É≥„Éâ„É™„Éº„Å´„ÄÅ„Åã„Å§Á∞°ÊΩî„Å´ÂøúÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                "Ëá™ÁÑ∂„Å™‰ºöË©±„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            )
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input},
        ]
        
        try:
            # Simple chat completion without tools for the opening
            response = await self._llm.chat(messages=messages)  # type: ignore
            return response.content or ""
        except Exception as e:
            logger.warning(f"Failed to generate opening response via LLM: {e}")
            # Fallback to a simple acknowledgment
            if language.lower() in ("english", "en"):
                return "I'd be happy to discuss that with you. What would you like to start with?"
            return "ÊâøÁü•„Åó„Åæ„Åó„Åü„ÄÇ„Å©„ÅÆ„Çà„ÅÜ„Å™„Åì„Å®„Åã„Çâ„ÅäË©±„Åó„Åæ„Åó„Çá„ÅÜ„ÅãÔºü"
    
    def _format_prior_knowledge(self, items: list[KnowledgeItem], language: str) -> str:
        """Format knowledge items into context string.
        
        Args:
            items: List of knowledge items.
            language: Language for formatting.
            
        Returns:
            Formatted context string.
        """
        if not items:
            return ""
        
        if language.lower() in ("english", "en"):
            header = "\n\n### Related Past Knowledge\n\nThe following are relevant insights from previous conversations:\n\n"
            item_format = "- {subject} {predicate}: {object} (recorded on {date})\n"
        else:
            header = "\n\n### Èñ¢ÈÄ£„Åô„ÇãÈÅéÂéª„ÅÆÁü•Ë≠ò\n\n‰ª•‰∏ã„ÅØÈÅéÂéª„ÅÆÂØæË©±„ÅßÂæó„Çâ„Çå„ÅüÈñ¢ÈÄ£ÊÉÖÂ†±„Åß„Åô„ÄÇ„Åì„Çå„Çâ„ÇíË∏è„Åæ„Åà„Å¶‰ºöË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö\n\n"
            item_format = "- {subject}„ÅØ{predicate}Ôºö{object}Ôºà{date}„Å´Ë®òÈå≤Ôºâ\n"
        
        context = header
        for item in items:
            date_str = item.created_at.strftime("%YÂπ¥%mÊúà%dÊó•") if language.lower() not in ("english", "en") else item.created_at.strftime("%Y-%m-%d")
            context += item_format.format(
                subject=item.subject,
                predicate=item.predicate,
                object=item.object,
                date=date_str,
            )
        
        return context

    async def resume_session(
        self,
        session_id: str,
        user_id: str,
    ) -> tuple[str, str, list[KnowledgeItem]]:
        """Resume an existing session from Neo4j.

        Args:
            session_id: Session ID to resume.
            user_id: User ID (for verification).

        Returns:
            Tuple of (session_id, resume_message, prior_insights).

        Raises:
            ValueError: If session not found or user mismatch.
        """
        # First check if session is already in memory (with conversation history)
        existing_session = self._sessions.get(session_id)
        if existing_session is not None and existing_session.user_id == user_id:
            # Session is already loaded with conversation history
            logger.info(f"Resuming session {session_id} from memory ({len(existing_session.turns)} turns)")
            
            # Generate resume message
            lang = existing_session.language
            if lang.lower() in ("english", "en"):
                resume_msg = (
                    f"Welcome back! We were discussing \"{existing_session.topic}\". "
                    f"You have {len(existing_session.turns)} exchanges so far. "
                    f"Let's continue!"
                )
            else:
                resume_msg = (
                    f"„Äå{existing_session.topic}„Äç„ÅÆÁ∂ö„Åç„Åß„Åô„Å≠„ÄÇ\n"
                    f"„Åì„Çå„Åæ„Åß{len(existing_session.turns)}„Çø„Éº„É≥„ÅÆ‰ºöË©±„Åå„ÅÇ„Çä„Åæ„Åó„Åü„ÄÇ\n"
                    f"Á∂ö„Åë„Åæ„Åó„Çá„ÅÜÔºÅ"
                )
            
            return session_id, resume_msg, existing_session.prior_knowledge
        
        # Session not in memory, load from Neo4j
        if self._kg_store is None:
            raise ValueError("Knowledge graph store not available")

        # Get session metadata from Neo4j
        metadata = await self._kg_store.get_session_metadata(session_id)
        if metadata is None:
            raise ValueError(f"Session {session_id} not found")
        
        if metadata.user_id != user_id:
            raise ValueError("User ID mismatch")

        # Get insights saved during this session
        session_insights = await self._kg_store.get_session_insights(session_id)

        # Detect language from topic
        detected_lang = detect_language(metadata.topic)

        # Create new session object with prior context
        session = Session(
            id=metadata.id,
            user_id=metadata.user_id,
            topic=metadata.topic,
            language=detected_lang,
            domain=metadata.domain,
            created_at=metadata.created_at,
            updated_at=metadata.updated_at,
        )

        # Set prior context from session insights
        if session_insights:
            session.prior_knowledge = session_insights
            session.prior_context = self._format_prior_knowledge(session_insights, detected_lang)

        # Load conversation history from Neo4j
        conversation_turns = await self._kg_store.get_conversation_history(session_id)
        for turn_data in conversation_turns:
            # Restore turn to session
            turn = ConversationTurn(
                user_message=turn_data["user_message"],
                assistant_response=turn_data["assistant_response"],
                timestamp=turn_data["timestamp"],
            )
            session.turns.append(turn)
            # Also restore to message_history for LLM context
            session.message_history.append({"role": "user", "content": turn_data["user_message"]})
            session.message_history.append({"role": "assistant", "content": turn_data["assistant_response"]})

        # Store in active sessions
        self._sessions[session_id] = session

        # Generate resume message
        has_history = len(conversation_turns) > 0
        if detected_lang.lower() in ("english", "en"):
            if has_history:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"Your previous {len(conversation_turns)} exchanges have been restored. "
                    f"Let's continue!"
                )
            else:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"(Note: No conversation history found. Starting fresh but with your insights!)"
                )
        else:
            if has_history:
                resume_msg = (
                    f"„Äå{metadata.topic}„Äç„ÅÆÁ∂ö„Åç„Åß„Åô„Å≠„ÄÇ„ÅäÂ∏∞„Çä„Å™„Åï„ÅÑÔºÅ\n"
                    f"ÂâçÂõû„ÅÆ{len(conversation_turns)}„Çø„Éº„É≥„ÅÆ‰ºöË©±„ÇíÂæ©ÂÖÉ„Åó„Åæ„Åó„Åü„ÄÇ\n"
                    f"Á∂ö„Åë„Åæ„Åó„Çá„ÅÜÔºÅ"
                )
            else:
                resume_msg = (
                    f"„Äå{metadata.topic}„Äç„ÅÆÁ∂ö„Åç„Åß„Åô„Å≠„ÄÇ„ÅäÂ∏∞„Çä„Å™„Åï„ÅÑÔºÅ\n"
                    f"Ôºà‚Äª‰ºöË©±Â±•Ê≠¥„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„Åå„ÄÅË®òÈå≤„Åó„Åü‰∫ãÂÆü„ÅØÂºï„ÅçÁ∂ô„ÅÑ„Åß„ÅÑ„Åæ„ÅôÔºâ"
                )

        logger.info(f"Resumed session {session_id} for user {user_id} ({len(conversation_turns)} turns restored)")

        return session_id, resume_msg, session_insights

    def get_session(self, session_id: str) -> Session | None:
        """Get a session by ID.

        Args:
            session_id: Session identifier.

        Returns:
            Session or None if not found.
        """
        return self._sessions.get(session_id)

    async def end_session(self, session_id: str, generate_summary: bool = True) -> tuple[Session | None, SessionSummary | None]:
        """End a session and optionally generate a summary.

        Args:
            session_id: Session identifier.
            generate_summary: Whether to generate and save a summary.

        Returns:
            Tuple of (ended session, generated summary or None).
        """
        session = self._sessions.pop(session_id, None)
        if session is None:
            return None, None

        summary = None
        if generate_summary and len(session.turns) > 0:
            # Generate summary
            summary = await self._generate_session_summary(session)
            
            # Save summary to KG
            if self._kg_store and summary:
                try:
                    await self._kg_store.save_session_summary(summary)
                    logger.info(f"Saved summary for session {session_id}")
                except Exception as e:
                    logger.warning(f"Failed to save session summary: {e}")

        # Update session status in KG
        if self._kg_store:
            try:
                await self._kg_store.update_session_status(session_id, "ended")
            except Exception as e:
                logger.warning(f"Failed to update session status: {e}")

        logger.info(f"Ended session {session_id}")
        return session, summary

    async def _generate_session_summary(self, session: Session) -> SessionSummary | None:
        """Generate a summary of the session using LLM.

        Args:
            session: The session to summarize.

        Returns:
            SessionSummary or None if generation fails.
        """
        if not session.turns:
            return None

        # Build conversation text
        conversation_text = ""
        for i, turn in enumerate(session.turns, 1):
            conversation_text += f"„Çø„Éº„É≥{i}:\n"
            conversation_text += f"„É¶„Éº„Ç∂„Éº: {turn.user_message}\n"
            conversation_text += f"„Ç¢„Ç∑„Çπ„Çø„É≥„Éà: {turn.assistant_response}\n\n"

        # Collect all insights from the session
        all_insights = []
        for turn in session.turns:
            for insight in turn.insights_saved:
                all_insights.append(f"{insight.subject} - {insight.predicate} - {insight.object}")

        insights_text = "\n".join(all_insights) if all_insights else "„Å™„Åó"

        if session.language.lower() in ("english", "en"):
            system_prompt = """You are a conversation summarizer. Summarize the following conversation.

Output ONLY valid JSON with these fields:
- content: 2-3 sentence natural summary
- key_points: Array of important facts (max 5)
- topics: Array of main topics discussed
- entities_mentioned: Array of people, projects, places mentioned

Example:
{"content":"Discussed project A progress and confirmed the deadline.","key_points":["Deadline is Feb 28","Tanaka is responsible"],"topics":["Project A","Progress report"],"entities_mentioned":["Tanaka","Project A"]}"""
        else:
            system_prompt = """„ÅÇ„Å™„Åü„ÅØ‰ºöË©±Ë¶ÅÁ¥ÑËÄÖ„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆ‰ºöË©±„ÇíË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

‰ª•‰∏ã„ÅÆ„Éï„Ç£„Éº„É´„Éâ„ÇíÊåÅ„Å§ÊúâÂäπ„Å™JSON„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- content: 2-3Êñá„ÅÆËá™ÁÑ∂„Å™Ë¶ÅÁ¥Ñ
- key_points: ÈáçË¶Å„Å™‰∫ãÂÆü„ÅÆÈÖçÂàóÔºàÊúÄÂ§ß5„Å§Ôºâ
- topics: Ë©±„ÅóÂêà„Çè„Çå„Åü‰∏ª„Å™„Éà„Éî„ÉÉ„ÇØ„ÅÆÈÖçÂàó
- entities_mentioned: Ë®ÄÂèä„Åï„Çå„Åü‰∫∫Âêç„ÄÅ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂêç„ÄÅÂ†¥ÊâÄ„Å™„Å©„ÅÆÈÖçÂàó

‰æã:
{"content":"„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA„ÅÆÈÄ≤Êçó„Å´„Å§„ÅÑ„Å¶Ë©±„ÅóÂêà„ÅÑ„ÄÅÁ∑†„ÇÅÂàá„Çä„ÇíÁ¢∫Ë™ç„Åó„Åü„ÄÇ","key_points":["Á∑†„ÇÅÂàá„Çä„ÅØ2Êúà28Êó•","Áî∞‰∏≠„Åï„Çì„ÅåÊãÖÂΩì"],"topics":["„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA","ÈÄ≤ÊçóÂ†±Âëä"],"entities_mentioned":["Áî∞‰∏≠„Åï„Çì","„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA"]}"""

        user_prompt = f"""„Éà„Éî„ÉÉ„ÇØ: {session.topic}
„Éâ„É°„Ç§„É≥: {session.domain.value}

„Äê‰ºöË©±„Äë
{conversation_text}

„ÄêÊäΩÂá∫„Åï„Çå„ÅüÊ¥ûÂØü„Äë
{insights_text}

‰∏äË®ò„ÅÆ‰ºöË©±„ÇíJSONÂΩ¢Âºè„ÅßË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "{}"
            
            # Clean up response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            data = json.loads(content)
            
            summary = SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=data.get("content", "‰ºöË©±„ÅÆË¶ÅÁ¥Ñ"),
                key_points=data.get("key_points", []),
                topics=data.get("topics", [session.topic]),
                entities_mentioned=data.get("entities_mentioned", []),
                turn_range=(1, len(session.turns)),
            )
            
            logger.info(f"Generated summary for session {session.id}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate session summary: {e}")
            # Return a basic summary on failure
            return SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=f"{session.topic}„Å´„Å§„ÅÑ„Å¶„ÅÆ‰ºöË©±Ôºà{len(session.turns)}„Çø„Éº„É≥Ôºâ",
                key_points=[],
                topics=[session.topic],
                entities_mentioned=[],
                turn_range=(1, len(session.turns)),
            )

    async def respond(
        self,
        session_id: str,
        user_message: str,
    ) -> AgentResponse:
        """Generate a response to a user message.

        Args:
            session_id: Session identifier.
            user_message: The user's message.

        Returns:
            AgentResponse with the assistant's message and metadata.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        # Pre-search knowledge graph for potential consistency issues
        pre_search_knowledge: list[KnowledgeItem] = []
        consistency_issues: list[ConsistencyIssue] = []
        consistency_context = ""
        chunk_context = ""  # Phase 2: Context from document chunks
        relevant_chunks: list[DocumentChunk] = []
        
        if self._kg_store:
            try:
                # Extract dates from user message for temporal filtering
                start_date, end_date = extract_dates_from_text(user_message)
                
                if start_date or end_date:
                    # User mentioned a date - search by date range first
                    logger.info(f"Detected date reference: {start_date} to {end_date}")
                    
                    # Phase 2: Search for document chunks by date (for accurate original content)
                    if start_date and end_date and start_date == end_date:
                        # Exact date match
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    elif start_date and end_date:
                        # Date range
                        relevant_chunks = await self._kg_store.get_chunks_by_date_range(start_date, end_date)
                    elif start_date:
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    
                    if relevant_chunks:
                        # Format chunk content as context for LLM
                        chunk_context = self._format_chunk_context(relevant_chunks, session.language)
                        logger.info(f"Found {len(relevant_chunks)} relevant chunks for date query")
                        # IMPORTANT: When we have chunk original content, skip fact search
                        # to avoid LLM mixing accurate chunk content with potentially inaccurate extracted facts
                        logger.info("Skipping fact search - using chunk original content as authoritative source")
                    else:
                        # No chunks found, fall back to extracted facts
                        date_filtered_knowledge = await self._kg_store.search_by_date_range(
                            start_date=start_date,
                            end_date=end_date,
                            query=user_message,
                            limit=5,
                        )
                        pre_search_knowledge.extend(date_filtered_knowledge)
                
                # Only do keyword search if we don't have chunk content
                # Chunk original content is the authoritative source
                if not relevant_chunks:
                    keyword_knowledge = await self._kg_store.search(
                        user_message,
                        limit=5,
                        start_date=start_date,
                        end_date=end_date,
                    )
                    
                    # Merge results, avoiding duplicates
                    seen_ids = {k.id for k in pre_search_knowledge}
                    for item in keyword_knowledge:
                        if item.id not in seen_ids:
                            pre_search_knowledge.append(item)
                            seen_ids.add(item.id)
                
                if pre_search_knowledge:
                    # Check for consistency issues BEFORE generating response
                    consistency_issues = await self._detect_consistency_issues(
                        user_message=user_message,
                        knowledge_used=pre_search_knowledge,
                        language=session.language,
                    )
                    
                    # If issues found, add context for LLM to address them
                    if consistency_issues:
                        consistency_context = self._format_consistency_context(
                            consistency_issues, session.language
                        )
                        logger.info(f"Found {len(consistency_issues)} consistency issues to address")
                        
            except Exception as e:
                logger.warning(f"Pre-search for consistency failed: {e}")

        # Build message history with prior context
        system_content = get_system_prompt(session.language)
        if session.prior_context:
            system_content += session.prior_context
        
        # Phase 2: Add chunk content as context (for accurate document reference)
        if chunk_context:
            system_content += chunk_context
        
        # Add consistency context if there are issues to address
        if consistency_context:
            system_content += consistency_context
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
        ]

        # Add conversation history
        messages.extend(session.message_history)

        # Add current user message
        messages.append({"role": "user", "content": user_message})

        # Set up tool executor with session_id for insight tracking
        tool_executor = ToolExecutor(self._kg_store, session_id=session_id)
        
        # Add pre-searched knowledge to tool executor so it's included in response
        tool_executor.used_knowledge.extend(pre_search_knowledge)

        # Call LLM with tools
        response_text, tool_results = await self._llm.chat_with_tools(
            messages=messages,  # type: ignore
            tools=ALL_TOOLS,
            tool_handlers=tool_executor.get_tool_handlers(),
        )

        # Detect domain from response and tool usage
        detected_domain = self._detect_domain(user_message, tool_results)

        # Create conversation turn
        turn = ConversationTurn(
            user_message=user_message,
            assistant_response=response_text,
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
        )
        session.add_turn(turn)

        # Update session metadata in Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to update session metadata: {e}")

        # Save conversation turn to Neo4j for persistence
        if self._kg_store:
            try:
                await self._kg_store.save_conversation_turn(
                    session_id=session_id,
                    turn_index=len(session.turns) - 1,
                    user_message=user_message,
                    assistant_response=response_text,
                    timestamp=turn.timestamp,
                )
            except Exception as e:
                logger.warning(f"Failed to save conversation turn: {e}")

        # Build tool calls list for response
        tool_calls = [
            {
                "id": tr.get("tool_call_id", ""),
                "name": tr.get("name", ""),
                "arguments": tr.get("arguments", {}),
            }
            for tr in tool_results
        ]

        logger.info(
            f"Session {session_id}: processed message, "
            f"saved {len(tool_executor.saved_insights)} insights, "
            f"used {len(tool_executor.used_knowledge)} knowledge items, "
            f"detected {len(consistency_issues)} consistency issues"
        )

        return AgentResponse(
            message=response_text,
            tool_calls=tool_calls,  # type: ignore
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
            consistency_issues=consistency_issues,
        )

    def _format_chunk_context(
        self,
        chunks: list[DocumentChunk],
        language: str,
    ) -> str:
        """Format document chunks as context for LLM.

        Phase 2: Provides the original document content for accurate reference.

        Args:
            chunks: List of relevant document chunks.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not chunks:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### üîí AUTHORITATIVE DOCUMENT CONTENT (Must Quote Exactly)\n"
            context += "The following is the EXACT original content from the user's uploaded documents.\n"
            context += "**CRITICAL RULES:**\n"
            context += "1. Quote EXACTLY what is written - do not paraphrase or summarize\n"
            context += "2. If document says '„Ç´„Ç®„É´ Ââç', say '„Ç´„Ç®„É´ Ââç' - NOT '„Ç´„Ç®„É´ÔºàÂâç/ÂæåÔºâ'\n"
            context += "3. Do NOT add information that is not in the document\n"
            context += "4. Do NOT guess or infer - only state what is explicitly written\n\n"
        else:
            context = "\n\n### üîí „Éâ„Ç≠„É•„É°„É≥„ÉàÂéüÊñáÔºàÊ≠£Á¢∫„Å´ÂºïÁî®„Åô„Çã„Åì„Å®Ôºâ\n"
            context += "‰ª•‰∏ã„ÅØ„É¶„Éº„Ç∂„Éº„Åå„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Åü„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ**ÂéüÊñá„Åù„ÅÆ„Åæ„Åæ**„Åß„Åô„ÄÇ\n"
            context += "**Áµ∂ÂØæ„Å´ÂÆà„Çã„Åπ„Åç„É´„Éº„É´Ôºö**\n"
            context += "1. Êõ∏„Åã„Çå„Å¶„ÅÑ„ÇãÂÜÖÂÆπ„Çí**‰∏ÄÂ≠ó‰∏ÄÂè•„Åù„ÅÆ„Åæ„Åæ**ÂºïÁî®„Åô„Çã„Åì„Å®\n"
            context += "2. ‰æãÔºö„Éâ„Ç≠„É•„É°„É≥„Éà„Å´„Äå„Ç´„Ç®„É´ Ââç„Äç„Å®„ÅÇ„Çå„Å∞„Äå„Ç´„Ç®„É´ Ââç„Äç„Å®ÂõûÁ≠î - „Äå„Ç´„Ç®„É´ÔºàÂâç/ÂæåÔºâ„Äç„ÅØ‚ùå\n"
            context += "3. „Éâ„Ç≠„É•„É°„É≥„Éà„Å´Êõ∏„Åã„Çå„Å¶„ÅÑ„Å™„ÅÑÊÉÖÂ†±„ÇíËøΩÂä†„Åó„Å™„ÅÑ„Åì„Å®\n"
            context += "4. Êé®Ê∏¨„ÇÑË£úÂÆå„ÅØÁµ∂ÂØæ„Å´„Åó„Å™„ÅÑ„Åì„Å® - ÊòéÁ§∫ÁöÑ„Å´Êõ∏„Åã„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÅÆ„ÅøÂõûÁ≠î\n\n"

        for chunk in chunks:
            # Add date header if available
            if chunk.chunk_date:
                date_str = chunk.chunk_date.strftime("%YÂπ¥%mÊúà%dÊó•") if language.lower() not in ("english", "en") else chunk.chunk_date.strftime("%Y-%m-%d")
                context += f"---\nüìÖ {date_str}"
                if chunk.heading:
                    context += f" - {chunk.heading}"
                context += "\n\n"
            elif chunk.heading:
                context += f"---\nüìÑ {chunk.heading}\n\n"
            else:
                context += "---\n\n"

            # Add the original content (preserved exactly as uploaded)
            context += "„ÄêÂéüÊñá„Åì„Åì„Åã„Çâ„Äë\n"
            context += chunk.content
            context += "\n„ÄêÂéüÊñá„Åì„Åì„Åæ„Åß„Äë\n\n"

        context += "---\n"
        context += "‰∏äË®ò„ÅÆÂéüÊñá„Åã„Çâ„ÄÅË≥™Âïè„Å´Ë©≤ÂΩì„Åô„ÇãÈÉ®ÂàÜ„Çí„Åù„ÅÆ„Åæ„ÅæÂºïÁî®„Åó„Å¶ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n"
        
        return context

    def _format_consistency_context(
        self,
        issues: list[ConsistencyIssue],
        language: str,
    ) -> str:
        """Format consistency issues as context for LLM.

        Args:
            issues: List of detected consistency issues.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not issues:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### IMPORTANT: Consistency Issues Detected\n"
            context += "The user's current message appears to contradict or change previous information.\n"
            context += "**You MUST address these in your response** - ask for clarification naturally.\n\n"
            
            for issue in issues:
                issue_type = "CONTRADICTION" if issue.kind.value == "contradiction" else "CHANGE"
                context += f"- [{issue_type}] {issue.title}\n"
                context += f"  Previously: \"{issue.previous_text}\"\n"
                context += f"  Now: \"{issue.current_text}\"\n"
                context += f"  Suggested question: \"{issue.suggested_question}\"\n\n"
            
            context += "Address this naturally by asking about the change/contradiction.\n"
        else:
            context = "\n\n### ÈáçË¶ÅÔºöÊï¥ÂêàÊÄß„ÅÆÂïèÈ°å„ÇíÊ§úÂá∫\n"
            context += "„É¶„Éº„Ç∂„Éº„ÅÆÁèæÂú®„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÅåÈÅéÂéª„ÅÆÊÉÖÂ†±„Å®ÁüõÁõæ„Åæ„Åü„ÅØÂ§âÊõ¥„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n"
            context += "**ÂøÖ„Åö„Åì„ÅÆÁÇπ„Å´„Å§„ÅÑ„Å¶Á¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ** - Ëá™ÁÑ∂„Å™ÂΩ¢„ÅßË≥™Âïè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"
            
            for issue in issues:
                issue_type = "ÁüõÁõæ" if issue.kind.value == "contradiction" else "Â§âÊõ¥"
                context += f"- „Äê{issue_type}„Äë{issue.title}\n"
                context += f"  ‰ª•ÂâçÔºö„Äå{issue.previous_text}„Äç\n"
                context += f"  ‰ªäÂõûÔºö„Äå{issue.current_text}„Äç\n"
                context += f"  Á¢∫Ë™ç„Åô„Åπ„ÅçË≥™ÂïèÔºö„Äå{issue.suggested_question}„Äç\n\n"
            
            context += "„Åì„ÅÆÂ§âÊõ¥/ÁüõÁõæ„Å´„Å§„ÅÑ„Å¶Ëá™ÁÑ∂„Å´Á¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n"

        return context

    def _detect_domain(
        self,
        user_message: str,
        tool_results: list[dict[str, Any]],
    ) -> Domain:
        """Detect the conversation domain.

        Args:
            user_message: The user's message.
            tool_results: Results from tool calls.

        Returns:
            Detected domain.
        """
        text_lower = user_message.lower()

        # Check for domain-specific keywords
        daily_work_keywords = [
            "„Çø„Çπ„ÇØ",
            "task",
            "„Éó„É≠„Ç∏„Çß„ÇØ„Éà",
            "project",
            "Ê•≠Âãô",
            "work",
            "„Éü„Éº„ÉÜ„Ç£„É≥„Ç∞",
            "meeting",
            "ÈÄ≤Êçó",
            "progress",
            "‰ªäÊó•",
            "today",
            "ÊòéÊó•",
            "tomorrow",
            "Á∑†„ÇÅÂàá„Çä",
            "deadline",
            "pr",
            "„Ç≥„Éº„Éâ„É¨„Éì„É•„Éº",
            "„Éá„Éó„É≠„Ç§",
            "deploy",
        ]

        recipe_keywords = [
            "ÊñôÁêÜ",
            "cook",
            "„É¨„Ç∑„Éî",
            "recipe",
            "ÊùêÊñô",
            "ingredient",
            "Ë™øÁêÜ",
            "ÁÑº„Åè",
            "ÁÖÆ„Çã",
            "ÁÇí„ÇÅ„Çã",
            "ÂàÜÈáè",
            "„Ç™„Éº„Éñ„É≥",
            "oven",
            "Èçã",
            "„Éï„É©„Ç§„Éë„É≥",
        ]

        postmortem_keywords = [
            "ÈöúÂÆ≥",
            "incident",
            "„Ç§„É≥„Ç∑„Éá„É≥„Éà",
            "„ÉÄ„Ç¶„É≥",
            "down",
            "Âæ©Êóß",
            "recover",
            "Ê†πÊú¨ÂéüÂõ†",
            "root cause",
            "„Çø„Ç§„É†„É©„Ç§„É≥",
            "timeline",
            "ÂÜçÁô∫Èò≤Ê≠¢",
            "„Ç¢„É©„Éº„Éà",
            "alert",
            "„Ç®„É©„Éº",
            "error",
        ]

        creative_keywords = [
            "„Ç¢„Ç§„Éá„Ç¢",
            "idea",
            "Ââµ‰Ωú",
            "creative",
            "„Éá„Ç∂„Ç§„É≥",
            "design",
            "„Ç§„É©„Çπ„Éà",
            "illustration",
            "Áâ©Ë™û",
            "story",
            "Èü≥Ê•Ω",
            "music",
            "„Ç§„É≥„Çπ„Éî„É¨„Éº„Ç∑„Éß„É≥",
            "inspiration",
        ]

        # Check save_insight tool calls for domain hints
        for result in tool_results:
            if result.get("name") == "save_insight":
                args = result.get("arguments", {})
                if "domain" in args:
                    try:
                        return Domain(args["domain"])
                    except ValueError:
                        pass

        # Check keywords
        if any(kw in text_lower for kw in daily_work_keywords):
            return Domain.DAILY_WORK
        if any(kw in text_lower for kw in recipe_keywords):
            return Domain.RECIPE
        if any(kw in text_lower for kw in postmortem_keywords):
            return Domain.POSTMORTEM
        if any(kw in text_lower for kw in creative_keywords):
            return Domain.CREATIVE

        return Domain.GENERAL

    async def get_session_summary(self, session_id: str) -> dict[str, Any]:
        """Get a summary of the session.

        Args:
            session_id: Session identifier.

        Returns:
            Dict with session summary.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        all_insights: list[Insight] = []
        all_knowledge: list[KnowledgeItem] = []

        for turn in session.turns:
            all_insights.extend(turn.insights_saved)
            all_knowledge.extend(turn.knowledge_used)

        return {
            "session_id": session.id,
            "topic": session.topic,
            "language": session.language,
            "domain": session.domain.value,
            "turn_count": len(session.turns),
            "insights_saved": len(all_insights),
            "knowledge_used": len(all_knowledge),
            "created_at": session.created_at.isoformat(),
            "updated_at": session.updated_at.isoformat(),
        }

    async def _detect_consistency_issues(
        self,
        user_message: str,
        knowledge_used: list[KnowledgeItem],
        language: str,
    ) -> list[ConsistencyIssue]:
        """Detect potential consistency issues between user message and past knowledge.

        Args:
            user_message: Current user message.
            knowledge_used: Knowledge items retrieved for this turn.
            language: Session language.

        Returns:
            List of detected consistency issues.
        """
        if not knowledge_used:
            return []

        # Build context for LLM to analyze, including fact_id for matching
        knowledge_context = "\n".join([
            f"- [ID:{item.id}] {item.subject}„ÅØ{item.predicate} „Äå{item.object}„Äç"
            for item in knowledge_used
        ])
        
        # Create a lookup map for fact_id matching
        knowledge_map = {item.id: item for item in knowledge_used}

        if language.lower() in ("english", "en"):
            system_prompt = """You are a consistency checker. Analyze if the user's current message contradicts or changes any past recorded information.

**CRITICAL: Distinguish between "contradiction" and "change":**
- "contradiction": Impossible to be both true (e.g., "A is responsible" vs "B is responsible for the same task")
- "change": Information has been updated/revised (e.g., deadline moved from date A to date B)

Output ONLY valid JSON array. Each item should have:
- kind: "contradiction" or "change" (use "contradiction" when both cannot be true simultaneously)
- fact_id: The ID from the past record (e.g., "abc-123")
- title: Brief title in 5 words or less
- previous_text: What was recorded before
- current_text: What user says now
- suggested_question: Question to clarify

If no issues found, output empty array: []

Contradiction example:
[{"kind":"contradiction","fact_id":"abc-123","title":"Different person responsible","previous_text":"Tanaka is responsible","current_text":"Yamada is responsible","suggested_question":"Previously Tanaka was mentioned as responsible. Has this changed to Yamada?"}]

Change example:
[{"kind":"change","fact_id":"abc-123","title":"Deadline moved","previous_text":"Deadline is March 31","current_text":"Deadline is February 28","suggested_question":"The deadline was March 31 before. Has it been moved to February 28?"}]"""
        else:
            system_prompt = """„ÅÇ„Å™„Åü„ÅØÊï¥ÂêàÊÄß„ÉÅ„Çß„ÉÉ„Ç´„Éº„Åß„Åô„ÄÇ„É¶„Éº„Ç∂„Éº„ÅÆÁèæÂú®„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÅåÈÅéÂéª„ÅÆË®òÈå≤„Å®ÁüõÁõæ„Åæ„Åü„ÅØÂ§âÂåñ„Åó„Å¶„ÅÑ„Çã„ÅãÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

**ÈáçË¶ÅÔºö„ÄåÁüõÁõæ„Äç„Å®„ÄåÂ§âÊõ¥„Äç„ÇíÂå∫Âà•„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö**
- "contradiction"ÔºàÁüõÁõæÔºâ: ‰∏°Êñπ„ÅåÂêåÊôÇ„Å´Áúü„Åß„ÅÇ„Çã„Åì„Å®„Åå‰∏çÂèØËÉΩÔºà‰æãÔºö„ÄåA„Åï„Çì„ÅåÊãÖÂΩì„Äçvs„ÄåÂêå„Åò„Çø„Çπ„ÇØ„ÇíB„Åï„Çì„ÅåÊãÖÂΩì„ÄçÔºâ
- "change"ÔºàÂ§âÊõ¥Ôºâ: ÊÉÖÂ†±„ÅåÊõ¥Êñ∞„Éª‰øÆÊ≠£„Åï„Çå„ÅüÔºà‰æãÔºöÁ∑†„ÇÅÂàá„Çä„ÅåÊó•‰ªòA„Åã„ÇâÊó•‰ªòB„Å´Â§âÊõ¥Ôºâ

ÊúâÂäπ„Å™JSONÈÖçÂàó„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂêÑÈ†ÖÁõÆ„Å´„ÅØ‰ª•‰∏ã„ÇíÂê´„ÇÅ„Åæ„ÅôÔºö
- kind: "contradiction" „Åæ„Åü„ÅØ "change"Ôºà‰∏°Êñπ„ÅåÂêåÊôÇ„Å´Áúü„Åß„ÅÇ„ÇäÂæó„Å™„ÅÑÂ†¥Âêà„ÅØ "contradiction"Ôºâ
- fact_id: ÈÅéÂéª„ÅÆË®òÈå≤„ÅÆIDÔºà‰æã: "abc-123"Ôºâ
- title: 5Ë™û‰ª•ÂÜÖ„ÅÆÁ∞°ÊΩî„Å™„Çø„Ç§„Éà„É´
- previous_text: ‰ª•ÂâçË®òÈå≤„Åï„Çå„Å¶„ÅÑ„ÅüÂÜÖÂÆπ
- current_text: ÁèæÂú®„É¶„Éº„Ç∂„Éº„ÅåË®Ä„Å£„Å¶„ÅÑ„ÇãÂÜÖÂÆπ
- suggested_question: Á¢∫Ë™ç„ÅÆ„Åü„ÇÅ„ÅÆË≥™Âïè

ÂïèÈ°å„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÁ©∫„ÅÆÈÖçÂàó„ÇíÂá∫Âäõ: []

ÁüõÁõæ„ÅÆ‰æã:
[{"kind":"contradiction","fact_id":"abc-123","title":"ÊãÖÂΩìËÄÖ„ÅåÁï∞„Å™„Çã","previous_text":"Áî∞‰∏≠„Åï„Çì„ÅåÊãÖÂΩì","current_text":"Â±±Áî∞„Åï„Çì„ÅåÊãÖÂΩì","suggested_question":"‰ª•Ââç„ÅØÁî∞‰∏≠„Åï„Çì„ÅåÊãÖÂΩì„Å®„ÅÆ„Åì„Å®„Åß„Åó„Åü„Åå„ÄÅÂ±±Áî∞„Åï„Çì„Å´Â§âÊõ¥„Å´„Å™„Çä„Åæ„Åó„Åü„ÅãÔºü"}]

Â§âÊõ¥„ÅÆ‰æã:
[{"kind":"change","fact_id":"abc-123","title":"Á∑†„ÇÅÂàá„ÇäÂ§âÊõ¥","previous_text":"Á∑†„ÇÅÂàá„Çä„ÅØ3Êúà31Êó•","current_text":"Á∑†„ÇÅÂàá„Çä„ÅØ2Êúà28Êó•","suggested_question":"‰ª•Ââç„ÅØ3Êúà31Êó•„ÅåÁ∑†„ÇÅÂàá„Çä„Åß„Åó„Åü„Åå„ÄÅ2Êúà28Êó•„Å´Â§âÊõ¥„Å´„Å™„Çä„Åæ„Åó„Åü„ÅãÔºü"}]"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ÈÅéÂéª„ÅÆË®òÈå≤:\n{knowledge_context}\n\nÁèæÂú®„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏:\n{user_message}"},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "[]"
            
            # Clean up response (remove markdown code blocks if present)
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            # Parse JSON
            issues_data = json.loads(content)
            if not isinstance(issues_data, list):
                return []

            issues = []
            for item in issues_data:
                try:
                    kind = ConsistencyIssueKind(item.get("kind", "change"))
                    fact_id = item.get("fact_id")
                    
                    # Validate fact_id exists in our knowledge
                    if fact_id and fact_id not in knowledge_map:
                        fact_id = None  # Invalid ID, set to None
                    
                    issues.append(ConsistencyIssue(
                        kind=kind,
                        title=item.get("title", "Êï¥ÂêàÊÄß„ÉÅ„Çß„ÉÉ„ÇØ"),
                        fact_id=fact_id,
                        previous_text=item.get("previous_text", ""),
                        previous_source="ÈÅéÂéª„ÅÆË®òÈå≤",
                        current_text=item.get("current_text", ""),
                        current_source="ÁèæÂú®„ÅÆ‰ºöË©±",
                        suggested_question=item.get("suggested_question", ""),
                        confidence=0.7,
                    ))
                except Exception as e:
                    logger.warning(f"Failed to parse consistency issue: {e}")
                    continue

            if issues:
                logger.info(f"Detected {len(issues)} consistency issues")

            return issues

        except Exception as e:
            logger.warning(f"Failed to detect consistency issues: {e}")
            return []

    # ==================== Document Processing ====================

    async def extract_from_document(
        self,
        content: str,
        filename: str,
        language: str = "Japanese",
    ) -> DocumentExtractionResult:
        """Extract structured information from document content using LLM.

        Args:
            content: The parsed text content of the document.
            filename: Original filename for context.
            language: Language for extraction prompts.

        Returns:
            DocumentExtractionResult with summary, facts, topics, entities.
        """
        # Note: Content should already be truncated by caller (process_document_background)
        # This is a safety limit for direct calls - GPT-5.2 can handle 128k+ tokens
        max_chars = 100000
        if len(content) > max_chars:
            # Keep start 20% + end 80% to prioritize recent content
            start_chars = int(max_chars * 0.2)
            end_chars = max_chars - start_chars - 100
            content = (
                content[:start_chars] 
                + f"\n\n[... ‰∏≠Èñì Á¥Ñ{len(content) - start_chars - end_chars:,}ÊñáÂ≠óÁúÅÁï• ...]\n\n" 
                + content[-end_chars:]
            )

        if language.lower() in ("english", "en"):
            system_prompt = """You are a document analyzer. Extract key information from the document, paying special attention to temporal (date/time) information.

Output ONLY valid JSON with these fields:
- summary: 2-3 sentence summary of the document
- facts: Array of factual statements as objects with:
  - subject: The subject entity
  - predicate: The relationship/attribute
  - object: The value/content
  - source_context: Brief context from document (max 50 chars)
  - event_date: Date when this event occurred (YYYY-MM-DD format, null if unknown)
  - event_date_end: End date for date ranges (YYYY-MM-DD format, null if not a range)
  - date_type: One of "exact", "approximate", "range", "unknown"
- topics: Array of main topics/themes
- entities: Array of mentioned people, organizations, projects, places
- domain: One of "daily_work", "recipe", "postmortem", "creative", "general"

IMPORTANT: Extract ALL date information carefully. For each fact:
- If an exact date is mentioned (e.g., "May 1, 2024"), use date_type: "exact"
- If approximate (e.g., "around May", "early 2024"), use date_type: "approximate"
- If a range (e.g., "May 1-15"), use date_type: "range" and set event_date_end
- If no date context, use date_type: "unknown"

Example:
{"summary":"Project status report showing 80% completion with deadline Feb 28, 2024.","facts":[{"subject":"Project A","predicate":"completion rate","object":"80%","source_context":"According to the report","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"Project A","predicate":"deadline","object":"Feb 28, 2024","source_context":"Confirmed in section 3","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["Project Management","Progress Report"],"entities":["Project A","Tanaka"],"domain":"daily_work"}"""
        else:
            system_prompt = """„ÅÇ„Å™„Åü„ÅØ„Éâ„Ç≠„É•„É°„É≥„ÉàÂàÜÊûêËÄÖ„Åß„Åô„ÄÇ„Éâ„Ç≠„É•„É°„É≥„Éà„Åã„ÇâÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁâπ„Å´Êó•‰ªò„ÉªÊôÇÈñìÊÉÖÂ†±„Å´Ê≥®ÊÑè„ÇíÊâï„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

‰ª•‰∏ã„ÅÆ„Éï„Ç£„Éº„É´„Éâ„ÇíÊåÅ„Å§ÊúâÂäπ„Å™JSON„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- summary: 2-3Êñá„ÅÆ„Éâ„Ç≠„É•„É°„É≥„ÉàË¶ÅÁ¥Ñ
- facts: ‰∫ãÂÆü„ÅÆÈÖçÂàóÔºàÂêÑÈ†ÖÁõÆ„ÅØ‰ª•‰∏ã„ÅÆÂΩ¢ÂºèÔºâ
  - subject: ‰∏ªË™û„Ç®„É≥„ÉÜ„Ç£„ÉÜ„Ç£
  - predicate: Èñ¢‰øÇ„ÉªÂ±ûÊÄß
  - object: ÂÄ§„ÉªÂÜÖÂÆπ
  - source_context: „Éâ„Ç≠„É•„É°„É≥„Éà„Åã„Çâ„ÅÆÁ∞°ÊΩî„Å™ÊñáËÑàÔºàÊúÄÂ§ß50ÊñáÂ≠óÔºâ
  - event_date: „Ç§„Éô„É≥„Éà„ÅåÁô∫Áîü„Åó„ÅüÊó•‰ªòÔºàYYYY-MM-DDÂΩ¢Âºè„ÄÅ‰∏çÊòé„Å™„ÇânullÔºâ
  - event_date_end: ÊúüÈñì„ÅÆÁµÇ‰∫ÜÊó•ÔºàYYYY-MM-DDÂΩ¢Âºè„ÄÅÁØÑÂõ≤„Åß„Å™„Åë„Çå„Å∞nullÔºâ
  - date_type: "exact"ÔºàÊ≠£Á¢∫Ôºâ, "approximate"ÔºàÁ¥ÑÔºâ, "range"ÔºàÊúüÈñìÔºâ, "unknown"Ôºà‰∏çÊòéÔºâ„ÅÆ„ÅÑ„Åö„Çå„Åã
- topics: ‰∏ª„Å™„Éà„Éî„ÉÉ„ÇØ„Éª„ÉÜ„Éº„Éû„ÅÆÈÖçÂàó
- entities: Ë®ÄÂèä„Åï„Çå„Åü‰∫∫Áâ©„ÄÅÁµÑÁπî„ÄÅ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÄÅÂ†¥ÊâÄ„ÅÆÈÖçÂàó
- domain: "daily_work", "recipe", "postmortem", "creative", "general" „ÅÆ„ÅÑ„Åö„Çå„Åã

ÈáçË¶ÅÔºö„Åô„Åπ„Å¶„ÅÆÊó•‰ªòÊÉÖÂ†±„ÇíÊ≥®ÊÑèÊ∑±„ÅèÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- Ê≠£Á¢∫„Å™Êó•‰ªò„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºà‰æãÔºö„Äå2024Âπ¥5Êúà1Êó•„ÄçÔºâ‚Üí date_type: "exact"
- ÊõñÊòß„Å™Êó•‰ªò„ÅÆÂ†¥ÂêàÔºà‰æãÔºö„Äå5ÊúàÈ†É„Äç„Äå2024Âπ¥Âàù„ÇÅ„ÄçÔºâ‚Üí date_type: "approximate"
- ÊúüÈñì„ÅÆÂ†¥ÂêàÔºà‰æãÔºö„Äå5Êúà1Êó•„Äú15Êó•„ÄçÔºâ‚Üí date_type: "range"„ÄÅevent_date_end„ÇíË®≠ÂÆö
- Êó•‰ªò„ÅÆÊñáËÑà„Åå„Å™„ÅÑÂ†¥Âêà ‚Üí date_type: "unknown"

‰æã:
{"summary":"„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÈÄ≤Êçó„É¨„Éù„Éº„Éà„ÄÇÂÆå‰∫ÜÁéá80%„ÄÅÁ∑†„ÇÅÂàá„Çä„ÅØ2024Âπ¥2Êúà28Êó•„ÄÇ","facts":[{"subject":"„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA","predicate":"ÂÆå‰∫ÜÁéá","object":"80%","source_context":"„É¨„Éù„Éº„Éà„Å´„Çà„Çã„Å®","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA","predicate":"Á∑†„ÇÅÂàá„Çä","object":"2024Âπ¥2Êúà28Êó•","source_context":"„Çª„ÇØ„Ç∑„Éß„É≥3„ÅßÁ¢∫Ë™ç","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÁÆ°ÁêÜ","ÈÄ≤ÊçóÂ†±Âëä"],"entities":["„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA","Áî∞‰∏≠„Åï„Çì"],"domain":"daily_work"}"""

        user_prompt = f"""„Éï„Ç°„Ç§„É´Âêç: {filename}

„Äê„Éâ„Ç≠„É•„É°„É≥„ÉàÂÜÖÂÆπ„Äë
{content}

‰∏äË®ò„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíÂàÜÊûê„Åó„ÄÅJSONÂΩ¢Âºè„ÅßÊÉÖÂ†±„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            # Parse facts with date information
            facts = []
            for f in data.get("facts", []):
                if isinstance(f, dict) and "subject" in f and "predicate" in f and "object" in f:
                    # Parse event_date
                    event_date = None
                    if f.get("event_date"):
                        try:
                            from datetime import datetime
                            event_date = datetime.strptime(f["event_date"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse event_date_end
                    event_date_end = None
                    if f.get("event_date_end"):
                        try:
                            from datetime import datetime
                            event_date_end = datetime.strptime(f["event_date_end"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse date_type
                    date_type_str = f.get("date_type", "unknown")
                    try:
                        date_type = DateType(date_type_str)
                    except ValueError:
                        date_type = DateType.UNKNOWN
                    
                    facts.append(ExtractedFact(
                        subject=f["subject"],
                        predicate=f["predicate"],
                        object=f["object"],
                        source_context=f.get("source_context", ""),
                        event_date=event_date,
                        event_date_end=event_date_end,
                        date_type=date_type,
                    ))

            # Parse domain
            domain_str = data.get("domain", "general")
            try:
                domain = Domain(domain_str)
            except ValueError:
                domain = Domain.GENERAL

            result = DocumentExtractionResult(
                summary=data.get("summary", "„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆË¶ÅÁ¥Ñ"),
                facts=facts,
                topics=data.get("topics", []),
                entities=data.get("entities", []),
                domain=domain,
            )

            logger.info(f"Extracted {len(facts)} facts from document: {filename}")
            return result

        except Exception as e:
            logger.warning(f"Failed to extract from document: {e}")
            return DocumentExtractionResult(
                summary=f"{filename} „ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà",
                facts=[],
                topics=[],
                entities=[],
                domain=Domain.GENERAL,
            )

    async def generate_topic_summary(
        self,
        topic: str,
        facts: list[KnowledgeItem],
        language: str = "Japanese",
    ) -> TopicSummary:
        """Generate a comprehensive summary for a topic based on accumulated facts.

        Args:
            topic: The topic/entity name.
            facts: List of facts related to the topic.
            language: Language for the summary.

        Returns:
            TopicSummary with comprehensive overview.
        """
        if not facts:
            return TopicSummary(
                topic=topic,
                summary=f"{topic}„Å´Èñ¢„Åô„ÇãË®òÈå≤„ÅØ„Åæ„Å†„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ",
                key_points=[],
                related_entities=[],
                fact_count=0,
            )

        # Format facts for LLM
        facts_text = "\n".join([
            f"- {f.subject}„ÅØ{f.predicate}„Äå{f.object}„Äç({f.created_at.strftime('%Y-%m-%d')})"
            for f in facts
        ])

        # Extract unique entities
        all_entities = set()
        for f in facts:
            all_entities.add(f.subject)
            all_entities.add(f.object)
        all_entities.discard(topic)

        # Get time range
        dates = [f.created_at for f in facts]
        time_range = f"{min(dates).strftime('%Y-%m-%d')} ÔΩû {max(dates).strftime('%Y-%m-%d')}"

        if language.lower() in ("english", "en"):
            system_prompt = """You are a knowledge synthesizer. Create a comprehensive summary of the topic based on the facts provided.

Output ONLY valid JSON with these fields:
- summary: 3-5 sentence comprehensive overview
- key_points: Array of the most important points (max 7)
- insights: Array of insights or patterns you noticed
- questions: Array of follow-up questions that might be useful

Example:
{"summary":"Project A has been progressing steadily...","key_points":["Deadline is Feb 28","80% complete","Tanaka leading"],"insights":["Progress accelerated in January","Risk of deadline slip"],"questions":["What are the remaining tasks?","Any blockers?"]}"""
        else:
            system_prompt = """„ÅÇ„Å™„Åü„ÅØ„Éä„É¨„ÉÉ„Ç∏Áµ±ÂêàËÄÖ„Åß„Åô„ÄÇÊèê‰æõ„Åï„Çå„Åü‰∫ãÂÆü„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅ„Éà„Éî„ÉÉ„ÇØ„ÅÆÂåÖÊã¨ÁöÑ„Å™Ë¶ÅÁ¥Ñ„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

‰ª•‰∏ã„ÅÆ„Éï„Ç£„Éº„É´„Éâ„ÇíÊåÅ„Å§ÊúâÂäπ„Å™JSON„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- summary: 3-5Êñá„ÅÆÂåÖÊã¨ÁöÑ„Å™Ê¶ÇË¶Å
- key_points: ÊúÄ„ÇÇÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà„ÅÆÈÖçÂàóÔºàÊúÄÂ§ß7„Å§Ôºâ
- insights: Ê∞ó„Å•„ÅÑ„Åü„Éë„Çø„Éº„É≥„ÇÑÊ¥ûÂØü„ÅÆÈÖçÂàó
- questions: ÊúâÁî®„Å®ÊÄù„Çè„Çå„Çã„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„ÉóË≥™Âïè„ÅÆÈÖçÂàó

‰æã:
{"summary":"„Éó„É≠„Ç∏„Çß„ÇØ„ÉàA„ÅØÈ†ÜË™ø„Å´ÈÄ≤Ë°å„Åó„Å¶„Åä„Çä...","key_points":["Á∑†„ÇÅÂàá„Çä„ÅØ2Êúà28Êó•","ÂÆå‰∫ÜÁéá80%","Áî∞‰∏≠„Åï„Çì„Åå„É™„Éº„Éâ"],"insights":["1Êúà„Å´ÈÄ≤Êçó„ÅåÂä†ÈÄü","Á∑†„ÇÅÂàá„ÇäÈÅÖÂª∂„ÅÆ„É™„Çπ„ÇØ„ÅÇ„Çä"],"questions":["ÊÆã„Çä„ÅÆ„Çø„Çπ„ÇØ„ÅØ‰Ωï„ÅãÔºü","„Éñ„É≠„ÉÉ„Ç´„Éº„ÅØ„ÅÇ„Çã„ÅãÔºü"]}"""

        user_prompt = f"""„Éà„Éî„ÉÉ„ÇØ: {topic}
Èñ¢ÈÄ£„Åô„Çã‰∫ãÂÆüÊï∞: {len(facts)}
ÊúüÈñì: {time_range}

„ÄêË®òÈå≤„Åï„Çå„Åü‰∫ãÂÆü„Äë
{facts_text}

‰∏äË®ò„ÅÆ‰∫ãÂÆü„ÇíÁµ±Âêà„Åó„ÄÅ{topic}„Å´„Å§„ÅÑ„Å¶„ÅÆÂåÖÊã¨ÁöÑ„Å™Ë¶ÅÁ¥Ñ„ÇíJSONÂΩ¢Âºè„Åß‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            summary = TopicSummary(
                topic=topic,
                summary=data.get("summary", f"{topic}„Å´„Å§„ÅÑ„Å¶„ÅÆË¶ÅÁ¥Ñ"),
                key_points=data.get("key_points", []) + data.get("insights", []),
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )

            logger.info(f"Generated summary for topic: {topic}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate topic summary: {e}")
            return TopicSummary(
                topic=topic,
                summary=f"{topic}„Å´Èñ¢„Åô„Çã{len(facts)}‰ª∂„ÅÆ‰∫ãÂÆü„ÅåË®òÈå≤„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
                key_points=[f.object for f in facts[:5]],
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )

    # ==================== Tag Suggestion Methods ====================

    async def suggest_tags(
        self,
        content: str,
        existing_tags: list[Tag] | None = None,
        max_tags: int = 5,
        language: str | None = None,
    ) -> TagSuggestionResult:
        """Suggest tags for given content using LLM.

        Args:
            content: Content to analyze for tags.
            existing_tags: List of existing tags to consider for reuse.
            max_tags: Maximum number of tags to suggest.
            language: Language for prompts (auto-detect if None).

        Returns:
            TagSuggestionResult with suggested tags.
        """
        import json

        if existing_tags is None:
            existing_tags = await self._kg_store.get_all_tags(limit=100)

        lang = language or detect_language(content)

        # Format existing tags for the prompt
        existing_tags_str = ""
        if existing_tags:
            tag_names = [f"- {t.name}" + (f" (Âà•Âêç: {', '.join(t.aliases[:3])})" if t.aliases else "") 
                        for t in existing_tags[:50]]
            existing_tags_str = "\n".join(tag_names)

        if lang == "Japanese":
            system_prompt = """„ÅÇ„Å™„Åü„ÅØ„Ç≥„É≥„ÉÜ„É≥„ÉÑÂàÜÈ°û„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ
‰∏é„Åà„Çâ„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÊûê„Åó„ÄÅÈÅ©Âàá„Å™„Çø„Ç∞„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Çø„Ç∞„ÅØ‰ª•‰∏ã„ÅÆÁâπÂæ¥„ÇíÊåÅ„Å§„Åπ„Åç„Åß„ÅôÔºö
- Áü≠„ÅèÁ∞°ÊΩîÔºà1„Äú3Ë™ûÁ®ãÂ∫¶Ôºâ
- ÂÜÖÂÆπ„ÅÆÊú¨Ë≥™„ÇíÊçâ„Åà„Å¶„ÅÑ„Çã
- Ê§úÁ¥¢„ÇÑ„Ç∞„É´„Éº„Éî„É≥„Ç∞„Å´ÂΩπÁ´ã„Å§
- Êó¢Â≠ò„Çø„Ç∞„ÅßÈÅ©Âàá„Å™„ÇÇ„ÅÆ„Åå„ÅÇ„Çå„Å∞ÂÑ™ÂÖàÁöÑ„Å´ÂÜçÂà©Áî®

JSON„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
{
  "tags": [
    {"name": "„Çø„Ç∞Âêç", "relevance": 0.9, "reason": "„Åì„ÅÆ„Çø„Ç∞„ÇíÈÅ∏„Çì„Å†ÁêÜÁî±", "is_existing": false}
  ],
  "content_summary": "„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÁ∞°ÊΩî„Å™Ë¶ÅÁ¥Ñ"
}

relevance„ÅØ0.0„Äú1.0„Åß„ÄÅ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Å®„ÅÆÈñ¢ÈÄ£Â∫¶„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ
is_existing„ÅØÊó¢Â≠ò„Çø„Ç∞„É™„Çπ„Éà„Åã„ÇâÈÅ∏„Çì„Å†Â†¥Âêàtrue„ÄÅÊñ∞Ë¶èÊèêÊ°à„ÅÆÂ†¥Âêàfalse„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

            user_prompt = f"""‰ª•‰∏ã„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Å´ÈÅ©Âàá„Å™„Çø„Ç∞„ÇíÊúÄÂ§ß{max_tags}ÂÄãÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Äë
{content[:2000]}

„ÄêÊó¢Â≠ò„Çø„Ç∞‰∏ÄË¶ß„Äë
{existing_tags_str if existing_tags_str else "Ôºà„Åæ„Å†„Çø„Ç∞„Åå„ÅÇ„Çä„Åæ„Åõ„ÇìÔºâ"}

ÈÅ©Âàá„Å™Êó¢Â≠ò„Çø„Ç∞„Åå„ÅÇ„Çå„Å∞ÂÑ™ÂÖàÁöÑ„Å´‰ΩøÁî®„Åó„ÄÅÂøÖË¶Å„Åß„ÅÇ„Çå„Å∞Êñ∞„Åó„ÅÑ„Çø„Ç∞„ÇíÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        else:
            system_prompt = """You are a content classification expert.
Analyze the given text and suggest appropriate tags.

Tags should be:
- Short and concise (1-3 words)
- Capture the essence of the content
- Useful for search and grouping
- Reuse existing tags when appropriate

Output in JSON format:
{
  "tags": [
    {"name": "tag name", "relevance": 0.9, "reason": "why this tag", "is_existing": false}
  ],
  "content_summary": "brief summary of content"
}

relevance is 0.0-1.0 indicating how related the tag is to the content.
is_existing should be true if selected from existing tags, false if newly suggested."""

            user_prompt = f"""Suggest up to {max_tags} appropriate tags for the following content.

„ÄêContent„Äë
{content[:2000]}

„ÄêExisting Tags„Äë
{existing_tags_str if existing_tags_str else "(No tags yet)"}

Prioritize existing tags when appropriate, and suggest new ones if needed."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            suggestions: list[TagSuggestion] = []
            existing_matched = 0
            new_suggested = 0

            for tag_data in data.get("tags", [])[:max_tags]:
                tag_name = tag_data.get("name", "").strip()
                if not tag_name:
                    continue

                is_existing = tag_data.get("is_existing", False)
                existing_tag_id = None

                # Check if this matches an existing tag
                if existing_tags:
                    for existing_tag in existing_tags:
                        if (existing_tag.name.lower() == tag_name.lower() or
                            any(alias.lower() == tag_name.lower() for alias in existing_tag.aliases)):
                            existing_tag_id = existing_tag.id
                            is_existing = True
                            tag_name = existing_tag.name  # Use canonical name
                            break

                if is_existing:
                    existing_matched += 1
                else:
                    new_suggested += 1

                suggestions.append(TagSuggestion(
                    name=tag_name,
                    relevance=min(1.0, max(0.0, tag_data.get("relevance", 0.8))),
                    reason=tag_data.get("reason", ""),
                    existing_tag_id=existing_tag_id,
                    is_new=not is_existing,
                ))

            return TagSuggestionResult(
                suggestions=suggestions,
                content_summary=data.get("content_summary", ""),
                existing_tags_matched=existing_matched,
                new_tags_suggested=new_suggested,
            )

        except Exception as e:
            logger.warning(f"Failed to suggest tags: {e}")
            return TagSuggestionResult(
                suggestions=[],
                content_summary="",
                existing_tags_matched=0,
                new_tags_suggested=0,
            )

    async def check_similar_tags(
        self,
        new_tag_name: str,
        existing_tags: list[Tag] | None = None,
        language: str | None = None,
    ) -> tuple[str | None, float]:
        """Check if a new tag is similar to existing tags.

        Args:
            new_tag_name: Name of the potential new tag.
            existing_tags: List of existing tags to compare.
            language: Language for prompts (auto-detect if None).

        Returns:
            Tuple of (existing_tag_id_to_merge_into, similarity_score).
            Returns (None, 0.0) if no similar tag found.
        """
        import json

        if existing_tags is None:
            existing_tags = await self._kg_store.get_all_tags(limit=100)

        if not existing_tags:
            return None, 0.0

        lang = language or detect_language(new_tag_name)

        tag_names_list = [f"{t.name} (ID: {t.id})" for t in existing_tags[:50]]
        tag_names_str = "\n".join(tag_names_list)

        if lang == "Japanese":
            system_prompt = """„ÅÇ„Å™„Åü„ÅØ„Çø„Ç∞„ÅÆÈ°û‰ººÂ∫¶Âà§ÂÆö„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ
Êñ∞„Åó„ÅÑ„Çø„Ç∞„ÅåÊó¢Â≠ò„Çø„Ç∞„Å®ÊÑèÂë≥ÁöÑ„Å´Âêå„Åò„ÅãÂà§ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

JSON„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßÂá∫ÂäõÔºö
{
  "is_similar": true/false,
  "similar_tag_id": "È°û‰ºº„Çø„Ç∞„ÅÆIDÔºà„Å™„Åë„Çå„Å∞nullÔºâ",
  "similarity_score": 0.9,
  "reason": "Âà§ÂÆöÁêÜÁî±"
}

similarity_score„ÅØ0.0„Äú1.0„Åß„ÄÅ0.8‰ª•‰∏ä„Çí„ÄåÂêåÁæ©Ë™û„Äç„Å®„Åø„Å™„Åó„Åæ„Åô„ÄÇ
‰æã: „ÄåÊóÖË°å„Äç„Å®„Äå„Éà„É©„Éô„É´„Äç„ÄÅ„Äå„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞„Äç„Å®„Äå„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Äç„ÅØÂêåÁæ©Ë™û„Åß„Åô„ÄÇ"""

            user_prompt = f"""Êñ∞Ë¶è„Çø„Ç∞ÂÄôË£ú: „Äå{new_tag_name}„Äç

„ÄêÊó¢Â≠ò„Çø„Ç∞‰∏ÄË¶ß„Äë
{tag_names_str}

„Åì„ÅÆÊñ∞Ë¶è„Çø„Ç∞„ÅØÊó¢Â≠ò„Çø„Ç∞„ÅÆ„ÅÑ„Åö„Çå„Åã„Å®ÂêåÁæ©„Åß„Åô„ÅãÔºü"""

        else:
            system_prompt = """You are an expert in tag similarity assessment.
Determine if a new tag is semantically the same as existing tags.

Output in JSON format:
{
  "is_similar": true/false,
  "similar_tag_id": "ID of similar tag or null",
  "similarity_score": 0.9,
  "reason": "reason for judgment"
}

similarity_score is 0.0-1.0, with 0.8+ considered synonyms.
Example: "travel" and "trip", "programming" and "coding" are synonyms."""

            user_prompt = f"""New tag candidate: "{new_tag_name}"

„ÄêExisting Tags„Äë
{tag_names_str}

Is this new tag synonymous with any existing tag?"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            is_similar = data.get("is_similar", False)
            similar_tag_id = data.get("similar_tag_id")
            similarity_score = data.get("similarity_score", 0.0)

            if is_similar and similar_tag_id and similarity_score >= 0.8:
                # Verify the tag ID exists
                for tag in existing_tags:
                    if tag.id == similar_tag_id:
                        logger.info(f"Tag '{new_tag_name}' is similar to '{tag.name}' (score: {similarity_score})")
                        return similar_tag_id, similarity_score
                # Tag ID not found, try to find by name match in reason
                logger.warning(f"Similar tag ID '{similar_tag_id}' not found in existing tags")

            return None, 0.0

        except Exception as e:
            logger.warning(f"Failed to check tag similarity: {e}")
            return None, 0.0

    async def auto_tag_content(
        self,
        content: str,
        max_tags: int = 5,
        min_relevance: float = 0.6,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag content by suggesting, checking similarity, and creating tags.

        Args:
            content: Content to tag.
            max_tags: Maximum number of tags.
            min_relevance: Minimum relevance score to include.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples for the created/matched tags.
        """
        # Get existing tags
        existing_tags = await self._kg_store.get_all_tags(limit=100)

        # Get tag suggestions
        result = await self.suggest_tags(
            content=content,
            existing_tags=existing_tags,
            max_tags=max_tags,
            language=language,
        )

        tags_with_relevance: list[tuple[Tag, float]] = []

        for suggestion in result.suggestions:
            if suggestion.relevance < min_relevance:
                continue

            if suggestion.existing_tag_id:
                # Use existing tag
                tag = await self._kg_store.get_tag(suggestion.existing_tag_id)
                if tag:
                    tags_with_relevance.append((tag, suggestion.relevance))
            else:
                # Check for similar existing tags
                similar_tag_id, similarity = await self.check_similar_tags(
                    new_tag_name=suggestion.name,
                    existing_tags=existing_tags,
                    language=language,
                )

                if similar_tag_id and similarity >= 0.8:
                    # Use similar existing tag
                    tag = await self._kg_store.get_tag(similar_tag_id)
                    if tag:
                        # Add the suggested name as an alias if not already there
                        if (suggestion.name.lower() != tag.name.lower() and
                            suggestion.name.lower() not in [a.lower() for a in tag.aliases]):
                            await self._kg_store.update_tag(
                                tag.id,
                                aliases=tag.aliases + [suggestion.name],
                            )
                            tag = await self._kg_store.get_tag(similar_tag_id)  # Refresh
                        if tag:
                            tags_with_relevance.append((tag, suggestion.relevance))
                else:
                    # Create new tag
                    tag = await self._kg_store.create_tag(name=suggestion.name)
                    tags_with_relevance.append((tag, suggestion.relevance))
                    # Add to existing_tags for subsequent similarity checks
                    existing_tags.append(tag)

        return tags_with_relevance

    async def auto_tag_insight(
        self,
        insight_id: str,
        content: str,
        max_tags: int = 3,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag an insight.

        Args:
            insight_id: ID of the insight to tag.
            content: Content to analyze for tags (usually subject + predicate + object).
            max_tags: Maximum number of tags.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples applied to the insight.
        """
        tags_with_relevance = await self.auto_tag_content(
            content=content,
            max_tags=max_tags,
            language=language,
        )

        for tag, relevance in tags_with_relevance:
            await self._kg_store.tag_insight(insight_id, tag.id, relevance)

        logger.info(f"Auto-tagged insight {insight_id} with {len(tags_with_relevance)} tags")
        return tags_with_relevance

    async def auto_tag_document(
        self,
        document_id: str,
        content: str,
        max_tags: int = 5,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag a document.

        Args:
            document_id: ID of the document to tag.
            content: Content to analyze for tags (usually summary + extracted facts).
            max_tags: Maximum number of tags.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples applied to the document.
        """
        tags_with_relevance = await self.auto_tag_content(
            content=content,
            max_tags=max_tags,
            language=language,
        )

        for tag, relevance in tags_with_relevance:
            await self._kg_store.tag_document(document_id, tag.id, relevance)

        logger.info(f"Auto-tagged document {document_id} with {len(tags_with_relevance)} tags")
        return tags_with_relevance

    async def auto_tag_insights_batch(
        self,
        insights: list[dict[str, str]],
        max_tags_per_insight: int = 2,
        language: str | None = None,
        existing_tags: list[str] | None = None,
    ) -> dict[str, list[tuple[Tag, float]]]:
        """Automatically tag multiple insights in a single LLM call.

        Args:
            insights: List of dicts with 'id', 'subject', 'predicate', 'object' keys.
            max_tags_per_insight: Maximum tags per insight.
            language: Language for prompts.
            existing_tags: List of existing tag names to prefer for consistency.

        Returns:
            Dict mapping insight_id to list of (Tag, relevance) tuples.
        """
        if not insights:
            return {}

        lang = language or "ja"  # Default to Japanese

        # Build batch prompt
        insights_text = "\n".join([
            f"[{i+1}] {ins['subject']} {ins['predicate']} {ins['object']}"
            for i, ins in enumerate(insights)
        ])

        # Get existing tags for consistency
        existing_tags_text = ""
        if existing_tags:
            existing_tags_text = f"\nÊó¢Â≠ò„Çø„Ç∞ÔºàÂÑ™ÂÖàÁöÑ„Å´‰ΩøÁî®Ôºâ: {', '.join(existing_tags[:30])}\n"

        prompt = f"""‰ª•‰∏ã„ÅÆ„Éï„Ç°„ÇØ„Éà„Å´ÂØæ„Åó„Å¶„ÄÅ**ÊäΩË±°ÁöÑ„Å™„Ç´„ÉÜ„Ç¥„É™„Çø„Ç∞**„Çí‰ªò‰∏é„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Éï„Ç°„ÇØ„Éà‰∏ÄË¶ß:
{insights_text}
{existing_tags_text}
## „Çø„Ç∞‰ªò„Åë„ÅÆ„É´„Éº„É´

1. **ÊäΩË±°Â∫¶„Çí‰∏ä„Åí„Çã**: ÂÖ∑‰ΩìÁöÑ„Åô„Åé„Çã„Çø„Ç∞„ÅØÈÅø„Åë„Çã
   - ‚ùå ÊÇ™„ÅÑ‰æã: „ÄåÁµåÈÅé5Êó•„Äç„ÄåÁµåÈÅé7Êó•„Äç„ÄåÂ∑¶„Åã„Åã„Å®„Äç„ÄåÂè≥„Åã„Åã„Å®„Äç
   - ‚úÖ ËâØ„ÅÑ‰æã: „ÄåÁµåÈÅéË¶≥ÂØü„Äç„Äå„Åã„Åã„Å®„Äç„ÄåË∂≥„Äç

2. **„Ç´„ÉÜ„Ç¥„É™„Å®„Åó„Å¶Ê©üËÉΩ„Åô„Çã„Çø„Ç∞**: Ë§áÊï∞„ÅÆ„Éï„Ç°„ÇØ„Éà„Çí„Ç∞„É´„Éº„ÉóÂåñ„Åß„Åç„Çã„ÇÇ„ÅÆ
   - ‚ùå ÊÇ™„ÅÑ‰æã: „Äå„Çπ„Éà„É¨„ÉÉ„ÉÅÊú™ÂÆüÊñΩ„Äç„Äå„Çπ„Éà„É¨„ÉÉ„ÉÅÂÜÖÂÆπ„Äç„Äå„Çπ„Éà„É¨„ÉÉ„ÉÅ„É°„Éã„É•„Éº„Äç
   - ‚úÖ ËâØ„ÅÑ‰æã: „Äå„Çπ„Éà„É¨„ÉÉ„ÉÅ„Äç„ÄåÈÅãÂãï„Äç„Äå„Éà„É¨„Éº„Éã„É≥„Ç∞„Äç

3. **„Çø„Ç∞„ÅÆÁ®ÆÈ°û**:
   - „ÉÜ„Éº„Éû: ÂÅ•Â∫∑, ‰ªï‰∫ã, Ë∂£Âë≥, ÂÆ∂Ë®à, Êó•Â∏∏
   - Ê¥ªÂãï: ÈÅãÂãï, È£ü‰∫ã, ÈÄöÈô¢, ÂãâÂº∑
   - Áä∂ÊÖã: ‰ΩìË™ø, Ê∞óÂàÜ, ÈÄ≤Êçó
   - Ë∫´‰ΩìÈÉ®‰Ωç: ËÖ∞, Ë∂≥, ËÇ© ÔºàÂ∑¶Âè≥„ÅØÂå∫Âà•„Åó„Å™„ÅÑÔºâ

4. **Êó¢Â≠ò„Çø„Ç∞„ÇíÂÑ™ÂÖà**: È°û‰ºº„ÅÆÊó¢Â≠ò„Çø„Ç∞„Åå„ÅÇ„Çå„Å∞Êñ∞Ë¶è‰ΩúÊàê„Åõ„ÅöÂÜçÂà©Áî®

## ÂõûÁ≠îÂΩ¢ÂºèÔºàJSONÈÖçÂàóÔºâ:
[
  {{"index": 1, "tags": [{{"name": "„Çø„Ç∞Âêç", "relevance": 0.9}}]}},
  ...
]

Èñ¢ÈÄ£Â∫¶(relevance)„ÅØ0.7-1.0„ÅÆÁØÑÂõ≤„Åß„ÄÅ„Åù„ÅÆ„Çø„Ç∞„Åå„Éï„Ç°„ÇØ„Éà„Çí„Å©„Çå„Å†„Åë‰ª£Ë°®„Åô„Çã„Åã„ÇíÁ§∫„Åô„ÄÇ"""

        try:
            response = await self._llm.chat(
                messages=[{"role": "user", "content": prompt}],
            )

            import json
            import re

            # Parse response
            content = response.content or ""
            content = content.strip()
            # Extract JSON array from response
            match = re.search(r'\[[\s\S]*\]', content)
            if not match:
                logger.warning(f"Failed to find JSON array in LLM response")
                return {}

            results = json.loads(match.group())
            
            # Process results
            tagged_insights: dict[str, list[tuple[Tag, float]]] = {}
            
            for result in results:
                idx = result.get("index", 0) - 1
                if idx < 0 or idx >= len(insights):
                    continue
                    
                insight_id = insights[idx]["id"]
                insight_tags: list[tuple[Tag, float]] = []
                
                for tag_info in result.get("tags", [])[:max_tags_per_insight]:
                    tag_name = tag_info.get("name", "").strip()
                    relevance = min(1.0, max(0.5, float(tag_info.get("relevance", 0.8))))
                    
                    if not tag_name:
                        continue
                    
                    try:
                        # Get or create tag
                        tag = await self._kg_store.get_or_create_tag(tag_name)
                        
                        # Check for similar tags and merge if needed
                        try:
                            similar = await self._kg_store.find_similar_tags(tag_name, threshold=0.85, limit=1)
                            if similar and similar[0][0].id != tag.id:
                                tag = similar[0][0]
                        except Exception:
                            pass  # Use the created tag if similarity check fails
                        
                        # Tag the insight
                        await self._kg_store.tag_insight(insight_id, tag.id, relevance)
                        insight_tags.append((tag, relevance))
                    except Exception as tag_err:
                        logger.warning(f"Failed to apply tag {tag_name}: {tag_err}")
                
                if insight_tags:
                    tagged_insights[insight_id] = insight_tags
            
            total_tags = sum(len(tags) for tags in tagged_insights.values())
            logger.info(f"Batch-tagged {len(tagged_insights)} insights with {total_tags} total tags")
            return tagged_insights

        except Exception as e:
            logger.error(f"Failed to batch tag insights: {e}")
            return {}
