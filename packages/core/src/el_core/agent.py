"""Eager Learner (EL) - LLM-native interview agent with knowledge graph integration."""

from __future__ import annotations

import logging
import uuid
from typing import Any

from el_core.llm.client import LLMClient
from el_core.prompts import get_system_prompt
from el_core.schemas import (
    AgentResponse,
    ConsistencyIssue,
    ConsistencyIssueKind,
    ConversationTurn,
    DateType,
    DocumentChunk,
    DocumentExtractionResult,
    Domain,
    ExtractedFact,
    Insight,
    KnowledgeItem,
    PendingQuestion,
    QuestionKind,
    QuestionStatus,
    Session,
    SessionSummary,
    Tag,
    TagSuggestion,
    TagSuggestionResult,
    TopicSummary,
)
from el_core.stores.kg_store import KnowledgeGraphStore
from el_core.tools import ALL_TOOLS, ToolExecutor

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """Detect if text is Japanese or English.

    Args:
        text: Text to analyze.

    Returns:
        "Japanese" or "English".
    """
    # Check for Japanese characters
    for char in text:
        if (
            "\u3040" <= char <= "\u309f"  # Hiragana
            or "\u30a0" <= char <= "\u30ff"  # Katakana
            or "\u4e00" <= char <= "\u9faf"  # Kanji
        ):
            return "Japanese"
    return "English"


def extract_dates_from_text(text: str) -> tuple[Any, Any]:
    """Extract date references from text for search filtering.

    Supports:
    - Japanese relative dates: ä»Šæ—¥, æ˜¨æ—¥, ä¸€æ˜¨æ—¥, å…ˆé€±, ä»Šé€±, etc.
    - English relative dates: today, yesterday, last week, etc.
    - Japanese date formats: 2024å¹´5æœˆ1æ—¥, 5æœˆ1æ—¥, 5/1, 5æœˆ
    - English date formats: May 1, 2024, May 1st, May 2024
    - ISO format: 2024-05-01

    Args:
        text: Text to extract dates from.

    Returns:
        Tuple of (start_date, end_date) as datetime objects, or (None, None) if no dates found.
    """
    import re
    from datetime import datetime, timedelta

    now = datetime.now()
    current_year = now.year
    today = now.replace(hour=0, minute=0, second=0, microsecond=0)

    # ===== RELATIVE DATES (Japanese) =====
    # Check for relative date references first (highest priority)
    text_lower = text.lower()
    
    # ä¸€æ˜¨æ—¥ (day before yesterday)
    if "ä¸€æ˜¨æ—¥" in text or "ãŠã¨ã¨ã„" in text:
        target = today - timedelta(days=2)
        return target, target
    
    # æ˜¨æ—¥ (yesterday)
    if "æ˜¨æ—¥" in text or "ãã®ã†" in text:
        target = today - timedelta(days=1)
        return target, target
    
    # ä»Šæ—¥ (today)
    if "ä»Šæ—¥" in text or "ãã‚‡ã†" in text or "æœ¬æ—¥" in text:
        return today, today
    
    # æ˜æ—¥ (tomorrow)
    if "æ˜æ—¥" in text or "ã‚ã—ãŸ" in text or "ã‚ã™" in text:
        target = today + timedelta(days=1)
        return target, target
    
    # æ˜å¾Œæ—¥ (day after tomorrow)
    if "æ˜å¾Œæ—¥" in text or "ã‚ã•ã£ã¦" in text:
        target = today + timedelta(days=2)
        return target, target
    
    # å…ˆé€± (last week)
    if "å…ˆé€±" in text:
        # Last week: Monday to Sunday of the previous week
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    # ä»Šé€± (this week)
    if "ä»Šé€±" in text:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    # å…ˆæœˆ (last month)
    if "å…ˆæœˆ" in text:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # ä»Šæœˆ (this month)
    if "ä»Šæœˆ" in text:
        first_of_this_month = today.replace(day=1)
        if today.month == 12:
            last_of_this_month = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
        else:
            last_of_this_month = today.replace(month=today.month + 1, day=1) - timedelta(days=1)
        return first_of_this_month, last_of_this_month
    
    # Næ—¥å‰ (N days ago)
    n_days_ago_match = re.search(r"(\d+)æ—¥å‰", text)
    if n_days_ago_match:
        days = int(n_days_ago_match.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # Né€±é–“å‰ (N weeks ago)
    n_weeks_ago_match = re.search(r"(\d+)é€±é–“å‰", text)
    if n_weeks_ago_match:
        weeks = int(n_weeks_ago_match.group(1))
        target = today - timedelta(weeks=weeks)
        return target, target
    
    # ===== RELATIVE DATES (English) =====
    if "day before yesterday" in text_lower:
        target = today - timedelta(days=2)
        return target, target
    
    if "yesterday" in text_lower:
        target = today - timedelta(days=1)
        return target, target
    
    if "today" in text_lower:
        return today, today
    
    if "tomorrow" in text_lower:
        target = today + timedelta(days=1)
        return target, target
    
    if "day after tomorrow" in text_lower:
        target = today + timedelta(days=2)
        return target, target
    
    if "last week" in text_lower:
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    if "this week" in text_lower:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    if "last month" in text_lower:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # N days ago
    n_days_ago_en = re.search(r"(\d+)\s*days?\s*ago", text_lower)
    if n_days_ago_en:
        days = int(n_days_ago_en.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # ===== ABSOLUTE DATES =====
    # Japanese date patterns
    jp_full_date = re.search(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    jp_month_day = re.search(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    jp_month_only = re.search(r"(\d{1,2})æœˆ(?!æ—¥)", text)
    
    # Slash format: 1/12, 12/25 (month/day) - common Japanese diary format
    slash_date = re.search(r"(?<!\d)(\d{1,2})/(\d{1,2})(?!\d)", text)

    # English date patterns
    en_full_date = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?,?\s*(\d{4})",
        text,
        re.IGNORECASE,
    )
    en_month_day = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?",
        text,
        re.IGNORECASE,
    )
    en_month_only = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*(\d{4})?",
        text,
        re.IGNORECASE,
    )

    # ISO format
    iso_date = re.search(r"(\d{4})-(\d{2})-(\d{2})", text)

    month_map = {
        "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
        "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12,
    }

    try:
        # Try Japanese full date first
        if jp_full_date:
            year, month, day = int(jp_full_date.group(1)), int(jp_full_date.group(2)), int(jp_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # ISO format
        if iso_date:
            year, month, day = int(iso_date.group(1)), int(iso_date.group(2)), int(iso_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # English full date
        if en_full_date:
            month = month_map[en_full_date.group(1).lower()[:3]]
            day = int(en_full_date.group(2))
            year = int(en_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # Japanese month + day
        if jp_month_day:
            month, day = int(jp_month_day.group(1)), int(jp_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # Slash format: 1/12 -> January 12
        if slash_date:
            month, day = int(slash_date.group(1)), int(slash_date.group(2))
            # Validate month and day
            if 1 <= month <= 12 and 1 <= day <= 31:
                date = datetime(current_year, month, day)
                return date, date

        # English month + day
        if en_month_day:
            month = month_map[en_month_day.group(1).lower()[:3]]
            day = int(en_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # Japanese month only - return range for the whole month
        if jp_month_only:
            month = int(jp_month_only.group(1))
            start_date = datetime(current_year, month, 1)
            # End of month
            if month == 12:
                end_date = datetime(current_year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(current_year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

        # English month only
        if en_month_only:
            month = month_map[en_month_only.group(1).lower()[:3]]
            year = int(en_month_only.group(2)) if en_month_only.group(2) else current_year
            start_date = datetime(year, month, 1)
            if month == 12:
                end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

    except (ValueError, AttributeError):
        pass

    return None, None


class ELAgent:
    """Eager Learner (EL) - A curious and empathetic interviewer powered by LLM.

    This agent uses GPT-5.2 with function calling to:
    - Search knowledge graph for relevant context
    - Save important insights from conversations
    - Dynamically detect conversation domains
    - Generate empathetic, insightful questions
    """

    def __init__(
        self,
        llm_client: LLMClient | None = None,
        kg_store: KnowledgeGraphStore | None = None,
    ) -> None:
        """Initialize the Eager Learner agent.

        Args:
            llm_client: LLM client for chat completions. Creates default if None.
            kg_store: Knowledge graph store. Tools will work in local-only mode if None.
        """
        self._llm = llm_client or LLMClient()
        self._kg_store = kg_store
        self._sessions: dict[str, Session] = {}

    async def start_session(
        self,
        user_id: str,
        topic: str,
        language: str | None = None,
    ) -> tuple[str, str]:
        """Start a new conversation session.

        Args:
            user_id: User identifier.
            topic: Conversation topic.
            language: Language preference. Auto-detected if None.

        Returns:
            Tuple of (session_id, opening_message).
        """
        detected_lang = language or detect_language(topic)
        session_id = str(uuid.uuid4())

        session = Session(
            id=session_id,
            user_id=user_id,
            topic=topic,
            language=detected_lang,
        )
        
        # Pre-fetch related knowledge from KG
        if self._kg_store:
            try:
                related_items = await self._kg_store.search(topic, limit=5)
                if related_items:
                    session.prior_knowledge = related_items
                    session.prior_context = self._format_prior_knowledge(related_items, detected_lang)
                    logger.info(f"Found {len(related_items)} related knowledge items for topic: {topic}")
            except Exception as e:
                logger.warning(f"Failed to pre-fetch knowledge: {e}")
        
        self._sessions[session_id] = session

        # Save session metadata to Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to save session metadata: {e}")

        # Generate natural opening response using LLM
        opening = await self._generate_opening_response(session, topic, detected_lang)
        logger.info(f"Started session {session_id} for user {user_id} on topic: {topic}")

        return session_id, opening
    
    async def _save_session_metadata(self, session: Session) -> None:
        """Save session metadata to Neo4j.
        
        Args:
            session: Session to save metadata for.
        """
        if self._kg_store is None:
            return
        
        await self._kg_store.save_session_metadata(
            session_id=session.id,
            user_id=session.user_id,
            topic=session.topic,
            domain=session.domain,
            turn_count=len(session.turns),
            insights_count=session.insights_count,
            created_at=session.created_at,
            updated_at=session.updated_at,
        )
    
    async def _generate_opening_response(
        self,
        session: Session,
        user_input: str,
        language: str,
    ) -> str:
        """Generate a natural opening response using LLM.
        
        Instead of using a template, we let the LLM respond naturally to
        whatever the user typed - whether it's a topic, a request, or a question.
        
        Args:
            session: The session object.
            user_input: What the user typed to start the conversation.
            language: Detected language.
            
        Returns:
            Natural opening response from the LLM.
        """
        # Use a simplified prompt for opening - no tool instructions
        if language.lower() in ("english", "en"):
            system_content = (
                "You are EL, a curious and empathetic interviewer. "
                "Your role is to deeply understand what people share and help them discover new insights.\n\n"
                "Guidelines:\n"
                "- Show genuine interest and empathy\n"
                "- Keep responses conversational and warm\n"
                "- Ask 1-2 open-ended questions to start the dialogue\n"
                "- Be concise but engaging\n"
                "- Do NOT output any JSON, tool calls, or code\n"
            )
        else:
            system_content = (
                "ã‚ãªãŸã¯ã€ŒELã€ã§ã™ã€‚å¥½å¥‡å¿ƒæ—ºç››ã§å…±æ„Ÿæ€§ã®é«˜ã„ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¯ãƒ¼ã¨ã—ã¦ã€"
                "ç›¸æ‰‹ã®è©±ã‚’æ·±ãç†è§£ã—ã€æ–°ãŸãªæ°—ã¥ãã‚’å¼•ãå‡ºã™å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚\n\n"
                "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼š\n"
                "- å…±æ„Ÿã¨é–¢å¿ƒã‚’æŒã£ã¦å¿œç­”ã™ã‚‹\n"
                "- è‡ªç„¶ã§æ¸©ã‹ã„ä¼šè©±èª¿ã‚’å¿ƒãŒã‘ã‚‹\n"
                "- ä¼šè©±ã‚’å§‹ã‚ã‚‹ãŸã‚ã«1ã€œ2å€‹ã®è³ªå•ã‚’ã™ã‚‹\n"
                "- ç°¡æ½”ã ã‘ã©é­…åŠ›çš„ã«\n"
                "- JSONã‚„ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ã€ã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„\n"
            )
        
        # Add prior context if available
        if session.prior_context:
            system_content += session.prior_context
        
        # Add instruction for first response
        if language.lower() in ("english", "en"):
            system_content += (
                "\n\n### First Response Instructions\n"
                "This is the start of a new conversation. The user has just shared "
                "what they want to discuss. Respond naturally and warmly - acknowledge "
                "their input, show interest, and ask a relevant follow-up question to "
                "get the conversation started. Keep it conversational and friendly. "
                "Output ONLY natural conversational text."
            )
        else:
            system_content += (
                "\n\n### æœ€åˆã®å¿œç­”ã«é–¢ã™ã‚‹æŒ‡ç¤º\n"
                "ã“ã‚Œã¯æ–°ã—ã„ä¼šè©±ã®é–‹å§‹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—ãŸã„ã“ã¨ã‚’ä¼ãˆã¦ãã¾ã—ãŸã€‚"
                "è‡ªç„¶ã§æ¸©ã‹ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å—ã‘æ­¢ã‚ã€é–¢å¿ƒã‚’ç¤ºã—ã€"
                "ä¼šè©±ã‚’å§‹ã‚ã‚‹ãŸã‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚"
                "ä¼šè©±èª¿ã§ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã«ã€ã‹ã¤ç°¡æ½”ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
                "è‡ªç„¶ãªä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            )
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input},
        ]
        
        try:
            # Simple chat completion without tools for the opening
            response = await self._llm.chat(messages=messages)  # type: ignore
            return response.content or ""
        except Exception as e:
            logger.warning(f"Failed to generate opening response via LLM: {e}")
            # Fallback to a simple acknowledgment
            if language.lower() in ("english", "en"):
                return "I'd be happy to discuss that with you. What would you like to start with?"
            return "æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ã©ã®ã‚ˆã†ãªã“ã¨ã‹ã‚‰ãŠè©±ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"
    
    def _format_prior_knowledge(self, items: list[KnowledgeItem], language: str) -> str:
        """Format knowledge items into context string.
        
        Args:
            items: List of knowledge items.
            language: Language for formatting.
            
        Returns:
            Formatted context string.
        """
        if not items:
            return ""
        
        if language.lower() in ("english", "en"):
            header = "\n\n### Related Past Knowledge\n\nThe following are relevant insights from previous conversations:\n\n"
            item_format = "- {subject} {predicate}: {object} (recorded on {date})\n"
        else:
            header = "\n\n### é–¢é€£ã™ã‚‹éå»ã®çŸ¥è­˜\n\nä»¥ä¸‹ã¯éå»ã®å¯¾è©±ã§å¾—ã‚‰ã‚ŒãŸé–¢é€£æƒ…å ±ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’è¸ã¾ãˆã¦ä¼šè©±ã—ã¦ãã ã•ã„ï¼š\n\n"
            item_format = "- {subject}ã¯{predicate}ï¼š{object}ï¼ˆ{date}ã«è¨˜éŒ²ï¼‰\n"
        
        context = header
        for item in items:
            date_str = item.created_at.strftime("%Yå¹´%mæœˆ%dæ—¥") if language.lower() not in ("english", "en") else item.created_at.strftime("%Y-%m-%d")
            context += item_format.format(
                subject=item.subject,
                predicate=item.predicate,
                object=item.object,
                date=date_str,
            )
        
        return context

    async def resume_session(
        self,
        session_id: str,
        user_id: str,
    ) -> tuple[str, str, list[KnowledgeItem]]:
        """Resume an existing session from Neo4j.

        Args:
            session_id: Session ID to resume.
            user_id: User ID (for verification).

        Returns:
            Tuple of (session_id, resume_message, prior_insights).

        Raises:
            ValueError: If session not found or user mismatch.
        """
        # First check if session is already in memory (with conversation history)
        existing_session = self._sessions.get(session_id)
        if existing_session is not None and existing_session.user_id == user_id:
            # Session is already loaded with conversation history
            logger.info(f"Resuming session {session_id} from memory ({len(existing_session.turns)} turns)")
            
            # Count pending questions
            pending_count = sum(
                1 for q in existing_session.pending_questions
                if q.status == QuestionStatus.PENDING
            )
            
            # Generate resume message
            lang = existing_session.language
            if lang.lower() in ("english", "en"):
                resume_msg = (
                    f"Welcome back! We were discussing \"{existing_session.topic}\". "
                    f"You have {len(existing_session.turns)} exchanges so far. "
                )
                if pending_count > 0:
                    resume_msg += f"You have {pending_count} unanswered questions from our last conversation. "
                resume_msg += "Let's continue!"
            else:
                resume_msg = (
                    f"ã€Œ{existing_session.topic}ã€ã®ç¶šãã§ã™ã­ã€‚\n"
                    f"ã“ã‚Œã¾ã§{len(existing_session.turns)}ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ãŒã‚ã‚Šã¾ã—ãŸã€‚\n"
                )
                if pending_count > 0:
                    resume_msg += f"å‰å›ã®æœªå›ç­”è³ªå•ãŒ{pending_count}ä»¶ã‚ã‚Šã¾ã™ã€‚\n"
                resume_msg += "ç¶šã‘ã¾ã—ã‚‡ã†ï¼"
            
            return session_id, resume_msg, existing_session.prior_knowledge
        
        # Session not in memory, load from Neo4j
        if self._kg_store is None:
            raise ValueError("Knowledge graph store not available")

        # Get session metadata from Neo4j
        metadata = await self._kg_store.get_session_metadata(session_id)
        if metadata is None:
            raise ValueError(f"Session {session_id} not found")
        
        if metadata.user_id != user_id:
            raise ValueError("User ID mismatch")

        # Get insights saved during this session
        session_insights = await self._kg_store.get_session_insights(session_id)

        # Detect language from topic
        detected_lang = detect_language(metadata.topic)

        # Create new session object with prior context
        session = Session(
            id=metadata.id,
            user_id=metadata.user_id,
            topic=metadata.topic,
            language=detected_lang,
            domain=metadata.domain,
            created_at=metadata.created_at,
            updated_at=metadata.updated_at,
        )

        # Set prior context from session insights
        if session_insights:
            session.prior_knowledge = session_insights
            session.prior_context = self._format_prior_knowledge(session_insights, detected_lang)

        # Load conversation history from Neo4j
        conversation_turns = await self._kg_store.get_conversation_history(session_id)
        for turn_data in conversation_turns:
            # Restore turn to session
            turn = ConversationTurn(
                user_message=turn_data["user_message"],
                assistant_response=turn_data["assistant_response"],
                timestamp=turn_data["timestamp"],
            )
            session.turns.append(turn)
            # Also restore to message_history for LLM context
            session.message_history.append({"role": "user", "content": turn_data["user_message"]})
            session.message_history.append({"role": "assistant", "content": turn_data["assistant_response"]})

        # Restore pending questions from Neo4j
        restored_questions: list[PendingQuestion] = []
        try:
            restored_questions = await self._kg_store.get_unresolved_questions_for_session(session_id)
            if restored_questions:
                session.pending_questions = restored_questions
                logger.info(f"Restored {len(restored_questions)} pending questions for session {session_id}")
        except Exception as e:
            logger.warning(f"Failed to restore pending questions: {e}")

        # Store in active sessions
        self._sessions[session_id] = session

        # Generate resume message
        has_history = len(conversation_turns) > 0
        pending_count = len(restored_questions)

        if detected_lang.lower() in ("english", "en"):
            if has_history:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"Your previous {len(conversation_turns)} exchanges have been restored. "
                )
                if pending_count > 0:
                    resume_msg += f"You have {pending_count} unanswered questions from our last conversation. "
                resume_msg += "Let's continue!"
            else:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"(Note: No conversation history found. Starting fresh but with your insights!)"
                )
        else:
            if has_history:
                resume_msg = (
                    f"ã€Œ{metadata.topic}ã€ã®ç¶šãã§ã™ã­ã€‚ãŠå¸°ã‚Šãªã•ã„ï¼\n"
                    f"å‰å›ã®{len(conversation_turns)}ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚\n"
                )
                if pending_count > 0:
                    resume_msg += f"å‰å›ã®æœªå›ç­”è³ªå•ãŒ{pending_count}ä»¶ã‚ã‚Šã¾ã™ã€‚\n"
                resume_msg += "ç¶šã‘ã¾ã—ã‚‡ã†ï¼"
            else:
                resume_msg = (
                    f"ã€Œ{metadata.topic}ã€ã®ç¶šãã§ã™ã­ã€‚ãŠå¸°ã‚Šãªã•ã„ï¼\n"
                    f"ï¼ˆâ€»ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€è¨˜éŒ²ã—ãŸäº‹å®Ÿã¯å¼•ãç¶™ã„ã§ã„ã¾ã™ï¼‰"
                )

        logger.info(f"Resumed session {session_id} for user {user_id} ({len(conversation_turns)} turns, {pending_count} pending questions restored)")

        return session_id, resume_msg, session_insights

    def get_session(self, session_id: str) -> Session | None:
        """Get a session by ID.

        Args:
            session_id: Session identifier.

        Returns:
            Session or None if not found.
        """
        return self._sessions.get(session_id)

    async def end_session(self, session_id: str, generate_summary: bool = True) -> tuple[Session | None, SessionSummary | None]:
        """End a session and optionally generate a summary.

        Args:
            session_id: Session identifier.
            generate_summary: Whether to generate and save a summary.

        Returns:
            Tuple of (ended session, generated summary or None).
        """
        session = self._sessions.pop(session_id, None)
        if session is None:
            return None, None

        summary = None
        if generate_summary and len(session.turns) > 0:
            # Generate summary
            summary = await self._generate_session_summary(session)
            
            # Save summary to KG
            if self._kg_store and summary:
                try:
                    await self._kg_store.save_session_summary(summary)
                    logger.info(f"Saved summary for session {session_id}")
                except Exception as e:
                    logger.warning(f"Failed to save session summary: {e}")

        # Update session status in KG
        if self._kg_store:
            try:
                await self._kg_store.update_session_status(session_id, "ended")
            except Exception as e:
                logger.warning(f"Failed to update session status: {e}")

        logger.info(f"Ended session {session_id}")
        return session, summary

    async def _generate_session_summary(self, session: Session) -> SessionSummary | None:
        """Generate a summary of the session using LLM.

        Args:
            session: The session to summarize.

        Returns:
            SessionSummary or None if generation fails.
        """
        if not session.turns:
            return None

        # Build conversation text
        conversation_text = ""
        for i, turn in enumerate(session.turns, 1):
            conversation_text += f"ã‚¿ãƒ¼ãƒ³{i}:\n"
            conversation_text += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {turn.user_message}\n"
            conversation_text += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {turn.assistant_response}\n\n"

        # Collect all insights from the session
        all_insights = []
        for turn in session.turns:
            for insight in turn.insights_saved:
                all_insights.append(f"{insight.subject} - {insight.predicate} - {insight.object}")

        insights_text = "\n".join(all_insights) if all_insights else "ãªã—"

        if session.language.lower() in ("english", "en"):
            system_prompt = """You are a conversation summarizer. Summarize the following conversation.

Output ONLY valid JSON with these fields:
- content: 2-3 sentence natural summary
- key_points: Array of important facts (max 5)
- topics: Array of main topics discussed
- entities_mentioned: Array of people, projects, places mentioned

Example:
{"content":"Discussed project A progress and confirmed the deadline.","key_points":["Deadline is Feb 28","Tanaka is responsible"],"topics":["Project A","Progress report"],"entities_mentioned":["Tanaka","Project A"]}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ä¼šè©±è¦ç´„è€…ã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- content: 2-3æ–‡ã®è‡ªç„¶ãªè¦ç´„
- key_points: é‡è¦ãªäº‹å®Ÿã®é…åˆ—ï¼ˆæœ€å¤§5ã¤ï¼‰
- topics: è©±ã—åˆã‚ã‚ŒãŸä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã®é…åˆ—
- entities_mentioned: è¨€åŠã•ã‚ŒãŸäººåã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€å ´æ‰€ãªã©ã®é…åˆ—

ä¾‹:
{"content":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã®é€²æ—ã«ã¤ã„ã¦è©±ã—åˆã„ã€ç· ã‚åˆ‡ã‚Šã‚’ç¢ºèªã—ãŸã€‚","key_points":["ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","ç”°ä¸­ã•ã‚“ãŒæ‹…å½“"],"topics":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","é€²æ—å ±å‘Š"],"entities_mentioned":["ç”°ä¸­ã•ã‚“","ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA"]}"""

        user_prompt = f"""ãƒˆãƒ”ãƒƒã‚¯: {session.topic}
ãƒ‰ãƒ¡ã‚¤ãƒ³: {session.domain.value}

ã€ä¼šè©±ã€‘
{conversation_text}

ã€æŠ½å‡ºã•ã‚ŒãŸæ´å¯Ÿã€‘
{insights_text}

ä¸Šè¨˜ã®ä¼šè©±ã‚’JSONå½¢å¼ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "{}"
            
            # Clean up response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            data = json.loads(content)
            
            summary = SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=data.get("content", "ä¼šè©±ã®è¦ç´„"),
                key_points=data.get("key_points", []),
                topics=data.get("topics", [session.topic]),
                entities_mentioned=data.get("entities_mentioned", []),
                turn_range=(1, len(session.turns)),
            )
            
            logger.info(f"Generated summary for session {session.id}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate session summary: {e}")
            # Return a basic summary on failure
            return SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=f"{session.topic}ã«ã¤ã„ã¦ã®ä¼šè©±ï¼ˆ{len(session.turns)}ã‚¿ãƒ¼ãƒ³ï¼‰",
                key_points=[],
                topics=[session.topic],
                entities_mentioned=[],
                turn_range=(1, len(session.turns)),
            )

    async def respond(
        self,
        session_id: str,
        user_message: str,
    ) -> AgentResponse:
        """Generate a response to a user message.

        Enhanced flow:
        1. Retrieve pending questions from session
        2. Analyze if user message answers any pending questions
        3. Apply answers to knowledge graph automatically
        4. Detect new consistency issues and missing information
        5. Generate response with question context
        6. Track questions asked/answered in the turn

        Args:
            session_id: Session identifier.
            user_message: The user's message.

        Returns:
            AgentResponse with the assistant's message and metadata.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        from datetime import datetime

        # ===== Step 1: Analyze answers to pending questions =====
        answered_question_ids: list[str] = []
        existing_pending = [q for q in session.pending_questions if q.status == QuestionStatus.PENDING]

        if existing_pending:
            answer_analyses = await self._analyze_answer_to_questions(
                user_message=user_message,
                pending_questions=existing_pending,
                language=session.language,
            )

            # ===== Step 2: Apply answers to knowledge graph =====
            pending_map = {q.id: q for q in existing_pending}
            for analysis in answer_analyses:
                qid = analysis["question_id"]
                pq = pending_map.get(qid)
                if pq and analysis.get("answered"):
                    # Update question status
                    pq.status = QuestionStatus.ANSWERED
                    pq.answer = analysis.get("new_value") or user_message
                    pq.answered_at = datetime.now()
                    answered_question_ids.append(qid)

                    # Apply to knowledge graph
                    await self._apply_answer_to_knowledge(analysis, pq, session_id)

                    logger.info(f"Question {qid} answered: action={analysis.get('action')}")
                elif pq and analysis.get("action") == "skip":
                    pq.status = QuestionStatus.SKIPPED
                    answered_question_ids.append(qid)

        # ===== Step 3: Pre-search knowledge graph =====
        pre_search_knowledge: list[KnowledgeItem] = []
        consistency_issues: list[ConsistencyIssue] = []
        new_pending_questions: list[PendingQuestion] = []
        consistency_context = ""
        questions_context = ""
        chunk_context = ""
        relevant_chunks: list[DocumentChunk] = []
        
        if self._kg_store:
            try:
                # Extract dates from user message for temporal filtering
                start_date, end_date = extract_dates_from_text(user_message)
                
                # If a date is detected, save it to session for future reference
                if start_date:
                    if start_date not in session.referenced_dates:
                        session.referenced_dates.append(start_date)
                        logger.info(f"Added date to session context: {start_date}")
                
                # If no date in current message, use previously referenced dates
                if not start_date and session.referenced_dates:
                    start_date = session.referenced_dates[-1]
                    end_date = start_date
                    logger.info(f"Using previously referenced date: {start_date}")
                
                if start_date or end_date:
                    logger.info(f"Detected date reference: {start_date} to {end_date}")
                    
                    if start_date and end_date and start_date == end_date:
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    elif start_date and end_date:
                        relevant_chunks = await self._kg_store.get_chunks_by_date_range(start_date, end_date)
                    elif start_date:
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    
                    if relevant_chunks:
                        chunk_context = self._format_chunk_context(relevant_chunks, session.language)
                        logger.info(f"Found {len(relevant_chunks)} relevant chunks for date query")
                        logger.info("Skipping fact search - using chunk original content as authoritative source")
                    else:
                        date_filtered_knowledge = await self._kg_store.search_by_date_range(
                            start_date=start_date,
                            end_date=end_date,
                            query=user_message,
                            limit=5,
                        )
                        pre_search_knowledge.extend(date_filtered_knowledge)
                
                if not relevant_chunks:
                    keyword_knowledge = await self._kg_store.search(
                        user_message,
                        limit=5,
                        start_date=start_date,
                        end_date=end_date,
                    )
                    
                    seen_ids = {k.id for k in pre_search_knowledge}
                    for item in keyword_knowledge:
                        if item.id not in seen_ids:
                            pre_search_knowledge.append(item)
                            seen_ids.add(item.id)

                # ===== Step 4: Detect new questions (contradictions + missing info) =====
                detected_domain = self._detect_domain(user_message, [])
                new_pending_questions, consistency_issues = await self._generate_all_questions(
                    user_message=user_message,
                    knowledge_used=pre_search_knowledge,
                    extracted_facts=None,
                    domain=detected_domain,
                    language=session.language,
                    session_id=session_id,
                )

                # If consistency issues found, add context for LLM
                if consistency_issues:
                    consistency_context = self._format_consistency_context(
                        consistency_issues, session.language
                    )
                    logger.info(f"Found {len(consistency_issues)} consistency issues to address")

            except Exception as e:
                logger.warning(f"Pre-search for consistency failed: {e}")

        # ===== Step 5: Build pending questions context for LLM =====
        # Combine remaining unanswered questions + new questions
        still_pending = [q for q in session.pending_questions if q.status == QuestionStatus.PENDING]
        all_active_questions = still_pending + new_pending_questions

        # Sort by priority and deduplicate
        seen_question_texts = set()
        deduped_questions: list[PendingQuestion] = []
        for q in sorted(all_active_questions, key=lambda x: x.priority, reverse=True):
            if q.question not in seen_question_texts:
                deduped_questions.append(q)
                seen_question_texts.add(q.question)
        all_active_questions = deduped_questions

        if all_active_questions:
            questions_context = self._format_questions_context(all_active_questions, session.language)

        # ===== Step 6: Build messages for LLM =====
        system_content = get_system_prompt(session.language)
        if session.prior_context:
            system_content += session.prior_context
        
        if chunk_context:
            system_content += chunk_context
        
        # Inject pre-searched knowledge (extracted facts) as reference context
        if pre_search_knowledge:
            knowledge_context = self._format_knowledge_context(
                pre_search_knowledge, session.language
            )
            system_content += knowledge_context
        
        if consistency_context:
            system_content += consistency_context

        if questions_context:
            system_content += questions_context
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
        ]

        messages.extend(session.message_history)
        messages.append({"role": "user", "content": user_message})

        # Set up tool executor with session_id for insight tracking
        tool_executor = ToolExecutor(self._kg_store, session_id=session_id)
        tool_executor.used_knowledge.extend(pre_search_knowledge)

        # Call LLM with tools
        response_text, tool_results = await self._llm.chat_with_tools(
            messages=messages,  # type: ignore
            tools=ALL_TOOLS,
            tool_handlers=tool_executor.get_tool_handlers(),
        )

        # Detect domain from response and tool usage
        detected_domain = self._detect_domain(user_message, tool_results)

        # ===== Step 7: Update session state =====
        # Mark asked_at for new questions
        questions_asked_ids: list[str] = []
        for q in new_pending_questions:
            q.asked_at = datetime.now()
            questions_asked_ids.append(q.id)

        # Update session's pending_questions: keep answered/skipped for history, add new ones
        session.pending_questions = [
            q for q in session.pending_questions
        ] + new_pending_questions

        # Create conversation turn with question tracking
        turn = ConversationTurn(
            user_message=user_message,
            assistant_response=response_text,
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
            questions_asked=questions_asked_ids,
            questions_answered=answered_question_ids,
        )
        session.add_turn(turn)

        # Update session metadata in Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to update session metadata: {e}")

        # Save conversation turn to Neo4j for persistence
        if self._kg_store:
            try:
                await self._kg_store.save_conversation_turn(
                    session_id=session_id,
                    turn_index=len(session.turns) - 1,
                    user_message=user_message,
                    assistant_response=response_text,
                    timestamp=turn.timestamp,
                )
            except Exception as e:
                logger.warning(f"Failed to save conversation turn: {e}")

        # Build tool calls list for response
        tool_calls = [
            {
                "id": tr.get("tool_call_id", ""),
                "name": tr.get("name", ""),
                "arguments": tr.get("arguments", {}),
            }
            for tr in tool_results
        ]

        # ===== Step 8: Persist questions to KG =====
        if self._kg_store and new_pending_questions:
            try:
                await self._kg_store.save_pending_questions_batch(
                    new_pending_questions, session_id
                )
            except Exception as e:
                logger.warning(f"Failed to persist pending questions: {e}")

        # Persist answered question status updates
        if self._kg_store and answered_question_ids:
            for qid in answered_question_ids:
                try:
                    pq = next((q for q in session.pending_questions if q.id == qid), None)
                    if pq:
                        await self._kg_store.update_question_status(
                            question_id=qid,
                            status=pq.status.value,
                            answer=pq.answer,
                        )
                except Exception as e:
                    logger.warning(f"Failed to update question {qid} status in KG: {e}")

        logger.info(
            f"Session {session_id}: processed message, "
            f"saved {len(tool_executor.saved_insights)} insights, "
            f"used {len(tool_executor.used_knowledge)} knowledge items, "
            f"detected {len(consistency_issues)} consistency issues, "
            f"new questions {len(new_pending_questions)}, "
            f"answered {len(answered_question_ids)}"
        )

        return AgentResponse(
            message=response_text,
            tool_calls=tool_calls,  # type: ignore
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
            consistency_issues=consistency_issues,
            pending_questions=[q for q in all_active_questions if q.status == QuestionStatus.PENDING],
            questions_answered=answered_question_ids,
        )

    def _format_knowledge_context(
        self,
        knowledge: list[KnowledgeItem],
        language: str,
    ) -> str:
        """Format pre-searched knowledge items as context for LLM.

        Injects extracted facts from documents and prior sessions so the LLM
        can directly reference them without needing to call a tool.

        Args:
            knowledge: List of relevant knowledge items from pre-search.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not knowledge:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### ğŸ“š Related Knowledge (from documents and prior sessions)\n"
            context += "The following facts were found in the knowledge base. Use them to answer accurately.\n\n"
        else:
            context = "\n\n### ğŸ“š é–¢é€£ã™ã‚‹çŸ¥è­˜ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»éå»ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æŠ½å‡ºï¼‰\n"
            context += "ä»¥ä¸‹ã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§è¦‹ã¤ã‹ã£ãŸé–¢é€£ãƒ•ã‚¡ã‚¯ãƒˆã§ã™ã€‚å›ç­”ã®å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚\n\n"

        for i, item in enumerate(knowledge, 1):
            date_str = ""
            if item.event_date:
                date_str = f" [{item.event_date.strftime('%Y-%m-%d')}"
                if item.event_date_end and item.event_date_end != item.event_date:
                    date_str += f"ã€œ{item.event_date_end.strftime('%Y-%m-%d')}"
                date_str += "]"
            context += f"- **{item.subject}** {item.predicate} **{item.object}**{date_str} (ä¿¡é ¼åº¦: {item.confidence:.0%})\n"

        context += "\n"
        return context

    def _format_chunk_context(
        self,
        chunks: list[DocumentChunk],
        language: str,
    ) -> str:
        """Format document chunks as context for LLM.

        Phase 2: Provides the original document content for accurate reference.

        Args:
            chunks: List of relevant document chunks.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not chunks:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### ğŸ”’ AUTHORITATIVE DOCUMENT CONTENT (Must Quote Exactly)\n"
            context += "The following is the EXACT original content from the user's uploaded documents.\n"
            context += "**CRITICAL RULES:**\n"
            context += "1. Quote EXACTLY what is written - do not paraphrase or summarize\n"
            context += "2. If document says 'ã‚«ã‚¨ãƒ« å‰', say 'ã‚«ã‚¨ãƒ« å‰' - NOT 'ã‚«ã‚¨ãƒ«ï¼ˆå‰/å¾Œï¼‰'\n"
            context += "3. Do NOT add information that is not in the document\n"
            context += "4. Do NOT guess or infer - only state what is explicitly written\n\n"
        else:
            context = "\n\n### ğŸ”’ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸæ–‡ï¼ˆæ­£ç¢ºã«å¼•ç”¨ã™ã‚‹ã“ã¨ï¼‰\n"
            context += "ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®**åŸæ–‡ãã®ã¾ã¾**ã§ã™ã€‚\n"
            context += "**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«ï¼š**\n"
            context += "1. æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã‚’**ä¸€å­—ä¸€å¥ãã®ã¾ã¾**å¼•ç”¨ã™ã‚‹ã“ã¨\n"
            context += "2. ä¾‹ï¼šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã€Œã‚«ã‚¨ãƒ« å‰ã€ã¨ã‚ã‚Œã°ã€Œã‚«ã‚¨ãƒ« å‰ã€ã¨å›ç­” - ã€Œã‚«ã‚¨ãƒ«ï¼ˆå‰/å¾Œï¼‰ã€ã¯âŒ\n"
            context += "3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ›¸ã‹ã‚Œã¦ã„ãªã„æƒ…å ±ã‚’è¿½åŠ ã—ãªã„ã“ã¨\n"
            context += "4. æ¨æ¸¬ã‚„è£œå®Œã¯çµ¶å¯¾ã«ã—ãªã„ã“ã¨ - æ˜ç¤ºçš„ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã“ã¨ã®ã¿å›ç­”\n\n"

        for chunk in chunks:
            # Add date header if available
            if chunk.chunk_date:
                date_str = chunk.chunk_date.strftime("%Yå¹´%mæœˆ%dæ—¥") if language.lower() not in ("english", "en") else chunk.chunk_date.strftime("%Y-%m-%d")
                context += f"---\nğŸ“… {date_str}"
                if chunk.heading:
                    context += f" - {chunk.heading}"
                context += "\n\n"
            elif chunk.heading:
                context += f"---\nğŸ“„ {chunk.heading}\n\n"
            else:
                context += "---\n\n"

            # Add the original content (preserved exactly as uploaded)
            context += "ã€åŸæ–‡ã“ã“ã‹ã‚‰ã€‘\n"
            context += chunk.content
            context += "\nã€åŸæ–‡ã“ã“ã¾ã§ã€‘\n\n"

        context += "---\n"
        context += "ä¸Šè¨˜ã®åŸæ–‡ã‹ã‚‰ã€è³ªå•ã«è©²å½“ã™ã‚‹éƒ¨åˆ†ã‚’ãã®ã¾ã¾å¼•ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        
        return context

    def _format_consistency_context(
        self,
        issues: list[ConsistencyIssue],
        language: str,
    ) -> str:
        """Format consistency issues as context for LLM.

        Args:
            issues: List of detected consistency issues.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not issues:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### IMPORTANT: Consistency Issues Detected\n"
            context += "The user's current message appears to contradict or change previous information.\n"
            context += "**You MUST address these in your response** - ask for clarification naturally.\n\n"
            
            for issue in issues:
                issue_type = "CONTRADICTION" if issue.kind.value == "contradiction" else "CHANGE"
                context += f"- [{issue_type}] {issue.title}\n"
                context += f"  Previously: \"{issue.previous_text}\"\n"
                context += f"  Now: \"{issue.current_text}\"\n"
                context += f"  Suggested question: \"{issue.suggested_question}\"\n\n"
            
            context += "Address this naturally by asking about the change/contradiction.\n"
        else:
            context = "\n\n### é‡è¦ï¼šæ•´åˆæ€§ã®å•é¡Œã‚’æ¤œå‡º\n"
            context += "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéå»ã®æƒ…å ±ã¨çŸ›ç›¾ã¾ãŸã¯å¤‰æ›´ãŒã‚ã‚Šã¾ã™ã€‚\n"
            context += "**å¿…ãšã“ã®ç‚¹ã«ã¤ã„ã¦ç¢ºèªã—ã¦ãã ã•ã„** - è‡ªç„¶ãªå½¢ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚\n\n"
            
            for issue in issues:
                issue_type = "çŸ›ç›¾" if issue.kind.value == "contradiction" else "å¤‰æ›´"
                context += f"- ã€{issue_type}ã€‘{issue.title}\n"
                context += f"  ä»¥å‰ï¼šã€Œ{issue.previous_text}ã€\n"
                context += f"  ä»Šå›ï¼šã€Œ{issue.current_text}ã€\n"
                context += f"  ç¢ºèªã™ã¹ãè³ªå•ï¼šã€Œ{issue.suggested_question}ã€\n\n"
            
            context += "ã“ã®å¤‰æ›´/çŸ›ç›¾ã«ã¤ã„ã¦è‡ªç„¶ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"

        return context

    def _format_questions_context(
        self,
        questions: list[PendingQuestion],
        language: str,
    ) -> str:
        """Format pending questions as context for LLM.

        Args:
            questions: List of active pending questions.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not questions:
            return ""

        # Only include top priority questions (max 5)
        top_questions = questions[:5]

        if language.lower() in ("english", "en"):
            context = "\n\n### Pending Questions to Address\n"
            context += "The following questions are waiting for user response.\n"
            context += "If the user's message answers any of these, acknowledge it. "
            context += "Otherwise, naturally incorporate the highest-priority unanswered question.\n\n"

            for q in top_questions:
                kind_label = {
                    QuestionKind.CONTRADICTION: "CONTRADICTION",
                    QuestionKind.CHANGE: "CHANGE",
                    QuestionKind.MISSING: "MISSING INFO",
                    QuestionKind.CLARIFICATION: "CLARIFICATION",
                }.get(q.kind, "QUESTION")
                context += f"- [{kind_label}] {q.question}\n"
                if q.context:
                    context += f"  Context: {q.context}\n"
        else:
            context = "\n\n### æœªå›ç­”ã®è³ªå•\n"
            context += "ä»¥ä¸‹ã®è³ªå•ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å›ç­”ã‚’å¾…ã£ã¦ã„ã¾ã™ã€‚\n"
            context += "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã“ã‚Œã‚‰ã«å›ç­”ã—ã¦ã„ã‚‹å ´åˆã¯ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
            context += "ãã†ã§ãªã‘ã‚Œã°ã€å„ªå…ˆåº¦ã®é«˜ã„æœªå›ç­”è³ªå•ã‚’è‡ªç„¶ã«ä¼šè©±ã«çµ„ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚\n\n"

            for q in top_questions:
                kind_label = {
                    QuestionKind.CONTRADICTION: "çŸ›ç›¾",
                    QuestionKind.CHANGE: "å¤‰æ›´",
                    QuestionKind.MISSING: "ä¸è¶³æƒ…å ±",
                    QuestionKind.CLARIFICATION: "ç¢ºèª",
                }.get(q.kind, "è³ªå•")
                context += f"- ã€{kind_label}ã€‘{q.question}\n"
                if q.context:
                    context += f"  èƒŒæ™¯: {q.context}\n"

        return context

    def _detect_domain(
        self,
        user_message: str,
        tool_results: list[dict[str, Any]],
    ) -> Domain:
        """Detect the conversation domain.

        Args:
            user_message: The user's message.
            tool_results: Results from tool calls.

        Returns:
            Detected domain.
        """
        text_lower = user_message.lower()

        # Check for domain-specific keywords
        daily_work_keywords = [
            "ã‚¿ã‚¹ã‚¯",
            "task",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "project",
            "æ¥­å‹™",
            "work",
            "ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
            "meeting",
            "é€²æ—",
            "progress",
            "ä»Šæ—¥",
            "today",
            "æ˜æ—¥",
            "tomorrow",
            "ç· ã‚åˆ‡ã‚Š",
            "deadline",
            "pr",
            "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼",
            "ãƒ‡ãƒ—ãƒ­ã‚¤",
            "deploy",
        ]

        recipe_keywords = [
            "æ–™ç†",
            "cook",
            "ãƒ¬ã‚·ãƒ”",
            "recipe",
            "ææ–™",
            "ingredient",
            "èª¿ç†",
            "ç„¼ã",
            "ç…®ã‚‹",
            "ç‚’ã‚ã‚‹",
            "åˆ†é‡",
            "ã‚ªãƒ¼ãƒ–ãƒ³",
            "oven",
            "é‹",
            "ãƒ•ãƒ©ã‚¤ãƒ‘ãƒ³",
        ]

        postmortem_keywords = [
            "éšœå®³",
            "incident",
            "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ",
            "ãƒ€ã‚¦ãƒ³",
            "down",
            "å¾©æ—§",
            "recover",
            "æ ¹æœ¬åŸå› ",
            "root cause",
            "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
            "timeline",
            "å†ç™ºé˜²æ­¢",
            "ã‚¢ãƒ©ãƒ¼ãƒˆ",
            "alert",
            "ã‚¨ãƒ©ãƒ¼",
            "error",
        ]

        creative_keywords = [
            "ã‚¢ã‚¤ãƒ‡ã‚¢",
            "idea",
            "å‰µä½œ",
            "creative",
            "ãƒ‡ã‚¶ã‚¤ãƒ³",
            "design",
            "ã‚¤ãƒ©ã‚¹ãƒˆ",
            "illustration",
            "ç‰©èª",
            "story",
            "éŸ³æ¥½",
            "music",
            "ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            "inspiration",
        ]

        # Check save_insight tool calls for domain hints
        for result in tool_results:
            if result.get("name") == "save_insight":
                args = result.get("arguments", {})
                if "domain" in args:
                    try:
                        return Domain(args["domain"])
                    except ValueError:
                        pass

        # Check keywords
        if any(kw in text_lower for kw in daily_work_keywords):
            return Domain.DAILY_WORK
        if any(kw in text_lower for kw in recipe_keywords):
            return Domain.RECIPE
        if any(kw in text_lower for kw in postmortem_keywords):
            return Domain.POSTMORTEM
        if any(kw in text_lower for kw in creative_keywords):
            return Domain.CREATIVE

        return Domain.GENERAL

    async def get_session_summary(self, session_id: str) -> dict[str, Any]:
        """Get a summary of the session.

        Args:
            session_id: Session identifier.

        Returns:
            Dict with session summary.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        all_insights: list[Insight] = []
        all_knowledge: list[KnowledgeItem] = []

        for turn in session.turns:
            all_insights.extend(turn.insights_saved)
            all_knowledge.extend(turn.knowledge_used)

        return {
            "session_id": session.id,
            "topic": session.topic,
            "language": session.language,
            "domain": session.domain.value,
            "turn_count": len(session.turns),
            "insights_saved": len(all_insights),
            "knowledge_used": len(all_knowledge),
            "created_at": session.created_at.isoformat(),
            "updated_at": session.updated_at.isoformat(),
        }

    async def _detect_consistency_issues(
        self,
        user_message: str,
        knowledge_used: list[KnowledgeItem],
        language: str,
        session_id: str | None = None,
    ) -> list[ConsistencyIssue]:
        """Detect potential consistency issues between user message and past knowledge.

        Args:
            user_message: Current user message.
            knowledge_used: Knowledge items retrieved for this turn.
            language: Session language.
            session_id: Session ID for persistence.

        Returns:
            List of detected consistency issues.
        """
        if not knowledge_used:
            return []

        # Build context for LLM to analyze, including fact_id for matching
        knowledge_context = "\n".join([
            f"- [ID:{item.id}] {item.subject}ã¯{item.predicate} ã€Œ{item.object}ã€"
            for item in knowledge_used
        ])
        
        # Create a lookup map for fact_id matching
        knowledge_map = {item.id: item for item in knowledge_used}

        if language.lower() in ("english", "en"):
            system_prompt = """You are a consistency checker. Analyze if the user's current message contradicts or changes any past recorded information.

**CRITICAL: Distinguish between "contradiction" and "change":**
- "contradiction": Impossible to be both true (e.g., "A is responsible" vs "B is responsible for the same task")
- "change": Information has been updated/revised (e.g., deadline moved from date A to date B)

Output ONLY valid JSON array. Each item should have:
- kind: "contradiction" or "change" (use "contradiction" when both cannot be true simultaneously)
- fact_id: The ID from the past record (e.g., "abc-123")
- title: Brief title in 5 words or less
- previous_text: What was recorded before
- current_text: What user says now
- suggested_question: Question to clarify

If no issues found, output empty array: []

Contradiction example:
[{"kind":"contradiction","fact_id":"abc-123","title":"Different person responsible","previous_text":"Tanaka is responsible","current_text":"Yamada is responsible","suggested_question":"Previously Tanaka was mentioned as responsible. Has this changed to Yamada?"}]

Change example:
[{"kind":"change","fact_id":"abc-123","title":"Deadline moved","previous_text":"Deadline is March 31","current_text":"Deadline is February 28","suggested_question":"The deadline was March 31 before. Has it been moved to February 28?"}]"""
        else:
            system_prompt = """ã‚ãªãŸã¯æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéå»ã®è¨˜éŒ²ã¨çŸ›ç›¾ã¾ãŸã¯å¤‰åŒ–ã—ã¦ã„ã‚‹ã‹åˆ†æã—ã¦ãã ã•ã„ã€‚

**é‡è¦ï¼šã€ŒçŸ›ç›¾ã€ã¨ã€Œå¤‰æ›´ã€ã‚’åŒºåˆ¥ã—ã¦ãã ã•ã„ï¼š**
- "contradiction"ï¼ˆçŸ›ç›¾ï¼‰: ä¸¡æ–¹ãŒåŒæ™‚ã«çœŸã§ã‚ã‚‹ã“ã¨ãŒä¸å¯èƒ½ï¼ˆä¾‹ï¼šã€ŒAã•ã‚“ãŒæ‹…å½“ã€vsã€ŒåŒã˜ã‚¿ã‚¹ã‚¯ã‚’Bã•ã‚“ãŒæ‹…å½“ã€ï¼‰
- "change"ï¼ˆå¤‰æ›´ï¼‰: æƒ…å ±ãŒæ›´æ–°ãƒ»ä¿®æ­£ã•ã‚ŒãŸï¼ˆä¾‹ï¼šç· ã‚åˆ‡ã‚ŠãŒæ—¥ä»˜Aã‹ã‚‰æ—¥ä»˜Bã«å¤‰æ›´ï¼‰

æœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„é …ç›®ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¾ã™ï¼š
- kind: "contradiction" ã¾ãŸã¯ "change"ï¼ˆä¸¡æ–¹ãŒåŒæ™‚ã«çœŸã§ã‚ã‚Šå¾—ãªã„å ´åˆã¯ "contradiction"ï¼‰
- fact_id: éå»ã®è¨˜éŒ²ã®IDï¼ˆä¾‹: "abc-123"ï¼‰
- title: 5èªä»¥å†…ã®ç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«
- previous_text: ä»¥å‰è¨˜éŒ²ã•ã‚Œã¦ã„ãŸå†…å®¹
- current_text: ç¾åœ¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€ã£ã¦ã„ã‚‹å†…å®¹
- suggested_question: ç¢ºèªã®ãŸã‚ã®è³ªå•

å•é¡ŒãŒãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’å‡ºåŠ›: []

çŸ›ç›¾ã®ä¾‹:
[{"kind":"contradiction","fact_id":"abc-123","title":"æ‹…å½“è€…ãŒç•°ãªã‚‹","previous_text":"ç”°ä¸­ã•ã‚“ãŒæ‹…å½“","current_text":"å±±ç”°ã•ã‚“ãŒæ‹…å½“","suggested_question":"ä»¥å‰ã¯ç”°ä¸­ã•ã‚“ãŒæ‹…å½“ã¨ã®ã“ã¨ã§ã—ãŸãŒã€å±±ç”°ã•ã‚“ã«å¤‰æ›´ã«ãªã‚Šã¾ã—ãŸã‹ï¼Ÿ"}]

å¤‰æ›´ã®ä¾‹:
[{"kind":"change","fact_id":"abc-123","title":"ç· ã‚åˆ‡ã‚Šå¤‰æ›´","previous_text":"ç· ã‚åˆ‡ã‚Šã¯3æœˆ31æ—¥","current_text":"ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","suggested_question":"ä»¥å‰ã¯3æœˆ31æ—¥ãŒç· ã‚åˆ‡ã‚Šã§ã—ãŸãŒã€2æœˆ28æ—¥ã«å¤‰æ›´ã«ãªã‚Šã¾ã—ãŸã‹ï¼Ÿ"}]"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"éå»ã®è¨˜éŒ²:\n{knowledge_context}\n\nç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:\n{user_message}"},
        ]

        try:
            import json
            from uuid import uuid4
            
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "[]"
            
            # Clean up response (remove markdown code blocks if present)
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            # Parse JSON
            issues_data = json.loads(content)
            if not isinstance(issues_data, list):
                return []

            issues = []
            for item in issues_data:
                try:
                    kind = ConsistencyIssueKind(item.get("kind", "change"))
                    fact_id = item.get("fact_id")
                    
                    # Validate fact_id exists in our knowledge
                    if fact_id and fact_id not in knowledge_map:
                        fact_id = None  # Invalid ID, set to None
                    
                    # Create issue with ID and session_id
                    issue = ConsistencyIssue(
                        id=str(uuid4()),
                        kind=kind,
                        title=item.get("title", "æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"),
                        fact_id=fact_id,
                        previous_text=item.get("previous_text", ""),
                        previous_source="éå»ã®è¨˜éŒ²",
                        current_text=item.get("current_text", ""),
                        current_source="ç¾åœ¨ã®ä¼šè©±",
                        suggested_question=item.get("suggested_question", ""),
                        confidence=0.7,
                        session_id=session_id,
                    )
                    issues.append(issue)
                    
                    # Persist to knowledge graph
                    if self._kg_store:
                        try:
                            await self._kg_store.save_consistency_issue(issue, session_id)
                            logger.info(f"Saved consistency issue {issue.id} to KG")
                        except Exception as save_error:
                            logger.warning(f"Failed to save consistency issue to KG: {save_error}")
                            
                except Exception as e:
                    logger.warning(f"Failed to parse consistency issue: {e}")
                    continue

            if issues:
                logger.info(f"Detected {len(issues)} consistency issues")

            return issues

        except Exception as e:
            logger.warning(f"Failed to detect consistency issues: {e}")
            return []

    def _consistency_issue_to_pending_question(
        self,
        issue: ConsistencyIssue,
        session_id: str | None = None,
    ) -> PendingQuestion:
        """Convert a ConsistencyIssue to a PendingQuestion for unified tracking.

        Args:
            issue: The consistency issue to convert.
            session_id: Session ID for the question.

        Returns:
            A PendingQuestion instance.
        """
        kind_map = {
            ConsistencyIssueKind.CONTRADICTION: QuestionKind.CONTRADICTION,
            ConsistencyIssueKind.CHANGE: QuestionKind.CHANGE,
        }
        return PendingQuestion(
            id=issue.id or str(uuid.uuid4()),
            kind=kind_map.get(issue.kind, QuestionKind.CONTRADICTION),
            question=issue.suggested_question,
            context=f"ä»¥å‰: ã€Œ{issue.previous_text}ã€ â†’ ç¾åœ¨: ã€Œ{issue.current_text}ã€",
            related_fact_id=issue.fact_id,
            related_entity=None,
            priority=8 if issue.kind == ConsistencyIssueKind.CONTRADICTION else 5,
            status=QuestionStatus.PENDING,
            session_id=session_id,
            created_at=issue.created_at,
        )

    async def _detect_missing_information(
        self,
        extracted_facts: list[ExtractedFact] | None,
        user_message: str,
        existing_knowledge: list[KnowledgeItem],
        domain: Domain,
        language: str,
        session_id: str | None = None,
    ) -> list[PendingQuestion]:
        """Detect missing information that should be asked about.

        Analyzes extracted facts and existing knowledge to identify gaps.
        Uses LLM to determine what information is expected but missing.

        Args:
            extracted_facts: Facts extracted from a document or message.
            user_message: The current user message for context.
            existing_knowledge: Previously stored knowledge items.
            domain: The detected domain of the conversation.
            language: Session language.
            session_id: Session ID for persistence.

        Returns:
            List of PendingQuestion objects for missing information.
        """
        # Build context from facts and existing knowledge
        facts_text = ""
        if extracted_facts:
            facts_text = "\n".join([
                f"- {f.subject}ã¯{f.predicate}ã€Œ{f.object}ã€"
                for f in extracted_facts
            ])

        knowledge_text = ""
        if existing_knowledge:
            knowledge_text = "\n".join([
                f"- [ID:{item.id}] {item.subject}ã¯{item.predicate}ã€Œ{item.object}ã€"
                for item in existing_knowledge
            ])

        if language.lower() in ("english", "en"):
            system_prompt = """You are a missing information detector. Analyze the given facts and existing knowledge to identify important gaps.

Think about what information SHOULD exist but is missing. Consider:
- If a project is mentioned, do we know: responsible person, deadline, status, priority?
- If a person is mentioned, do we know: role, team, contact info?
- If an event is mentioned, do we know: date, location, participants, outcome?
- If a decision is mentioned, do we know: reason, alternatives considered, who decided?
- If a problem is mentioned, do we know: impact, root cause, resolution plan?

Only suggest questions for GENUINELY USEFUL missing information. Don't ask trivial questions.

Output ONLY valid JSON array. Each item should have:
- kind: "missing" or "clarification"
- question: The question to ask (natural, conversational)
- context: Why this information would be useful
- related_entity: The entity this question is about (or null)
- priority: 1-10 (10 = most important)

If no important information is missing, output: []

Example:
[{"kind":"missing","question":"Who is responsible for Project A?","context":"Project A was mentioned but no responsible person is assigned","related_entity":"Project A","priority":7}]"""
        else:
            system_prompt = """ã‚ãªãŸã¯ä¸è¶³æƒ…å ±ã®æ¤œå‡ºè€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸäº‹å®Ÿã¨æ—¢å­˜ã®çŸ¥è­˜ã‚’åˆ†æã—ã€é‡è¦ãªæƒ…å ±ã®æ¬ è½ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

å­˜åœ¨ã™ã¹ãã ãŒä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ã‚’è€ƒãˆã¦ãã ã•ã„ï¼š
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼šæ‹…å½“è€…ã€ç· ã‚åˆ‡ã‚Šã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å„ªå…ˆåº¦ã¯ã‚ã‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- äººç‰©ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼šå½¹å‰²ã€ãƒãƒ¼ãƒ ã€é€£çµ¡å…ˆã¯ã‚ã‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- ã‚¤ãƒ™ãƒ³ãƒˆãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼šæ—¥æ™‚ã€å ´æ‰€ã€å‚åŠ è€…ã€çµæœã¯ã‚ã‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- æ±ºå®šäº‹é …ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼šç†ç”±ã€æ¤œè¨ã—ãŸä»£æ›¿æ¡ˆã€èª°ãŒæ±ºã‚ãŸã‹ã¯ã‚ã‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- å•é¡ŒãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼šå½±éŸ¿ç¯„å›²ã€æ ¹æœ¬åŸå› ã€è§£æ±ºè¨ˆç”»ã¯ã‚ã‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿ

**æœ¬å½“ã«æœ‰ç”¨ãª**ä¸è¶³æƒ…å ±ã®ã¿è³ªå•ã—ã¦ãã ã•ã„ã€‚äº›æœ«ãªè³ªå•ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

æœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„é …ç›®ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¾ã™ï¼š
- kind: "missing" ã¾ãŸã¯ "clarification"
- question: è³ªå•æ–‡ï¼ˆè‡ªç„¶ãªä¼šè©±èª¿ã§ï¼‰
- context: ã“ã®æƒ…å ±ãŒãªãœæœ‰ç”¨ã‹
- related_entity: è³ªå•ãŒé–¢é€£ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆã¾ãŸã¯nullï¼‰
- priority: 1ã€œ10ï¼ˆ10ãŒæœ€ã‚‚é‡è¦ï¼‰

é‡è¦ãªä¸è¶³æƒ…å ±ãŒãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’å‡ºåŠ›: []

ä¾‹:
[{"kind":"missing","question":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã®æ‹…å½“è€…ã¯èª°ã§ã™ã‹ï¼Ÿ","context":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAãŒè¨€åŠã•ã‚Œã¦ã„ã¾ã™ãŒæ‹…å½“è€…ãŒä¸æ˜","related_entity":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","priority":7}]"""

        user_prompt_parts = []
        if facts_text:
            user_prompt_parts.append(f"ã€æ–°ã—ã„äº‹å®Ÿã€‘\n{facts_text}")
        if knowledge_text:
            user_prompt_parts.append(f"ã€æ—¢å­˜ã®çŸ¥è­˜ã€‘\n{knowledge_text}")
        if user_message:
            user_prompt_parts.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘\n{user_message}")
        user_prompt_parts.append(f"ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã€‘\n{domain.value}")

        user_prompt = "\n\n".join(user_prompt_parts) + "\n\nä¸Šè¨˜ã®æƒ…å ±ã‹ã‚‰ã€ä¸è¶³ã—ã¦ã„ã‚‹é‡è¦ãªæƒ…å ±ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            from datetime import datetime

            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "[]"

            # Clean up response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()

            items_data = json.loads(content)
            if not isinstance(items_data, list):
                return []

            questions: list[PendingQuestion] = []
            for item in items_data:
                try:
                    kind_str = item.get("kind", "missing")
                    kind = QuestionKind.MISSING if kind_str == "missing" else QuestionKind.CLARIFICATION

                    question = PendingQuestion(
                        id=str(uuid.uuid4()),
                        kind=kind,
                        question=item.get("question", ""),
                        context=item.get("context", ""),
                        related_fact_id=None,
                        related_entity=item.get("related_entity"),
                        priority=min(10, max(1, int(item.get("priority", 5)))),
                        status=QuestionStatus.PENDING,
                        session_id=session_id,
                        created_at=datetime.now(),
                    )
                    questions.append(question)
                except Exception as e:
                    logger.warning(f"Failed to parse missing info question: {e}")
                    continue

            if questions:
                logger.info(f"Detected {len(questions)} missing information questions")

            return questions

        except Exception as e:
            logger.warning(f"Failed to detect missing information: {e}")
            return []

    async def _analyze_answer_to_questions(
        self,
        user_message: str,
        pending_questions: list[PendingQuestion],
        language: str,
    ) -> list[dict]:
        """Analyze user's message to determine if it answers any pending questions.

        Uses LLM to match user responses to pending questions and determine
        what knowledge updates should be made.

        Args:
            user_message: The user's message.
            pending_questions: List of currently pending questions.
            language: Session language.

        Returns:
            List of dicts with keys:
                - question_id: ID of the answered question
                - answered: True if the question was answered
                - action: "accept_current" | "keep_previous" | "new_value" | "skip"
                - new_value: The new value from the user's answer (if any)
                - confidence: How confident we are in this analysis
        """
        if not pending_questions:
            return []

        # Build question list for LLM
        questions_text = "\n".join([
            f"- [ID:{q.id}] (ç¨®é¡:{q.kind.value}) {q.question}\n  èƒŒæ™¯: {q.context}"
            for q in pending_questions
        ])

        if language.lower() in ("english", "en"):
            system_prompt = """You are a response analyzer. Determine if the user's message answers any of the pending questions.

For each question that is addressed by the user's message, provide:
- question_id: The ID of the question being answered
- answered: true if the user provided an answer, false if unclear
- action: 
  - "accept_current" = user confirms the new/current information is correct
  - "keep_previous" = user says the old information was correct  
  - "new_value" = user provides a completely different value
  - "skip" = user explicitly skips/ignores the question
- new_value: The specific value from the user's answer (null if not applicable)
- confidence: 0.0-1.0 how confident you are

Output ONLY valid JSON array. Only include questions that are actually addressed. If no questions are answered, output: []

Example:
[{"question_id":"abc-123","answered":true,"action":"accept_current","new_value":null,"confidence":0.9}]"""
        else:
            system_prompt = """ã‚ãªãŸã¯å›ç­”åˆ†æè€…ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæœªå›ç­”ã®è³ªå•ã«ç­”ãˆã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¯¾å¿œã—ã¦ã„ã‚‹å„è³ªå•ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
- question_id: å›ç­”ã•ã‚Œã¦ã„ã‚‹è³ªå•ã®ID
- answered: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå›ç­”ã‚’æä¾›ã—ãŸå ´åˆtrueã€ä¸æ˜ç¢ºãªå ´åˆfalse
- action:
  - "accept_current" = ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ–°ã—ã„/ç¾åœ¨ã®æƒ…å ±ãŒæ­£ã—ã„ã¨ç¢ºèª
  - "keep_previous" = ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¤ã„æƒ…å ±ãŒæ­£ã—ã„ã¨å›ç­”
  - "new_value" = ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã¾ã£ãŸãç•°ãªã‚‹å€¤ã‚’æä¾›
  - "skip" = ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«è³ªå•ã‚’ã‚¹ã‚­ãƒƒãƒ—/ç„¡è¦–
- new_value: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸå…·ä½“çš„ãªå€¤ï¼ˆè©²å½“ã—ãªã„å ´åˆã¯nullï¼‰
- confidence: 0.0ã€œ1.0ã§ç¢ºä¿¡åº¦

æœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å®Ÿéš›ã«å¯¾å¿œã•ã‚Œã¦ã„ã‚‹è³ªå•ã®ã¿å«ã‚ã¦ãã ã•ã„ã€‚
å›ç­”ãŒãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’å‡ºåŠ›: []

ä¾‹:
[{"question_id":"abc-123","answered":true,"action":"accept_current","new_value":null,"confidence":0.9}]"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ã€æœªå›ç­”ã®è³ªå•ã€‘\n{questions_text}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘\n{user_message}"},
        ]

        try:
            import json

            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "[]"

            # Clean up response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()

            results = json.loads(content)
            if not isinstance(results, list):
                return []

            # Validate question_ids
            valid_ids = {q.id for q in pending_questions}
            validated_results = []
            for r in results:
                if r.get("question_id") in valid_ids:
                    validated_results.append({
                        "question_id": r["question_id"],
                        "answered": bool(r.get("answered", False)),
                        "action": r.get("action", "accept_current"),
                        "new_value": r.get("new_value"),
                        "confidence": min(1.0, max(0.0, float(r.get("confidence", 0.7)))),
                    })

            if validated_results:
                logger.info(f"Analyzed {len(validated_results)} answers from user message")

            return validated_results

        except Exception as e:
            logger.warning(f"Failed to analyze answer to questions: {e}")
            return []

    async def _apply_answer_to_knowledge(
        self,
        answer_analysis: dict,
        pending_question: PendingQuestion,
        session_id: str,
    ) -> None:
        """Apply a user's answer to update the knowledge graph.

        Args:
            answer_analysis: Result from _analyze_answer_to_questions for one question.
            pending_question: The question that was answered.
            session_id: Session ID for tracking.
        """
        if not self._kg_store:
            return

        action = answer_analysis.get("action", "")
        new_value = answer_analysis.get("new_value")
        from datetime import datetime

        try:
            if pending_question.kind in (QuestionKind.CONTRADICTION, QuestionKind.CHANGE):
                # Handle contradiction/change resolution
                fact_id = pending_question.related_fact_id
                if fact_id:
                    if action == "accept_current":
                        # Update the fact to the new value
                        await self._kg_store.update_fact(
                            fact_id=fact_id,
                            new_value=new_value or pending_question.context.split("â†’")[-1].strip().strip("ã€Œã€ "),
                            source=f"session:{session_id}",
                        )
                        await self._kg_store.set_fact_status(fact_id, "active")
                        logger.info(f"Updated fact {fact_id}: accepted current value")
                    elif action == "keep_previous":
                        # Keep existing fact, mark as active
                        await self._kg_store.set_fact_status(fact_id, "active")
                        logger.info(f"Kept previous value for fact {fact_id}")
                    elif action == "new_value" and new_value:
                        # Set a completely new value
                        await self._kg_store.update_fact(
                            fact_id=fact_id,
                            new_value=new_value,
                            source=f"session:{session_id}",
                        )
                        await self._kg_store.set_fact_status(fact_id, "active")
                        logger.info(f"Set new value for fact {fact_id}: {new_value}")

            elif pending_question.kind == QuestionKind.MISSING and new_value:
                # Save new insight for missing information
                from el_core.schemas import Insight
                insight = Insight(
                    subject=pending_question.related_entity or "unknown",
                    predicate="has_info",
                    object=new_value,
                    confidence=0.9,
                    domain=Domain.GENERAL,
                )
                await self._kg_store.save_insight(insight, session_id=session_id)
                logger.info(f"Saved new insight from missing info answer: {new_value[:50]}")

            elif pending_question.kind == QuestionKind.CLARIFICATION and new_value:
                # Save clarification as insight
                from el_core.schemas import Insight
                insight = Insight(
                    subject=pending_question.related_entity or "unknown",
                    predicate="clarification",
                    object=new_value,
                    confidence=0.85,
                    domain=Domain.GENERAL,
                )
                await self._kg_store.save_insight(insight, session_id=session_id)
                logger.info(f"Saved clarification: {new_value[:50]}")

        except Exception as e:
            logger.warning(f"Failed to apply answer to knowledge: {e}")

    async def _generate_all_questions(
        self,
        user_message: str,
        knowledge_used: list[KnowledgeItem],
        extracted_facts: list[ExtractedFact] | None,
        domain: Domain,
        language: str,
        session_id: str | None = None,
    ) -> tuple[list[PendingQuestion], list[ConsistencyIssue]]:
        """Generate all questions (contradictions, changes, missing info) in a unified manner.

        Combines consistency issue detection and missing information detection,
        returning a unified list of PendingQuestion sorted by priority.

        Args:
            user_message: Current user message.
            knowledge_used: Knowledge items retrieved for this turn.
            extracted_facts: Facts extracted from document or message (optional).
            domain: Detected domain.
            language: Session language.
            session_id: Session ID for persistence.

        Returns:
            Tuple of (all PendingQuestions sorted by priority, raw ConsistencyIssues).
        """
        all_questions: list[PendingQuestion] = []
        consistency_issues: list[ConsistencyIssue] = []

        # 1. Detect consistency issues (contradictions + changes)
        if knowledge_used:
            consistency_issues = await self._detect_consistency_issues(
                user_message=user_message,
                knowledge_used=knowledge_used,
                language=language,
                session_id=session_id,
            )
            # Convert to PendingQuestion
            for issue in consistency_issues:
                pq = self._consistency_issue_to_pending_question(issue, session_id)
                all_questions.append(pq)

        # 2. Detect missing information
        missing_questions = await self._detect_missing_information(
            extracted_facts=extracted_facts,
            user_message=user_message,
            existing_knowledge=knowledge_used,
            domain=domain,
            language=language,
            session_id=session_id,
        )
        all_questions.extend(missing_questions)

        # Sort by priority (highest first)
        all_questions.sort(key=lambda q: q.priority, reverse=True)

        logger.info(
            f"Generated {len(all_questions)} total questions: "
            f"{len(consistency_issues)} consistency + {len(missing_questions)} missing"
        )

        return all_questions, consistency_issues

    # ==================== Document Processing ====================

    async def extract_from_document(
        self,
        content: str,
        filename: str,
        language: str = "Japanese",
    ) -> DocumentExtractionResult:
        """Extract structured information from document content using LLM.

        Args:
            content: The parsed text content of the document.
            filename: Original filename for context.
            language: Language for extraction prompts.

        Returns:
            DocumentExtractionResult with summary, facts, topics, entities.
        """
        # Note: Content should already be truncated by caller (process_document_background)
        # This is a safety limit for direct calls - GPT-5.2 can handle 128k+ tokens
        max_chars = 100000
        if len(content) > max_chars:
            # Keep start 20% + end 80% to prioritize recent content
            start_chars = int(max_chars * 0.2)
            end_chars = max_chars - start_chars - 100
            content = (
                content[:start_chars] 
                + f"\n\n[... ä¸­é–“ ç´„{len(content) - start_chars - end_chars:,}æ–‡å­—çœç•¥ ...]\n\n" 
                + content[-end_chars:]
            )

        if language.lower() in ("english", "en"):
            system_prompt = """You are a document analyzer. Extract key information from the document, paying special attention to temporal (date/time) information.

Output ONLY valid JSON with these fields:
- summary: 2-3 sentence summary of the document
- facts: Array of factual statements as objects with:
  - subject: The subject entity
  - predicate: The relationship/attribute
  - object: The value/content
  - source_context: Brief context from document (max 50 chars)
  - event_date: Date when this event occurred (YYYY-MM-DD format, null if unknown)
  - event_date_end: End date for date ranges (YYYY-MM-DD format, null if not a range)
  - date_type: One of "exact", "approximate", "range", "unknown"
- topics: Array of main topics/themes
- entities: Array of mentioned people, organizations, projects, places
- domain: One of "daily_work", "recipe", "postmortem", "creative", "general"

IMPORTANT: Extract ALL date information carefully. For each fact:
- If an exact date is mentioned (e.g., "May 1, 2024"), use date_type: "exact"
- If approximate (e.g., "around May", "early 2024"), use date_type: "approximate"
- If a range (e.g., "May 1-15"), use date_type: "range" and set event_date_end
- If no date context, use date_type: "unknown"

Example:
{"summary":"Project status report showing 80% completion with deadline Feb 28, 2024.","facts":[{"subject":"Project A","predicate":"completion rate","object":"80%","source_context":"According to the report","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"Project A","predicate":"deadline","object":"Feb 28, 2024","source_context":"Confirmed in section 3","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["Project Management","Progress Report"],"entities":["Project A","Tanaka"],"domain":"daily_work"}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æè€…ã§ã™ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ç‰¹ã«æ—¥ä»˜ãƒ»æ™‚é–“æƒ…å ±ã«æ³¨æ„ã‚’æ‰•ã£ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- summary: 2-3æ–‡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ç´„
- facts: äº‹å®Ÿã®é…åˆ—ï¼ˆå„é …ç›®ã¯ä»¥ä¸‹ã®å½¢å¼ï¼‰
  - subject: ä¸»èªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
  - predicate: é–¢ä¿‚ãƒ»å±æ€§
  - object: å€¤ãƒ»å†…å®¹
  - source_context: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ç°¡æ½”ãªæ–‡è„ˆï¼ˆæœ€å¤§50æ–‡å­—ï¼‰
  - event_date: ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸæ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ã€ä¸æ˜ãªã‚‰nullï¼‰
  - event_date_end: æœŸé–“ã®çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ç¯„å›²ã§ãªã‘ã‚Œã°nullï¼‰
  - date_type: "exact"ï¼ˆæ­£ç¢ºï¼‰, "approximate"ï¼ˆç´„ï¼‰, "range"ï¼ˆæœŸé–“ï¼‰, "unknown"ï¼ˆä¸æ˜ï¼‰ã®ã„ãšã‚Œã‹
- topics: ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ãƒ»ãƒ†ãƒ¼ãƒã®é…åˆ—
- entities: è¨€åŠã•ã‚ŒãŸäººç‰©ã€çµ„ç¹”ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€å ´æ‰€ã®é…åˆ—
- domain: "daily_work", "recipe", "postmortem", "creative", "general" ã®ã„ãšã‚Œã‹

é‡è¦ï¼šã™ã¹ã¦ã®æ—¥ä»˜æƒ…å ±ã‚’æ³¨æ„æ·±ãæŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
- æ­£ç¢ºãªæ—¥ä»˜ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šã€Œ2024å¹´5æœˆ1æ—¥ã€ï¼‰â†’ date_type: "exact"
- æ›–æ˜§ãªæ—¥ä»˜ã®å ´åˆï¼ˆä¾‹ï¼šã€Œ5æœˆé ƒã€ã€Œ2024å¹´åˆã‚ã€ï¼‰â†’ date_type: "approximate"
- æœŸé–“ã®å ´åˆï¼ˆä¾‹ï¼šã€Œ5æœˆ1æ—¥ã€œ15æ—¥ã€ï¼‰â†’ date_type: "range"ã€event_date_endã‚’è¨­å®š
- æ—¥ä»˜ã®æ–‡è„ˆãŒãªã„å ´åˆ â†’ date_type: "unknown"

ä¾‹:
{"summary":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—ãƒ¬ãƒãƒ¼ãƒˆã€‚å®Œäº†ç‡80%ã€ç· ã‚åˆ‡ã‚Šã¯2024å¹´2æœˆ28æ—¥ã€‚","facts":[{"subject":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","predicate":"å®Œäº†ç‡","object":"80%","source_context":"ãƒ¬ãƒãƒ¼ãƒˆã«ã‚ˆã‚‹ã¨","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","predicate":"ç· ã‚åˆ‡ã‚Š","object":"2024å¹´2æœˆ28æ—¥","source_context":"ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ç¢ºèª","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†","é€²æ—å ±å‘Š"],"entities":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","ç”°ä¸­ã•ã‚“"],"domain":"daily_work"}"""

        user_prompt = f"""ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}

ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã€‘
{content}

ä¸Šè¨˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã€JSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        result_content = ""
        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                # Handle ```json or ``` prefix
                first_line_end = result_content.find("\n")
                if first_line_end > 0:
                    result_content = result_content[first_line_end + 1:]
                else:
                    result_content = result_content[3:]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            logger.debug(f"LLM extraction response for {filename} (first 500 chars): {result_content[:500]}")

            if not result_content or result_content == "{}":
                logger.warning(f"LLM returned empty response for document: {filename}")
                return DocumentExtractionResult(
                    summary=f"LLMãŒç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã—ã¾ã—ãŸ: {filename}",
                    facts=[],
                    topics=[],
                    entities=[],
                    domain=Domain.GENERAL,
                )

            data = json.loads(result_content)

            # Validate that we got meaningful data
            raw_facts = data.get("facts", [])
            raw_summary = data.get("summary", "")

            if not raw_facts and not raw_summary:
                logger.warning(
                    f"LLM returned JSON with no facts and no summary for document: {filename}. "
                    f"Keys in response: {list(data.keys())}"
                )

            # Parse facts with date information
            facts = []
            skipped_facts = 0
            for f in raw_facts:
                if isinstance(f, dict) and "subject" in f and "predicate" in f and "object" in f:
                    # Parse event_date
                    event_date = None
                    if f.get("event_date"):
                        try:
                            from datetime import datetime
                            event_date = datetime.strptime(f["event_date"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse event_date_end
                    event_date_end = None
                    if f.get("event_date_end"):
                        try:
                            from datetime import datetime
                            event_date_end = datetime.strptime(f["event_date_end"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse date_type
                    date_type_str = f.get("date_type", "unknown")
                    try:
                        date_type = DateType(date_type_str)
                    except ValueError:
                        date_type = DateType.UNKNOWN
                    
                    facts.append(ExtractedFact(
                        subject=f["subject"],
                        predicate=f["predicate"],
                        object=f["object"],
                        source_context=f.get("source_context", ""),
                        event_date=event_date,
                        event_date_end=event_date_end,
                        date_type=date_type,
                    ))
                else:
                    skipped_facts += 1

            if skipped_facts > 0:
                logger.warning(
                    f"Skipped {skipped_facts} malformed facts from document: {filename}. "
                    f"Total valid: {len(facts)}"
                )

            # Parse domain
            domain_str = data.get("domain", "general")
            try:
                domain = Domain(domain_str)
            except ValueError:
                domain = Domain.GENERAL

            # Use meaningful summary, not the placeholder default
            summary = raw_summary if raw_summary else f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ '{filename}' ã‚’åˆ†æã—ã¾ã—ãŸã€‚"

            result = DocumentExtractionResult(
                summary=summary,
                facts=facts,
                topics=data.get("topics", []),
                entities=data.get("entities", []),
                domain=domain,
            )

            logger.info(
                f"Extracted from document '{filename}': "
                f"{len(facts)} facts, {len(result.topics)} topics, "
                f"{len(result.entities)} entities, domain={domain.value}, "
                f"summary='{summary[:80]}...'"
            )
            return result

        except json.JSONDecodeError as e:
            logger.error(
                f"Failed to parse LLM JSON response for document '{filename}': {e}. "
                f"Response content (first 300 chars): {result_content[:300]}"
            )
            return DocumentExtractionResult(
                summary=f"{filename} ã®è§£æã§JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                facts=[],
                topics=[],
                entities=[],
                domain=Domain.GENERAL,
            )
        except Exception as e:
            logger.error(f"Failed to extract from document '{filename}': {e}", exc_info=True)
            return DocumentExtractionResult(
                summary=f"{filename} ã®è§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                facts=[],
                topics=[],
                entities=[],
                domain=Domain.GENERAL,
            )

    async def create_document_review_session(
        self,
        user_id: str,
        document_id: str,
        document_filename: str,
        extracted_facts: list[ExtractedFact],
        language: str | None = None,
    ) -> tuple[str, str, list[ConsistencyIssue]]:
        """Create a session for reviewing uploaded document and checking consistency.
        
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«ã€æŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿã¨éå»ã®çŸ¥è­˜ã‚’ç…§åˆã—ã€
        çŸ›ç›¾ç‚¹ã‚„ä¸è¶³æƒ…å ±ã«ã¤ã„ã¦ç¢ºèªã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚
        
        Args:
            user_id: User identifier.
            document_id: Document ID that was uploaded.
            document_filename: Original filename.
            extracted_facts: Facts extracted from the document.
            language: Language preference. Auto-detected if None.
            
        Returns:
            Tuple of (session_id, opening_message, consistency_issues).
        """
        detected_lang = language or "Japanese"
        session_id = str(uuid.uuid4())
        
        # Create session with document review topic
        topic = f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€Œ{document_filename}ã€ã®ç¢ºèª"
        session = Session(
            id=session_id,
            user_id=user_id,
            topic=topic,
            language=detected_lang,
        )
        
        # Check each extracted fact against existing knowledge
        consistency_issues: list[ConsistencyIssue] = []
        related_knowledge: list[KnowledgeItem] = []
        
        if self._kg_store and extracted_facts:
            for fact in extracted_facts:
                # Search for related knowledge
                search_query = f"{fact.subject} {fact.predicate}"
                related = await self._kg_store.search(
                    search_query,
                    limit=5,
                    start_date=fact.event_date,
                    end_date=fact.event_date_end,
                )
                related_knowledge.extend(related)
                
                # Check for consistency issues
                if related:
                    fact_text = f"{fact.subject}ã¯{fact.predicate}ã€Œ{fact.object}ã€"
                    issues = await self._detect_consistency_issues(
                        user_message=fact_text,
                        knowledge_used=related,
                        language=detected_lang,
                        session_id=session_id,
                    )
                    consistency_issues.extend(issues)
        
        # Store related knowledge in session
        session.prior_knowledge = related_knowledge
        session.prior_context = self._format_prior_knowledge(related_knowledge, detected_lang)
        
        # Store document_id in session metadata (we'll add a field for this)
        self._sessions[session_id] = session
        
        # Save session metadata to Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
                # Link session to document
                await self._kg_store.link_session_to_document(session_id, document_id)
            except Exception as e:
                logger.warning(f"Failed to save session metadata: {e}")
        
        # Generate opening message with consistency issues
        opening = await self._generate_document_review_opening(
            session=session,
            document_filename=document_filename,
            extracted_facts=extracted_facts,
            consistency_issues=consistency_issues,
            language=detected_lang,
        )
        
        logger.info(
            f"Created document review session {session_id} for document {document_id}: "
            f"{len(extracted_facts)} facts, {len(consistency_issues)} issues"
        )
        
        return session_id, opening, consistency_issues
    
    async def _generate_document_review_opening(
        self,
        session: Session,
        document_filename: str,
        extracted_facts: list[ExtractedFact],
        consistency_issues: list[ConsistencyIssue],
        language: str,
    ) -> str:
        """Generate opening message for document review session.
        
        Args:
            session: The session object.
            document_filename: Name of the uploaded document.
            extracted_facts: Facts extracted from the document.
            consistency_issues: Detected consistency issues.
            language: Session language.
            
        Returns:
            Opening message for the review session.
        """
        if language.lower() in ("english", "en"):
            base_message = (
                f"I've processed the document \"{document_filename}\" and extracted "
                f"{len(extracted_facts)} facts. "
            )
            
            if consistency_issues:
                base_message += (
                    f"I found {len(consistency_issues)} potential inconsistencies "
                    f"with your previous records. Let me ask you about these:\n\n"
                )
                for i, issue in enumerate(consistency_issues[:5], 1):  # Limit to 5
                    base_message += f"{i}. {issue.suggested_question}\n"
                if len(consistency_issues) > 5:
                    base_message += f"\n... and {len(consistency_issues) - 5} more issues.\n"
            else:
                base_message += (
                    "I've checked these facts against your previous records and "
                    "they appear consistent. Is there anything you'd like to clarify or add?"
                )
        else:
            base_message = (
                f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€Œ{document_filename}ã€ã‚’å‡¦ç†ã—ã€"
                f"{len(extracted_facts)}ä»¶ã®äº‹å®Ÿã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚\n\n"
            )
            
            if consistency_issues:
                base_message += (
                    f"éå»ã®è¨˜éŒ²ã¨ç…§åˆã—ãŸã¨ã“ã‚ã€{len(consistency_issues)}ä»¶ã®"
                    f"çŸ›ç›¾ç‚¹ã‚„å¤‰æ›´ç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ç¢ºèªã•ã›ã¦ãã ã•ã„ï¼š\n\n"
                )
                for i, issue in enumerate(consistency_issues[:5], 1):  # æœ€å¤§5ä»¶ã¾ã§è¡¨ç¤º
                    base_message += f"{i}. {issue.suggested_question}\n"
                if len(consistency_issues) > 5:
                    base_message += f"\n... ä»–{len(consistency_issues) - 5}ä»¶ã®ç¢ºèªäº‹é …ãŒã‚ã‚Šã¾ã™ã€‚\n"
            else:
                base_message += (
                    "éå»ã®è¨˜éŒ²ã¨ç…§åˆã—ã¾ã—ãŸãŒã€ç‰¹ã«çŸ›ç›¾ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                    "è¿½åŠ ã§ç¢ºèªã—ãŸã„ã“ã¨ã‚„è£œè¶³ã—ãŸã„æƒ…å ±ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
                )
        
        return base_message

    async def generate_topic_summary(
        self,
        topic: str,
        facts: list[KnowledgeItem],
        language: str = "Japanese",
    ) -> TopicSummary:
        """Generate a comprehensive summary for a topic based on accumulated facts.

        Args:
            topic: The topic/entity name.
            facts: List of facts related to the topic.
            language: Language for the summary.

        Returns:
            TopicSummary with comprehensive overview.
        """
        if not facts:
            return TopicSummary(
                topic=topic,
                summary=f"{topic}ã«é–¢ã™ã‚‹è¨˜éŒ²ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚",
                key_points=[],
                related_entities=[],
                fact_count=0,
            )

        # Format facts for LLM
        facts_text = "\n".join([
            f"- {f.subject}ã¯{f.predicate}ã€Œ{f.object}ã€({f.created_at.strftime('%Y-%m-%d')})"
            for f in facts
        ])

        # Extract unique entities
        all_entities = set()
        for f in facts:
            all_entities.add(f.subject)
            all_entities.add(f.object)
        all_entities.discard(topic)

        # Get time range
        dates = [f.created_at for f in facts]
        time_range = f"{min(dates).strftime('%Y-%m-%d')} ï½ {max(dates).strftime('%Y-%m-%d')}"

        if language.lower() in ("english", "en"):
            system_prompt = """You are a knowledge synthesizer. Create a comprehensive summary of the topic based on the facts provided.

Output ONLY valid JSON with these fields:
- summary: 3-5 sentence comprehensive overview
- key_points: Array of the most important points (max 7)
- insights: Array of insights or patterns you noticed
- questions: Array of follow-up questions that might be useful

Example:
{"summary":"Project A has been progressing steadily...","key_points":["Deadline is Feb 28","80% complete","Tanaka leading"],"insights":["Progress accelerated in January","Risk of deadline slip"],"questions":["What are the remaining tasks?","Any blockers?"]}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ãƒŠãƒ¬ãƒƒã‚¸çµ±åˆè€…ã§ã™ã€‚æä¾›ã•ã‚ŒãŸäº‹å®Ÿã«åŸºã¥ã„ã¦ã€ãƒˆãƒ”ãƒƒã‚¯ã®åŒ…æ‹¬çš„ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- summary: 3-5æ–‡ã®åŒ…æ‹¬çš„ãªæ¦‚è¦
- key_points: æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®é…åˆ—ï¼ˆæœ€å¤§7ã¤ï¼‰
- insights: æ°—ã¥ã„ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„æ´å¯Ÿã®é…åˆ—
- questions: æœ‰ç”¨ã¨æ€ã‚ã‚Œã‚‹ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã®é…åˆ—

ä¾‹:
{"summary":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã¯é †èª¿ã«é€²è¡Œã—ã¦ãŠã‚Š...","key_points":["ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","å®Œäº†ç‡80%","ç”°ä¸­ã•ã‚“ãŒãƒªãƒ¼ãƒ‰"],"insights":["1æœˆã«é€²æ—ãŒåŠ é€Ÿ","ç· ã‚åˆ‡ã‚Šé…å»¶ã®ãƒªã‚¹ã‚¯ã‚ã‚Š"],"questions":["æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã¯ä½•ã‹ï¼Ÿ","ãƒ–ãƒ­ãƒƒã‚«ãƒ¼ã¯ã‚ã‚‹ã‹ï¼Ÿ"]}"""

        user_prompt = f"""ãƒˆãƒ”ãƒƒã‚¯: {topic}
é–¢é€£ã™ã‚‹äº‹å®Ÿæ•°: {len(facts)}
æœŸé–“: {time_range}

ã€è¨˜éŒ²ã•ã‚ŒãŸäº‹å®Ÿã€‘
{facts_text}

ä¸Šè¨˜ã®äº‹å®Ÿã‚’çµ±åˆã—ã€{topic}ã«ã¤ã„ã¦ã®åŒ…æ‹¬çš„ãªè¦ç´„ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            summary = TopicSummary(
                topic=topic,
                summary=data.get("summary", f"{topic}ã«ã¤ã„ã¦ã®è¦ç´„"),
                key_points=data.get("key_points", []) + data.get("insights", []),
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )

            logger.info(f"Generated summary for topic: {topic}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate topic summary: {e}")
            return TopicSummary(
                topic=topic,
                summary=f"{topic}ã«é–¢ã™ã‚‹{len(facts)}ä»¶ã®äº‹å®ŸãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                key_points=[f.object for f in facts[:5]],
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )

    # ==================== Tag Suggestion Methods ====================

    async def suggest_tags(
        self,
        content: str,
        existing_tags: list[Tag] | None = None,
        max_tags: int = 5,
        language: str | None = None,
    ) -> TagSuggestionResult:
        """Suggest tags for given content using LLM.

        Args:
            content: Content to analyze for tags.
            existing_tags: List of existing tags to consider for reuse.
            max_tags: Maximum number of tags to suggest.
            language: Language for prompts (auto-detect if None).

        Returns:
            TagSuggestionResult with suggested tags.
        """
        import json

        if existing_tags is None:
            existing_tags = await self._kg_store.get_all_tags(limit=100)

        lang = language or detect_language(content)

        # Format existing tags for the prompt
        existing_tags_str = ""
        if existing_tags:
            tag_names = [f"- {t.name}" + (f" (åˆ¥å: {', '.join(t.aliases[:3])})" if t.aliases else "") 
                        for t in existing_tags[:50]]
            existing_tags_str = "\n".join(tag_names)

        if lang == "Japanese":
            system_prompt = """ã‚ãªãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€é©åˆ‡ãªã‚¿ã‚°ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚°ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¤ã¹ãã§ã™ï¼š
- çŸ­ãç°¡æ½”ï¼ˆ1ã€œ3èªç¨‹åº¦ï¼‰
- å†…å®¹ã®æœ¬è³ªã‚’æ‰ãˆã¦ã„ã‚‹
- æ¤œç´¢ã‚„ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã«å½¹ç«‹ã¤
- æ—¢å­˜ã‚¿ã‚°ã§é©åˆ‡ãªã‚‚ã®ãŒã‚ã‚Œã°å„ªå…ˆçš„ã«å†åˆ©ç”¨

JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "tags": [
    {"name": "ã‚¿ã‚°å", "relevance": 0.9, "reason": "ã“ã®ã‚¿ã‚°ã‚’é¸ã‚“ã ç†ç”±", "is_existing": false}
  ],
  "content_summary": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç°¡æ½”ãªè¦ç´„"
}

relevanceã¯0.0ã€œ1.0ã§ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã®é–¢é€£åº¦ã‚’ç¤ºã—ã¾ã™ã€‚
is_existingã¯æ—¢å­˜ã‚¿ã‚°ãƒªã‚¹ãƒˆã‹ã‚‰é¸ã‚“ã å ´åˆtrueã€æ–°è¦ææ¡ˆã®å ´åˆfalseã«ã—ã¦ãã ã•ã„ã€‚"""

            user_prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«é©åˆ‡ãªã‚¿ã‚°ã‚’æœ€å¤§{max_tags}å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
{content[:2000]}

ã€æ—¢å­˜ã‚¿ã‚°ä¸€è¦§ã€‘
{existing_tags_str if existing_tags_str else "ï¼ˆã¾ã ã‚¿ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰"}

é©åˆ‡ãªæ—¢å­˜ã‚¿ã‚°ãŒã‚ã‚Œã°å„ªå…ˆçš„ã«ä½¿ç”¨ã—ã€å¿…è¦ã§ã‚ã‚Œã°æ–°ã—ã„ã‚¿ã‚°ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""

        else:
            system_prompt = """You are a content classification expert.
Analyze the given text and suggest appropriate tags.

Tags should be:
- Short and concise (1-3 words)
- Capture the essence of the content
- Useful for search and grouping
- Reuse existing tags when appropriate

Output in JSON format:
{
  "tags": [
    {"name": "tag name", "relevance": 0.9, "reason": "why this tag", "is_existing": false}
  ],
  "content_summary": "brief summary of content"
}

relevance is 0.0-1.0 indicating how related the tag is to the content.
is_existing should be true if selected from existing tags, false if newly suggested."""

            user_prompt = f"""Suggest up to {max_tags} appropriate tags for the following content.

ã€Contentã€‘
{content[:2000]}

ã€Existing Tagsã€‘
{existing_tags_str if existing_tags_str else "(No tags yet)"}

Prioritize existing tags when appropriate, and suggest new ones if needed."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            suggestions: list[TagSuggestion] = []
            existing_matched = 0
            new_suggested = 0

            for tag_data in data.get("tags", [])[:max_tags]:
                tag_name = tag_data.get("name", "").strip()
                if not tag_name:
                    continue

                is_existing = tag_data.get("is_existing", False)
                existing_tag_id = None

                # Check if this matches an existing tag
                if existing_tags:
                    for existing_tag in existing_tags:
                        if (existing_tag.name.lower() == tag_name.lower() or
                            any(alias.lower() == tag_name.lower() for alias in existing_tag.aliases)):
                            existing_tag_id = existing_tag.id
                            is_existing = True
                            tag_name = existing_tag.name  # Use canonical name
                            break

                if is_existing:
                    existing_matched += 1
                else:
                    new_suggested += 1

                suggestions.append(TagSuggestion(
                    name=tag_name,
                    relevance=min(1.0, max(0.0, tag_data.get("relevance", 0.8))),
                    reason=tag_data.get("reason", ""),
                    existing_tag_id=existing_tag_id,
                    is_new=not is_existing,
                ))

            return TagSuggestionResult(
                suggestions=suggestions,
                content_summary=data.get("content_summary", ""),
                existing_tags_matched=existing_matched,
                new_tags_suggested=new_suggested,
            )

        except Exception as e:
            logger.warning(f"Failed to suggest tags: {e}")
            return TagSuggestionResult(
                suggestions=[],
                content_summary="",
                existing_tags_matched=0,
                new_tags_suggested=0,
            )

    async def check_similar_tags(
        self,
        new_tag_name: str,
        existing_tags: list[Tag] | None = None,
        language: str | None = None,
    ) -> tuple[str | None, float]:
        """Check if a new tag is similar to existing tags.

        Args:
            new_tag_name: Name of the potential new tag.
            existing_tags: List of existing tags to compare.
            language: Language for prompts (auto-detect if None).

        Returns:
            Tuple of (existing_tag_id_to_merge_into, similarity_score).
            Returns (None, 0.0) if no similar tag found.
        """
        import json

        if existing_tags is None:
            existing_tags = await self._kg_store.get_all_tags(limit=100)

        if not existing_tags:
            return None, 0.0

        lang = language or detect_language(new_tag_name)

        tag_names_list = [f"{t.name} (ID: {t.id})" for t in existing_tags[:50]]
        tag_names_str = "\n".join(tag_names_list)

        if lang == "Japanese":
            system_prompt = """ã‚ãªãŸã¯ã‚¿ã‚°ã®é¡ä¼¼åº¦åˆ¤å®šã®å°‚é–€å®¶ã§ã™ã€‚
æ–°ã—ã„ã‚¿ã‚°ãŒæ—¢å­˜ã‚¿ã‚°ã¨æ„å‘³çš„ã«åŒã˜ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ï¼š
{
  "is_similar": true/false,
  "similar_tag_id": "é¡ä¼¼ã‚¿ã‚°ã®IDï¼ˆãªã‘ã‚Œã°nullï¼‰",
  "similarity_score": 0.9,
  "reason": "åˆ¤å®šç†ç”±"
}

similarity_scoreã¯0.0ã€œ1.0ã§ã€0.8ä»¥ä¸Šã‚’ã€ŒåŒç¾©èªã€ã¨ã¿ãªã—ã¾ã™ã€‚
ä¾‹: ã€Œæ—…è¡Œã€ã¨ã€Œãƒˆãƒ©ãƒ™ãƒ«ã€ã€ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã¨ã€Œã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ã¯åŒç¾©èªã§ã™ã€‚"""

            user_prompt = f"""æ–°è¦ã‚¿ã‚°å€™è£œ: ã€Œ{new_tag_name}ã€

ã€æ—¢å­˜ã‚¿ã‚°ä¸€è¦§ã€‘
{tag_names_str}

ã“ã®æ–°è¦ã‚¿ã‚°ã¯æ—¢å­˜ã‚¿ã‚°ã®ã„ãšã‚Œã‹ã¨åŒç¾©ã§ã™ã‹ï¼Ÿ"""

        else:
            system_prompt = """You are an expert in tag similarity assessment.
Determine if a new tag is semantically the same as existing tags.

Output in JSON format:
{
  "is_similar": true/false,
  "similar_tag_id": "ID of similar tag or null",
  "similarity_score": 0.9,
  "reason": "reason for judgment"
}

similarity_score is 0.0-1.0, with 0.8+ considered synonyms.
Example: "travel" and "trip", "programming" and "coding" are synonyms."""

            user_prompt = f"""New tag candidate: "{new_tag_name}"

ã€Existing Tagsã€‘
{tag_names_str}

Is this new tag synonymous with any existing tag?"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            is_similar = data.get("is_similar", False)
            similar_tag_id = data.get("similar_tag_id")
            similarity_score = data.get("similarity_score", 0.0)

            if is_similar and similar_tag_id and similarity_score >= 0.8:
                # Verify the tag ID exists
                for tag in existing_tags:
                    if tag.id == similar_tag_id:
                        logger.info(f"Tag '{new_tag_name}' is similar to '{tag.name}' (score: {similarity_score})")
                        return similar_tag_id, similarity_score
                # Tag ID not found, try to find by name match in reason
                logger.warning(f"Similar tag ID '{similar_tag_id}' not found in existing tags")

            return None, 0.0

        except Exception as e:
            logger.warning(f"Failed to check tag similarity: {e}")
            return None, 0.0

    async def auto_tag_content(
        self,
        content: str,
        max_tags: int = 5,
        min_relevance: float = 0.6,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag content by suggesting, checking similarity, and creating tags.

        Args:
            content: Content to tag.
            max_tags: Maximum number of tags.
            min_relevance: Minimum relevance score to include.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples for the created/matched tags.
        """
        # Get existing tags
        existing_tags = await self._kg_store.get_all_tags(limit=100)

        # Get tag suggestions
        result = await self.suggest_tags(
            content=content,
            existing_tags=existing_tags,
            max_tags=max_tags,
            language=language,
        )

        tags_with_relevance: list[tuple[Tag, float]] = []

        for suggestion in result.suggestions:
            if suggestion.relevance < min_relevance:
                continue

            if suggestion.existing_tag_id:
                # Use existing tag
                tag = await self._kg_store.get_tag(suggestion.existing_tag_id)
                if tag:
                    tags_with_relevance.append((tag, suggestion.relevance))
            else:
                # Check for similar existing tags
                similar_tag_id, similarity = await self.check_similar_tags(
                    new_tag_name=suggestion.name,
                    existing_tags=existing_tags,
                    language=language,
                )

                if similar_tag_id and similarity >= 0.8:
                    # Use similar existing tag
                    tag = await self._kg_store.get_tag(similar_tag_id)
                    if tag:
                        # Add the suggested name as an alias if not already there
                        if (suggestion.name.lower() != tag.name.lower() and
                            suggestion.name.lower() not in [a.lower() for a in tag.aliases]):
                            await self._kg_store.update_tag(
                                tag.id,
                                aliases=tag.aliases + [suggestion.name],
                            )
                            tag = await self._kg_store.get_tag(similar_tag_id)  # Refresh
                        if tag:
                            tags_with_relevance.append((tag, suggestion.relevance))
                else:
                    # Create new tag
                    tag = await self._kg_store.create_tag(name=suggestion.name)
                    tags_with_relevance.append((tag, suggestion.relevance))
                    # Add to existing_tags for subsequent similarity checks
                    existing_tags.append(tag)

        return tags_with_relevance

    async def auto_tag_insight(
        self,
        insight_id: str,
        content: str,
        max_tags: int = 3,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag an insight.

        Args:
            insight_id: ID of the insight to tag.
            content: Content to analyze for tags (usually subject + predicate + object).
            max_tags: Maximum number of tags.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples applied to the insight.
        """
        tags_with_relevance = await self.auto_tag_content(
            content=content,
            max_tags=max_tags,
            language=language,
        )

        for tag, relevance in tags_with_relevance:
            await self._kg_store.tag_insight(insight_id, tag.id, relevance)

        logger.info(f"Auto-tagged insight {insight_id} with {len(tags_with_relevance)} tags")
        return tags_with_relevance

    async def auto_tag_document(
        self,
        document_id: str,
        content: str,
        max_tags: int = 5,
        language: str | None = None,
    ) -> list[tuple[Tag, float]]:
        """Automatically tag a document.

        Args:
            document_id: ID of the document to tag.
            content: Content to analyze for tags (usually summary + extracted facts).
            max_tags: Maximum number of tags.
            language: Language for prompts.

        Returns:
            List of (Tag, relevance) tuples applied to the document.
        """
        tags_with_relevance = await self.auto_tag_content(
            content=content,
            max_tags=max_tags,
            language=language,
        )

        for tag, relevance in tags_with_relevance:
            await self._kg_store.tag_document(document_id, tag.id, relevance)

        logger.info(f"Auto-tagged document {document_id} with {len(tags_with_relevance)} tags")
        return tags_with_relevance

    async def auto_tag_insights_batch(
        self,
        insights: list[dict[str, str]],
        max_tags_per_insight: int = 2,
        language: str | None = None,
        existing_tags: list[str] | None = None,
    ) -> dict[str, list[tuple[Tag, float]]]:
        """Automatically tag multiple insights in a single LLM call.

        Args:
            insights: List of dicts with 'id', 'subject', 'predicate', 'object' keys.
            max_tags_per_insight: Maximum tags per insight.
            language: Language for prompts.
            existing_tags: List of existing tag names to prefer for consistency.

        Returns:
            Dict mapping insight_id to list of (Tag, relevance) tuples.
        """
        if not insights:
            return {}

        lang = language or "ja"  # Default to Japanese

        # Build batch prompt
        insights_text = "\n".join([
            f"[{i+1}] {ins['subject']} {ins['predicate']} {ins['object']}"
            for i, ins in enumerate(insights)
        ])

        # Get existing tags for consistency
        existing_tags_text = ""
        if existing_tags:
            existing_tags_text = f"\næ—¢å­˜ã‚¿ã‚°ï¼ˆå„ªå…ˆçš„ã«ä½¿ç”¨ï¼‰: {', '.join(existing_tags[:30])}\n"

        prompt = f"""ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¯ãƒˆã«å¯¾ã—ã¦ã€**æŠ½è±¡çš„ãªã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°**ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¯ãƒˆä¸€è¦§:
{insights_text}
{existing_tags_text}
## ã‚¿ã‚°ä»˜ã‘ã®ãƒ«ãƒ¼ãƒ«

1. **æŠ½è±¡åº¦ã‚’ä¸Šã’ã‚‹**: å…·ä½“çš„ã™ãã‚‹ã‚¿ã‚°ã¯é¿ã‘ã‚‹
   - âŒ æ‚ªã„ä¾‹: ã€ŒçµŒé5æ—¥ã€ã€ŒçµŒé7æ—¥ã€ã€Œå·¦ã‹ã‹ã¨ã€ã€Œå³ã‹ã‹ã¨ã€
   - âœ… è‰¯ã„ä¾‹: ã€ŒçµŒéè¦³å¯Ÿã€ã€Œã‹ã‹ã¨ã€ã€Œè¶³ã€

2. **ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã‚¿ã‚°**: è¤‡æ•°ã®ãƒ•ã‚¡ã‚¯ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§ãã‚‹ã‚‚ã®
   - âŒ æ‚ªã„ä¾‹: ã€Œã‚¹ãƒˆãƒ¬ãƒƒãƒæœªå®Ÿæ–½ã€ã€Œã‚¹ãƒˆãƒ¬ãƒƒãƒå†…å®¹ã€ã€Œã‚¹ãƒˆãƒ¬ãƒƒãƒãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€
   - âœ… è‰¯ã„ä¾‹: ã€Œã‚¹ãƒˆãƒ¬ãƒƒãƒã€ã€Œé‹å‹•ã€ã€Œãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€

3. **ã‚¿ã‚°ã®ç¨®é¡**:
   - ãƒ†ãƒ¼ãƒ: å¥åº·, ä»•äº‹, è¶£å‘³, å®¶è¨ˆ, æ—¥å¸¸
   - æ´»å‹•: é‹å‹•, é£Ÿäº‹, é€šé™¢, å‹‰å¼·
   - çŠ¶æ…‹: ä½“èª¿, æ°—åˆ†, é€²æ—
   - èº«ä½“éƒ¨ä½: è…°, è¶³, è‚© ï¼ˆå·¦å³ã¯åŒºåˆ¥ã—ãªã„ï¼‰

4. **æ—¢å­˜ã‚¿ã‚°ã‚’å„ªå…ˆ**: é¡ä¼¼ã®æ—¢å­˜ã‚¿ã‚°ãŒã‚ã‚Œã°æ–°è¦ä½œæˆã›ãšå†åˆ©ç”¨

## å›ç­”å½¢å¼ï¼ˆJSONé…åˆ—ï¼‰:
[
  {{"index": 1, "tags": [{{"name": "ã‚¿ã‚°å", "relevance": 0.9}}]}},
  ...
]

é–¢é€£åº¦(relevance)ã¯0.7-1.0ã®ç¯„å›²ã§ã€ãã®ã‚¿ã‚°ãŒãƒ•ã‚¡ã‚¯ãƒˆã‚’ã©ã‚Œã ã‘ä»£è¡¨ã™ã‚‹ã‹ã‚’ç¤ºã™ã€‚"""

        try:
            response = await self._llm.chat(
                messages=[{"role": "user", "content": prompt}],
            )

            import json
            import re

            # Parse response
            content = response.content or ""
            content = content.strip()
            # Extract JSON array from response
            match = re.search(r'\[[\s\S]*\]', content)
            if not match:
                logger.warning(f"Failed to find JSON array in LLM response")
                return {}

            results = json.loads(match.group())
            
            # Process results
            tagged_insights: dict[str, list[tuple[Tag, float]]] = {}
            
            for result in results:
                idx = result.get("index", 0) - 1
                if idx < 0 or idx >= len(insights):
                    continue
                    
                insight_id = insights[idx]["id"]
                insight_tags: list[tuple[Tag, float]] = []
                
                for tag_info in result.get("tags", [])[:max_tags_per_insight]:
                    tag_name = tag_info.get("name", "").strip()
                    relevance = min(1.0, max(0.5, float(tag_info.get("relevance", 0.8))))
                    
                    if not tag_name:
                        continue
                    
                    try:
                        # Get or create tag
                        tag = await self._kg_store.get_or_create_tag(tag_name)
                        
                        # Check for similar tags and merge if needed
                        try:
                            similar = await self._kg_store.find_similar_tags(tag_name, threshold=0.85, limit=1)
                            if similar and similar[0][0].id != tag.id:
                                tag = similar[0][0]
                        except Exception:
                            pass  # Use the created tag if similarity check fails
                        
                        # Tag the insight
                        await self._kg_store.tag_insight(insight_id, tag.id, relevance)
                        insight_tags.append((tag, relevance))
                    except Exception as tag_err:
                        logger.warning(f"Failed to apply tag {tag_name}: {tag_err}")
                
                if insight_tags:
                    tagged_insights[insight_id] = insight_tags
            
            total_tags = sum(len(tags) for tags in tagged_insights.values())
            logger.info(f"Batch-tagged {len(tagged_insights)} insights with {total_tags} total tags")
            return tagged_insights

        except Exception as e:
            logger.error(f"Failed to batch tag insights: {e}")
            return {}
