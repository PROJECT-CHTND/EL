"""EL Agent - LLM-native interview agent with knowledge graph integration."""

from __future__ import annotations

import logging
import uuid
from typing import Any

from el_core.llm.client import LLMClient
from el_core.prompts import get_system_prompt
from el_core.schemas import (
    AgentResponse,
    ConsistencyIssue,
    ConsistencyIssueKind,
    ConversationTurn,
    DateType,
    DocumentChunk,
    DocumentExtractionResult,
    Domain,
    ExtractedFact,
    Insight,
    KnowledgeItem,
    Session,
    SessionSummary,
    TopicSummary,
)
from el_core.stores.kg_store import KnowledgeGraphStore
from el_core.tools import ALL_TOOLS, ToolExecutor

logger = logging.getLogger(__name__)


def detect_language(text: str) -> str:
    """Detect if text is Japanese or English.

    Args:
        text: Text to analyze.

    Returns:
        "Japanese" or "English".
    """
    # Check for Japanese characters
    for char in text:
        if (
            "\u3040" <= char <= "\u309f"  # Hiragana
            or "\u30a0" <= char <= "\u30ff"  # Katakana
            or "\u4e00" <= char <= "\u9faf"  # Kanji
        ):
            return "Japanese"
    return "English"


def extract_dates_from_text(text: str) -> tuple[Any, Any]:
    """Extract date references from text for search filtering.

    Supports:
    - Japanese relative dates: ä»Šæ—¥, æ˜¨æ—¥, ä¸€æ˜¨æ—¥, å…ˆé€±, ä»Šé€±, etc.
    - English relative dates: today, yesterday, last week, etc.
    - Japanese date formats: 2024å¹´5æœˆ1æ—¥, 5æœˆ1æ—¥, 5/1, 5æœˆ
    - English date formats: May 1, 2024, May 1st, May 2024
    - ISO format: 2024-05-01

    Args:
        text: Text to extract dates from.

    Returns:
        Tuple of (start_date, end_date) as datetime objects, or (None, None) if no dates found.
    """
    import re
    from datetime import datetime, timedelta

    now = datetime.now()
    current_year = now.year
    today = now.replace(hour=0, minute=0, second=0, microsecond=0)

    # ===== RELATIVE DATES (Japanese) =====
    # Check for relative date references first (highest priority)
    text_lower = text.lower()
    
    # ä¸€æ˜¨æ—¥ (day before yesterday)
    if "ä¸€æ˜¨æ—¥" in text or "ãŠã¨ã¨ã„" in text:
        target = today - timedelta(days=2)
        return target, target
    
    # æ˜¨æ—¥ (yesterday)
    if "æ˜¨æ—¥" in text or "ãã®ã†" in text:
        target = today - timedelta(days=1)
        return target, target
    
    # ä»Šæ—¥ (today)
    if "ä»Šæ—¥" in text or "ãã‚‡ã†" in text or "æœ¬æ—¥" in text:
        return today, today
    
    # æ˜æ—¥ (tomorrow)
    if "æ˜æ—¥" in text or "ã‚ã—ãŸ" in text or "ã‚ã™" in text:
        target = today + timedelta(days=1)
        return target, target
    
    # æ˜å¾Œæ—¥ (day after tomorrow)
    if "æ˜å¾Œæ—¥" in text or "ã‚ã•ã£ã¦" in text:
        target = today + timedelta(days=2)
        return target, target
    
    # å…ˆé€± (last week)
    if "å…ˆé€±" in text:
        # Last week: Monday to Sunday of the previous week
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    # ä»Šé€± (this week)
    if "ä»Šé€±" in text:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    # å…ˆæœˆ (last month)
    if "å…ˆæœˆ" in text:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # ä»Šæœˆ (this month)
    if "ä»Šæœˆ" in text:
        first_of_this_month = today.replace(day=1)
        if today.month == 12:
            last_of_this_month = today.replace(year=today.year + 1, month=1, day=1) - timedelta(days=1)
        else:
            last_of_this_month = today.replace(month=today.month + 1, day=1) - timedelta(days=1)
        return first_of_this_month, last_of_this_month
    
    # Næ—¥å‰ (N days ago)
    n_days_ago_match = re.search(r"(\d+)æ—¥å‰", text)
    if n_days_ago_match:
        days = int(n_days_ago_match.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # Né€±é–“å‰ (N weeks ago)
    n_weeks_ago_match = re.search(r"(\d+)é€±é–“å‰", text)
    if n_weeks_ago_match:
        weeks = int(n_weeks_ago_match.group(1))
        target = today - timedelta(weeks=weeks)
        return target, target
    
    # ===== RELATIVE DATES (English) =====
    if "day before yesterday" in text_lower:
        target = today - timedelta(days=2)
        return target, target
    
    if "yesterday" in text_lower:
        target = today - timedelta(days=1)
        return target, target
    
    if "today" in text_lower:
        return today, today
    
    if "tomorrow" in text_lower:
        target = today + timedelta(days=1)
        return target, target
    
    if "day after tomorrow" in text_lower:
        target = today + timedelta(days=2)
        return target, target
    
    if "last week" in text_lower:
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)
        return last_monday, last_sunday
    
    if "this week" in text_lower:
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        this_sunday = this_monday + timedelta(days=6)
        return this_monday, this_sunday
    
    if "last month" in text_lower:
        first_of_this_month = today.replace(day=1)
        last_of_last_month = first_of_this_month - timedelta(days=1)
        first_of_last_month = last_of_last_month.replace(day=1)
        return first_of_last_month, last_of_last_month
    
    # N days ago
    n_days_ago_en = re.search(r"(\d+)\s*days?\s*ago", text_lower)
    if n_days_ago_en:
        days = int(n_days_ago_en.group(1))
        target = today - timedelta(days=days)
        return target, target
    
    # ===== ABSOLUTE DATES =====
    # Japanese date patterns
    jp_full_date = re.search(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    jp_month_day = re.search(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    jp_month_only = re.search(r"(\d{1,2})æœˆ(?!æ—¥)", text)

    # English date patterns
    en_full_date = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?,?\s*(\d{4})",
        text,
        re.IGNORECASE,
    )
    en_month_day = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+(\d{1,2})(?:st|nd|rd|th)?",
        text,
        re.IGNORECASE,
    )
    en_month_only = re.search(
        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*(\d{4})?",
        text,
        re.IGNORECASE,
    )

    # ISO format
    iso_date = re.search(r"(\d{4})-(\d{2})-(\d{2})", text)

    month_map = {
        "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
        "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12,
    }

    try:
        # Try Japanese full date first
        if jp_full_date:
            year, month, day = int(jp_full_date.group(1)), int(jp_full_date.group(2)), int(jp_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # ISO format
        if iso_date:
            year, month, day = int(iso_date.group(1)), int(iso_date.group(2)), int(iso_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # English full date
        if en_full_date:
            month = month_map[en_full_date.group(1).lower()[:3]]
            day = int(en_full_date.group(2))
            year = int(en_full_date.group(3))
            date = datetime(year, month, day)
            return date, date

        # Japanese month + day
        if jp_month_day:
            month, day = int(jp_month_day.group(1)), int(jp_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # English month + day
        if en_month_day:
            month = month_map[en_month_day.group(1).lower()[:3]]
            day = int(en_month_day.group(2))
            date = datetime(current_year, month, day)
            return date, date

        # Japanese month only - return range for the whole month
        if jp_month_only:
            month = int(jp_month_only.group(1))
            start_date = datetime(current_year, month, 1)
            # End of month
            if month == 12:
                end_date = datetime(current_year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(current_year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

        # English month only
        if en_month_only:
            month = month_map[en_month_only.group(1).lower()[:3]]
            year = int(en_month_only.group(2)) if en_month_only.group(2) else current_year
            start_date = datetime(year, month, 1)
            if month == 12:
                end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = datetime(year, month + 1, 1) - timedelta(days=1)
            return start_date, end_date

    except (ValueError, AttributeError):
        pass

    return None, None


class ELAgent:
    """EL Agent - A curious and empathetic interviewer powered by LLM.

    This agent uses GPT-5.2 with function calling to:
    - Search knowledge graph for relevant context
    - Save important insights from conversations
    - Dynamically detect conversation domains
    - Generate empathetic, insightful questions
    """

    def __init__(
        self,
        llm_client: LLMClient | None = None,
        kg_store: KnowledgeGraphStore | None = None,
    ) -> None:
        """Initialize the EL Agent.

        Args:
            llm_client: LLM client for chat completions. Creates default if None.
            kg_store: Knowledge graph store. Tools will work in local-only mode if None.
        """
        self._llm = llm_client or LLMClient()
        self._kg_store = kg_store
        self._sessions: dict[str, Session] = {}

    async def start_session(
        self,
        user_id: str,
        topic: str,
        language: str | None = None,
    ) -> tuple[str, str]:
        """Start a new conversation session.

        Args:
            user_id: User identifier.
            topic: Conversation topic.
            language: Language preference. Auto-detected if None.

        Returns:
            Tuple of (session_id, opening_message).
        """
        detected_lang = language or detect_language(topic)
        session_id = str(uuid.uuid4())

        session = Session(
            id=session_id,
            user_id=user_id,
            topic=topic,
            language=detected_lang,
        )
        
        # Pre-fetch related knowledge from KG
        if self._kg_store:
            try:
                related_items = await self._kg_store.search(topic, limit=5)
                if related_items:
                    session.prior_knowledge = related_items
                    session.prior_context = self._format_prior_knowledge(related_items, detected_lang)
                    logger.info(f"Found {len(related_items)} related knowledge items for topic: {topic}")
            except Exception as e:
                logger.warning(f"Failed to pre-fetch knowledge: {e}")
        
        self._sessions[session_id] = session

        # Save session metadata to Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to save session metadata: {e}")

        # Generate natural opening response using LLM
        opening = await self._generate_opening_response(session, topic, detected_lang)
        logger.info(f"Started session {session_id} for user {user_id} on topic: {topic}")

        return session_id, opening
    
    async def _save_session_metadata(self, session: Session) -> None:
        """Save session metadata to Neo4j.
        
        Args:
            session: Session to save metadata for.
        """
        if self._kg_store is None:
            return
        
        await self._kg_store.save_session_metadata(
            session_id=session.id,
            user_id=session.user_id,
            topic=session.topic,
            domain=session.domain,
            turn_count=len(session.turns),
            insights_count=session.insights_count,
            created_at=session.created_at,
            updated_at=session.updated_at,
        )
    
    async def _generate_opening_response(
        self,
        session: Session,
        user_input: str,
        language: str,
    ) -> str:
        """Generate a natural opening response using LLM.
        
        Instead of using a template, we let the LLM respond naturally to
        whatever the user typed - whether it's a topic, a request, or a question.
        
        Args:
            session: The session object.
            user_input: What the user typed to start the conversation.
            language: Detected language.
            
        Returns:
            Natural opening response from the LLM.
        """
        # Use a simplified prompt for opening - no tool instructions
        if language.lower() in ("english", "en"):
            system_content = (
                "You are EL, a curious and empathetic interviewer. "
                "Your role is to deeply understand what people share and help them discover new insights.\n\n"
                "Guidelines:\n"
                "- Show genuine interest and empathy\n"
                "- Keep responses conversational and warm\n"
                "- Ask 1-2 open-ended questions to start the dialogue\n"
                "- Be concise but engaging\n"
                "- Do NOT output any JSON, tool calls, or code\n"
            )
        else:
            system_content = (
                "ã‚ãªãŸã¯ã€ŒELã€ã§ã™ã€‚å¥½å¥‡å¿ƒæ—ºç››ã§å…±æ„Ÿæ€§ã®é«˜ã„ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¯ãƒ¼ã¨ã—ã¦ã€"
                "ç›¸æ‰‹ã®è©±ã‚’æ·±ãç†è§£ã—ã€æ–°ãŸãªæ°—ã¥ãã‚’å¼•ãå‡ºã™å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚\n\n"
                "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼š\n"
                "- å…±æ„Ÿã¨é–¢å¿ƒã‚’æŒã£ã¦å¿œç­”ã™ã‚‹\n"
                "- è‡ªç„¶ã§æ¸©ã‹ã„ä¼šè©±èª¿ã‚’å¿ƒãŒã‘ã‚‹\n"
                "- ä¼šè©±ã‚’å§‹ã‚ã‚‹ãŸã‚ã«1ã€œ2å€‹ã®è³ªå•ã‚’ã™ã‚‹\n"
                "- ç°¡æ½”ã ã‘ã©é­…åŠ›çš„ã«\n"
                "- JSONã‚„ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ã€ã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„\n"
            )
        
        # Add prior context if available
        if session.prior_context:
            system_content += session.prior_context
        
        # Add instruction for first response
        if language.lower() in ("english", "en"):
            system_content += (
                "\n\n### First Response Instructions\n"
                "This is the start of a new conversation. The user has just shared "
                "what they want to discuss. Respond naturally and warmly - acknowledge "
                "their input, show interest, and ask a relevant follow-up question to "
                "get the conversation started. Keep it conversational and friendly. "
                "Output ONLY natural conversational text."
            )
        else:
            system_content += (
                "\n\n### æœ€åˆã®å¿œç­”ã«é–¢ã™ã‚‹æŒ‡ç¤º\n"
                "ã“ã‚Œã¯æ–°ã—ã„ä¼šè©±ã®é–‹å§‹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—ãŸã„ã“ã¨ã‚’ä¼ãˆã¦ãã¾ã—ãŸã€‚"
                "è‡ªç„¶ã§æ¸©ã‹ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å—ã‘æ­¢ã‚ã€é–¢å¿ƒã‚’ç¤ºã—ã€"
                "ä¼šè©±ã‚’å§‹ã‚ã‚‹ãŸã‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚"
                "ä¼šè©±èª¿ã§ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã«ã€ã‹ã¤ç°¡æ½”ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
                "è‡ªç„¶ãªä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            )
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input},
        ]
        
        try:
            # Simple chat completion without tools for the opening
            response = await self._llm.chat(messages=messages)  # type: ignore
            return response.content or ""
        except Exception as e:
            logger.warning(f"Failed to generate opening response via LLM: {e}")
            # Fallback to a simple acknowledgment
            if language.lower() in ("english", "en"):
                return "I'd be happy to discuss that with you. What would you like to start with?"
            return "æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ã©ã®ã‚ˆã†ãªã“ã¨ã‹ã‚‰ãŠè©±ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"
    
    def _format_prior_knowledge(self, items: list[KnowledgeItem], language: str) -> str:
        """Format knowledge items into context string.
        
        Args:
            items: List of knowledge items.
            language: Language for formatting.
            
        Returns:
            Formatted context string.
        """
        if not items:
            return ""
        
        if language.lower() in ("english", "en"):
            header = "\n\n### Related Past Knowledge\n\nThe following are relevant insights from previous conversations:\n\n"
            item_format = "- {subject} {predicate}: {object} (recorded on {date})\n"
        else:
            header = "\n\n### é–¢é€£ã™ã‚‹éå»ã®çŸ¥è­˜\n\nä»¥ä¸‹ã¯éå»ã®å¯¾è©±ã§å¾—ã‚‰ã‚ŒãŸé–¢é€£æƒ…å ±ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’è¸ã¾ãˆã¦ä¼šè©±ã—ã¦ãã ã•ã„ï¼š\n\n"
            item_format = "- {subject}ã¯{predicate}ï¼š{object}ï¼ˆ{date}ã«è¨˜éŒ²ï¼‰\n"
        
        context = header
        for item in items:
            date_str = item.created_at.strftime("%Yå¹´%mæœˆ%dæ—¥") if language.lower() not in ("english", "en") else item.created_at.strftime("%Y-%m-%d")
            context += item_format.format(
                subject=item.subject,
                predicate=item.predicate,
                object=item.object,
                date=date_str,
            )
        
        return context

    async def resume_session(
        self,
        session_id: str,
        user_id: str,
    ) -> tuple[str, str, list[KnowledgeItem]]:
        """Resume an existing session from Neo4j.

        Args:
            session_id: Session ID to resume.
            user_id: User ID (for verification).

        Returns:
            Tuple of (session_id, resume_message, prior_insights).

        Raises:
            ValueError: If session not found or user mismatch.
        """
        # First check if session is already in memory (with conversation history)
        existing_session = self._sessions.get(session_id)
        if existing_session is not None and existing_session.user_id == user_id:
            # Session is already loaded with conversation history
            logger.info(f"Resuming session {session_id} from memory ({len(existing_session.turns)} turns)")
            
            # Generate resume message
            lang = existing_session.language
            if lang.lower() in ("english", "en"):
                resume_msg = (
                    f"Welcome back! We were discussing \"{existing_session.topic}\". "
                    f"You have {len(existing_session.turns)} exchanges so far. "
                    f"Let's continue!"
                )
            else:
                resume_msg = (
                    f"ã€Œ{existing_session.topic}ã€ã®ç¶šãã§ã™ã­ã€‚\n"
                    f"ã“ã‚Œã¾ã§{len(existing_session.turns)}ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ãŒã‚ã‚Šã¾ã—ãŸã€‚\n"
                    f"ç¶šã‘ã¾ã—ã‚‡ã†ï¼"
                )
            
            return session_id, resume_msg, existing_session.prior_knowledge
        
        # Session not in memory, load from Neo4j
        if self._kg_store is None:
            raise ValueError("Knowledge graph store not available")

        # Get session metadata from Neo4j
        metadata = await self._kg_store.get_session_metadata(session_id)
        if metadata is None:
            raise ValueError(f"Session {session_id} not found")
        
        if metadata.user_id != user_id:
            raise ValueError("User ID mismatch")

        # Get insights saved during this session
        session_insights = await self._kg_store.get_session_insights(session_id)

        # Detect language from topic
        detected_lang = detect_language(metadata.topic)

        # Create new session object with prior context
        session = Session(
            id=metadata.id,
            user_id=metadata.user_id,
            topic=metadata.topic,
            language=detected_lang,
            domain=metadata.domain,
            created_at=metadata.created_at,
            updated_at=metadata.updated_at,
        )

        # Set prior context from session insights
        if session_insights:
            session.prior_knowledge = session_insights
            session.prior_context = self._format_prior_knowledge(session_insights, detected_lang)

        # Load conversation history from Neo4j
        conversation_turns = await self._kg_store.get_conversation_history(session_id)
        for turn_data in conversation_turns:
            # Restore turn to session
            turn = ConversationTurn(
                user_message=turn_data["user_message"],
                assistant_response=turn_data["assistant_response"],
                timestamp=turn_data["timestamp"],
            )
            session.turns.append(turn)
            # Also restore to message_history for LLM context
            session.message_history.append({"role": "user", "content": turn_data["user_message"]})
            session.message_history.append({"role": "assistant", "content": turn_data["assistant_response"]})

        # Store in active sessions
        self._sessions[session_id] = session

        # Generate resume message
        has_history = len(conversation_turns) > 0
        if detected_lang.lower() in ("english", "en"):
            if has_history:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"Your previous {len(conversation_turns)} exchanges have been restored. "
                    f"Let's continue!"
                )
            else:
                resume_msg = (
                    f"Welcome back! We were discussing \"{metadata.topic}\". "
                    f"(Note: No conversation history found. Starting fresh but with your insights!)"
                )
        else:
            if has_history:
                resume_msg = (
                    f"ã€Œ{metadata.topic}ã€ã®ç¶šãã§ã™ã­ã€‚ãŠå¸°ã‚Šãªã•ã„ï¼\n"
                    f"å‰å›ã®{len(conversation_turns)}ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚\n"
                    f"ç¶šã‘ã¾ã—ã‚‡ã†ï¼"
                )
            else:
                resume_msg = (
                    f"ã€Œ{metadata.topic}ã€ã®ç¶šãã§ã™ã­ã€‚ãŠå¸°ã‚Šãªã•ã„ï¼\n"
                    f"ï¼ˆâ€»ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€è¨˜éŒ²ã—ãŸäº‹å®Ÿã¯å¼•ãç¶™ã„ã§ã„ã¾ã™ï¼‰"
                )

        logger.info(f"Resumed session {session_id} for user {user_id} ({len(conversation_turns)} turns restored)")

        return session_id, resume_msg, session_insights

    def get_session(self, session_id: str) -> Session | None:
        """Get a session by ID.

        Args:
            session_id: Session identifier.

        Returns:
            Session or None if not found.
        """
        return self._sessions.get(session_id)

    async def end_session(self, session_id: str, generate_summary: bool = True) -> tuple[Session | None, SessionSummary | None]:
        """End a session and optionally generate a summary.

        Args:
            session_id: Session identifier.
            generate_summary: Whether to generate and save a summary.

        Returns:
            Tuple of (ended session, generated summary or None).
        """
        session = self._sessions.pop(session_id, None)
        if session is None:
            return None, None

        summary = None
        if generate_summary and len(session.turns) > 0:
            # Generate summary
            summary = await self._generate_session_summary(session)
            
            # Save summary to KG
            if self._kg_store and summary:
                try:
                    await self._kg_store.save_session_summary(summary)
                    logger.info(f"Saved summary for session {session_id}")
                except Exception as e:
                    logger.warning(f"Failed to save session summary: {e}")

        # Update session status in KG
        if self._kg_store:
            try:
                await self._kg_store.update_session_status(session_id, "ended")
            except Exception as e:
                logger.warning(f"Failed to update session status: {e}")

        logger.info(f"Ended session {session_id}")
        return session, summary

    async def _generate_session_summary(self, session: Session) -> SessionSummary | None:
        """Generate a summary of the session using LLM.

        Args:
            session: The session to summarize.

        Returns:
            SessionSummary or None if generation fails.
        """
        if not session.turns:
            return None

        # Build conversation text
        conversation_text = ""
        for i, turn in enumerate(session.turns, 1):
            conversation_text += f"ã‚¿ãƒ¼ãƒ³{i}:\n"
            conversation_text += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {turn.user_message}\n"
            conversation_text += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {turn.assistant_response}\n\n"

        # Collect all insights from the session
        all_insights = []
        for turn in session.turns:
            for insight in turn.insights_saved:
                all_insights.append(f"{insight.subject} - {insight.predicate} - {insight.object}")

        insights_text = "\n".join(all_insights) if all_insights else "ãªã—"

        if session.language.lower() in ("english", "en"):
            system_prompt = """You are a conversation summarizer. Summarize the following conversation.

Output ONLY valid JSON with these fields:
- content: 2-3 sentence natural summary
- key_points: Array of important facts (max 5)
- topics: Array of main topics discussed
- entities_mentioned: Array of people, projects, places mentioned

Example:
{"content":"Discussed project A progress and confirmed the deadline.","key_points":["Deadline is Feb 28","Tanaka is responsible"],"topics":["Project A","Progress report"],"entities_mentioned":["Tanaka","Project A"]}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ä¼šè©±è¦ç´„è€…ã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- content: 2-3æ–‡ã®è‡ªç„¶ãªè¦ç´„
- key_points: é‡è¦ãªäº‹å®Ÿã®é…åˆ—ï¼ˆæœ€å¤§5ã¤ï¼‰
- topics: è©±ã—åˆã‚ã‚ŒãŸä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã®é…åˆ—
- entities_mentioned: è¨€åŠã•ã‚ŒãŸäººåã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€å ´æ‰€ãªã©ã®é…åˆ—

ä¾‹:
{"content":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã®é€²æ—ã«ã¤ã„ã¦è©±ã—åˆã„ã€ç· ã‚åˆ‡ã‚Šã‚’ç¢ºèªã—ãŸã€‚","key_points":["ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","ç”°ä¸­ã•ã‚“ãŒæ‹…å½“"],"topics":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","é€²æ—å ±å‘Š"],"entities_mentioned":["ç”°ä¸­ã•ã‚“","ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA"]}"""

        user_prompt = f"""ãƒˆãƒ”ãƒƒã‚¯: {session.topic}
ãƒ‰ãƒ¡ã‚¤ãƒ³: {session.domain.value}

ã€ä¼šè©±ã€‘
{conversation_text}

ã€æŠ½å‡ºã•ã‚ŒãŸæ´å¯Ÿã€‘
{insights_text}

ä¸Šè¨˜ã®ä¼šè©±ã‚’JSONå½¢å¼ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "{}"
            
            # Clean up response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            data = json.loads(content)
            
            summary = SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=data.get("content", "ä¼šè©±ã®è¦ç´„"),
                key_points=data.get("key_points", []),
                topics=data.get("topics", [session.topic]),
                entities_mentioned=data.get("entities_mentioned", []),
                turn_range=(1, len(session.turns)),
            )
            
            logger.info(f"Generated summary for session {session.id}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate session summary: {e}")
            # Return a basic summary on failure
            return SessionSummary(
                id=str(uuid.uuid4()),
                session_id=session.id,
                content=f"{session.topic}ã«ã¤ã„ã¦ã®ä¼šè©±ï¼ˆ{len(session.turns)}ã‚¿ãƒ¼ãƒ³ï¼‰",
                key_points=[],
                topics=[session.topic],
                entities_mentioned=[],
                turn_range=(1, len(session.turns)),
            )

    async def respond(
        self,
        session_id: str,
        user_message: str,
    ) -> AgentResponse:
        """Generate a response to a user message.

        Args:
            session_id: Session identifier.
            user_message: The user's message.

        Returns:
            AgentResponse with the assistant's message and metadata.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        # Pre-search knowledge graph for potential consistency issues
        pre_search_knowledge: list[KnowledgeItem] = []
        consistency_issues: list[ConsistencyIssue] = []
        consistency_context = ""
        chunk_context = ""  # Phase 2: Context from document chunks
        relevant_chunks: list[DocumentChunk] = []
        
        if self._kg_store:
            try:
                # Extract dates from user message for temporal filtering
                start_date, end_date = extract_dates_from_text(user_message)
                
                if start_date or end_date:
                    # User mentioned a date - search by date range first
                    logger.info(f"Detected date reference: {start_date} to {end_date}")
                    
                    # Phase 2: Search for document chunks by date (for accurate original content)
                    if start_date and end_date and start_date == end_date:
                        # Exact date match
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    elif start_date and end_date:
                        # Date range
                        relevant_chunks = await self._kg_store.get_chunks_by_date_range(start_date, end_date)
                    elif start_date:
                        relevant_chunks = await self._kg_store.get_chunks_by_date(start_date)
                    
                    if relevant_chunks:
                        # Format chunk content as context for LLM
                        chunk_context = self._format_chunk_context(relevant_chunks, session.language)
                        logger.info(f"Found {len(relevant_chunks)} relevant chunks for date query")
                        # IMPORTANT: When we have chunk original content, skip fact search
                        # to avoid LLM mixing accurate chunk content with potentially inaccurate extracted facts
                        logger.info("Skipping fact search - using chunk original content as authoritative source")
                    else:
                        # No chunks found, fall back to extracted facts
                        date_filtered_knowledge = await self._kg_store.search_by_date_range(
                            start_date=start_date,
                            end_date=end_date,
                            query=user_message,
                            limit=5,
                        )
                        pre_search_knowledge.extend(date_filtered_knowledge)
                
                # Only do keyword search if we don't have chunk content
                # Chunk original content is the authoritative source
                if not relevant_chunks:
                    keyword_knowledge = await self._kg_store.search(
                        user_message,
                        limit=5,
                        start_date=start_date,
                        end_date=end_date,
                    )
                    
                    # Merge results, avoiding duplicates
                    seen_ids = {k.id for k in pre_search_knowledge}
                    for item in keyword_knowledge:
                        if item.id not in seen_ids:
                            pre_search_knowledge.append(item)
                            seen_ids.add(item.id)
                
                if pre_search_knowledge:
                    # Check for consistency issues BEFORE generating response
                    consistency_issues = await self._detect_consistency_issues(
                        user_message=user_message,
                        knowledge_used=pre_search_knowledge,
                        language=session.language,
                    )
                    
                    # If issues found, add context for LLM to address them
                    if consistency_issues:
                        consistency_context = self._format_consistency_context(
                            consistency_issues, session.language
                        )
                        logger.info(f"Found {len(consistency_issues)} consistency issues to address")
                        
            except Exception as e:
                logger.warning(f"Pre-search for consistency failed: {e}")

        # Build message history with prior context
        system_content = get_system_prompt(session.language)
        if session.prior_context:
            system_content += session.prior_context
        
        # Phase 2: Add chunk content as context (for accurate document reference)
        if chunk_context:
            system_content += chunk_context
        
        # Add consistency context if there are issues to address
        if consistency_context:
            system_content += consistency_context
        
        messages: list[dict[str, Any]] = [
            {"role": "system", "content": system_content},
        ]

        # Add conversation history
        messages.extend(session.message_history)

        # Add current user message
        messages.append({"role": "user", "content": user_message})

        # Set up tool executor with session_id for insight tracking
        tool_executor = ToolExecutor(self._kg_store, session_id=session_id)
        
        # Add pre-searched knowledge to tool executor so it's included in response
        tool_executor.used_knowledge.extend(pre_search_knowledge)

        # Call LLM with tools
        response_text, tool_results = await self._llm.chat_with_tools(
            messages=messages,  # type: ignore
            tools=ALL_TOOLS,
            tool_handlers=tool_executor.get_tool_handlers(),
        )

        # Detect domain from response and tool usage
        detected_domain = self._detect_domain(user_message, tool_results)

        # Create conversation turn
        turn = ConversationTurn(
            user_message=user_message,
            assistant_response=response_text,
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
        )
        session.add_turn(turn)

        # Update session metadata in Neo4j
        if self._kg_store:
            try:
                await self._save_session_metadata(session)
            except Exception as e:
                logger.warning(f"Failed to update session metadata: {e}")

        # Save conversation turn to Neo4j for persistence
        if self._kg_store:
            try:
                await self._kg_store.save_conversation_turn(
                    session_id=session_id,
                    turn_index=len(session.turns) - 1,
                    user_message=user_message,
                    assistant_response=response_text,
                    timestamp=turn.timestamp,
                )
            except Exception as e:
                logger.warning(f"Failed to save conversation turn: {e}")

        # Build tool calls list for response
        tool_calls = [
            {
                "id": tr.get("tool_call_id", ""),
                "name": tr.get("name", ""),
                "arguments": tr.get("arguments", {}),
            }
            for tr in tool_results
        ]

        logger.info(
            f"Session {session_id}: processed message, "
            f"saved {len(tool_executor.saved_insights)} insights, "
            f"used {len(tool_executor.used_knowledge)} knowledge items, "
            f"detected {len(consistency_issues)} consistency issues"
        )

        return AgentResponse(
            message=response_text,
            tool_calls=tool_calls,  # type: ignore
            insights_saved=tool_executor.saved_insights,
            knowledge_used=tool_executor.used_knowledge,
            detected_domain=detected_domain,
            consistency_issues=consistency_issues,
        )

    def _format_chunk_context(
        self,
        chunks: list[DocumentChunk],
        language: str,
    ) -> str:
        """Format document chunks as context for LLM.

        Phase 2: Provides the original document content for accurate reference.

        Args:
            chunks: List of relevant document chunks.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not chunks:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### ğŸ”’ AUTHORITATIVE DOCUMENT CONTENT (Must Quote Exactly)\n"
            context += "The following is the EXACT original content from the user's uploaded documents.\n"
            context += "**CRITICAL RULES:**\n"
            context += "1. Quote EXACTLY what is written - do not paraphrase or summarize\n"
            context += "2. If document says 'ã‚«ã‚¨ãƒ« å‰', say 'ã‚«ã‚¨ãƒ« å‰' - NOT 'ã‚«ã‚¨ãƒ«ï¼ˆå‰/å¾Œï¼‰'\n"
            context += "3. Do NOT add information that is not in the document\n"
            context += "4. Do NOT guess or infer - only state what is explicitly written\n\n"
        else:
            context = "\n\n### ğŸ”’ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸæ–‡ï¼ˆæ­£ç¢ºã«å¼•ç”¨ã™ã‚‹ã“ã¨ï¼‰\n"
            context += "ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®**åŸæ–‡ãã®ã¾ã¾**ã§ã™ã€‚\n"
            context += "**çµ¶å¯¾ã«å®ˆã‚‹ã¹ããƒ«ãƒ¼ãƒ«ï¼š**\n"
            context += "1. æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã‚’**ä¸€å­—ä¸€å¥ãã®ã¾ã¾**å¼•ç”¨ã™ã‚‹ã“ã¨\n"
            context += "2. ä¾‹ï¼šãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã€Œã‚«ã‚¨ãƒ« å‰ã€ã¨ã‚ã‚Œã°ã€Œã‚«ã‚¨ãƒ« å‰ã€ã¨å›ç­” - ã€Œã‚«ã‚¨ãƒ«ï¼ˆå‰/å¾Œï¼‰ã€ã¯âŒ\n"
            context += "3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ›¸ã‹ã‚Œã¦ã„ãªã„æƒ…å ±ã‚’è¿½åŠ ã—ãªã„ã“ã¨\n"
            context += "4. æ¨æ¸¬ã‚„è£œå®Œã¯çµ¶å¯¾ã«ã—ãªã„ã“ã¨ - æ˜ç¤ºçš„ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã“ã¨ã®ã¿å›ç­”\n\n"

        for chunk in chunks:
            # Add date header if available
            if chunk.chunk_date:
                date_str = chunk.chunk_date.strftime("%Yå¹´%mæœˆ%dæ—¥") if language.lower() not in ("english", "en") else chunk.chunk_date.strftime("%Y-%m-%d")
                context += f"---\nğŸ“… {date_str}"
                if chunk.heading:
                    context += f" - {chunk.heading}"
                context += "\n\n"
            elif chunk.heading:
                context += f"---\nğŸ“„ {chunk.heading}\n\n"
            else:
                context += "---\n\n"

            # Add the original content (preserved exactly as uploaded)
            context += "ã€åŸæ–‡ã“ã“ã‹ã‚‰ã€‘\n"
            context += chunk.content
            context += "\nã€åŸæ–‡ã“ã“ã¾ã§ã€‘\n\n"

        context += "---\n"
        context += "ä¸Šè¨˜ã®åŸæ–‡ã‹ã‚‰ã€è³ªå•ã«è©²å½“ã™ã‚‹éƒ¨åˆ†ã‚’ãã®ã¾ã¾å¼•ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        
        return context

    def _format_consistency_context(
        self,
        issues: list[ConsistencyIssue],
        language: str,
    ) -> str:
        """Format consistency issues as context for LLM.

        Args:
            issues: List of detected consistency issues.
            language: Session language.

        Returns:
            Formatted context string to append to system prompt.
        """
        if not issues:
            return ""

        if language.lower() in ("english", "en"):
            context = "\n\n### IMPORTANT: Consistency Issues Detected\n"
            context += "The user's current message appears to contradict or change previous information.\n"
            context += "**You MUST address these in your response** - ask for clarification naturally.\n\n"
            
            for issue in issues:
                issue_type = "CONTRADICTION" if issue.kind.value == "contradiction" else "CHANGE"
                context += f"- [{issue_type}] {issue.title}\n"
                context += f"  Previously: \"{issue.previous_text}\"\n"
                context += f"  Now: \"{issue.current_text}\"\n"
                context += f"  Suggested question: \"{issue.suggested_question}\"\n\n"
            
            context += "Address this naturally by asking about the change/contradiction.\n"
        else:
            context = "\n\n### é‡è¦ï¼šæ•´åˆæ€§ã®å•é¡Œã‚’æ¤œå‡º\n"
            context += "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéå»ã®æƒ…å ±ã¨çŸ›ç›¾ã¾ãŸã¯å¤‰æ›´ãŒã‚ã‚Šã¾ã™ã€‚\n"
            context += "**å¿…ãšã“ã®ç‚¹ã«ã¤ã„ã¦ç¢ºèªã—ã¦ãã ã•ã„** - è‡ªç„¶ãªå½¢ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚\n\n"
            
            for issue in issues:
                issue_type = "çŸ›ç›¾" if issue.kind.value == "contradiction" else "å¤‰æ›´"
                context += f"- ã€{issue_type}ã€‘{issue.title}\n"
                context += f"  ä»¥å‰ï¼šã€Œ{issue.previous_text}ã€\n"
                context += f"  ä»Šå›ï¼šã€Œ{issue.current_text}ã€\n"
                context += f"  ç¢ºèªã™ã¹ãè³ªå•ï¼šã€Œ{issue.suggested_question}ã€\n\n"
            
            context += "ã“ã®å¤‰æ›´/çŸ›ç›¾ã«ã¤ã„ã¦è‡ªç„¶ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"

        return context

    def _detect_domain(
        self,
        user_message: str,
        tool_results: list[dict[str, Any]],
    ) -> Domain:
        """Detect the conversation domain.

        Args:
            user_message: The user's message.
            tool_results: Results from tool calls.

        Returns:
            Detected domain.
        """
        text_lower = user_message.lower()

        # Check for domain-specific keywords
        daily_work_keywords = [
            "ã‚¿ã‚¹ã‚¯",
            "task",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "project",
            "æ¥­å‹™",
            "work",
            "ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
            "meeting",
            "é€²æ—",
            "progress",
            "ä»Šæ—¥",
            "today",
            "æ˜æ—¥",
            "tomorrow",
            "ç· ã‚åˆ‡ã‚Š",
            "deadline",
            "pr",
            "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼",
            "ãƒ‡ãƒ—ãƒ­ã‚¤",
            "deploy",
        ]

        recipe_keywords = [
            "æ–™ç†",
            "cook",
            "ãƒ¬ã‚·ãƒ”",
            "recipe",
            "ææ–™",
            "ingredient",
            "èª¿ç†",
            "ç„¼ã",
            "ç…®ã‚‹",
            "ç‚’ã‚ã‚‹",
            "åˆ†é‡",
            "ã‚ªãƒ¼ãƒ–ãƒ³",
            "oven",
            "é‹",
            "ãƒ•ãƒ©ã‚¤ãƒ‘ãƒ³",
        ]

        postmortem_keywords = [
            "éšœå®³",
            "incident",
            "ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ",
            "ãƒ€ã‚¦ãƒ³",
            "down",
            "å¾©æ—§",
            "recover",
            "æ ¹æœ¬åŸå› ",
            "root cause",
            "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
            "timeline",
            "å†ç™ºé˜²æ­¢",
            "ã‚¢ãƒ©ãƒ¼ãƒˆ",
            "alert",
            "ã‚¨ãƒ©ãƒ¼",
            "error",
        ]

        creative_keywords = [
            "ã‚¢ã‚¤ãƒ‡ã‚¢",
            "idea",
            "å‰µä½œ",
            "creative",
            "ãƒ‡ã‚¶ã‚¤ãƒ³",
            "design",
            "ã‚¤ãƒ©ã‚¹ãƒˆ",
            "illustration",
            "ç‰©èª",
            "story",
            "éŸ³æ¥½",
            "music",
            "ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            "inspiration",
        ]

        # Check save_insight tool calls for domain hints
        for result in tool_results:
            if result.get("name") == "save_insight":
                args = result.get("arguments", {})
                if "domain" in args:
                    try:
                        return Domain(args["domain"])
                    except ValueError:
                        pass

        # Check keywords
        if any(kw in text_lower for kw in daily_work_keywords):
            return Domain.DAILY_WORK
        if any(kw in text_lower for kw in recipe_keywords):
            return Domain.RECIPE
        if any(kw in text_lower for kw in postmortem_keywords):
            return Domain.POSTMORTEM
        if any(kw in text_lower for kw in creative_keywords):
            return Domain.CREATIVE

        return Domain.GENERAL

    async def get_session_summary(self, session_id: str) -> dict[str, Any]:
        """Get a summary of the session.

        Args:
            session_id: Session identifier.

        Returns:
            Dict with session summary.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise ValueError(f"Session {session_id} not found")

        all_insights: list[Insight] = []
        all_knowledge: list[KnowledgeItem] = []

        for turn in session.turns:
            all_insights.extend(turn.insights_saved)
            all_knowledge.extend(turn.knowledge_used)

        return {
            "session_id": session.id,
            "topic": session.topic,
            "language": session.language,
            "domain": session.domain.value,
            "turn_count": len(session.turns),
            "insights_saved": len(all_insights),
            "knowledge_used": len(all_knowledge),
            "created_at": session.created_at.isoformat(),
            "updated_at": session.updated_at.isoformat(),
        }

    async def _detect_consistency_issues(
        self,
        user_message: str,
        knowledge_used: list[KnowledgeItem],
        language: str,
    ) -> list[ConsistencyIssue]:
        """Detect potential consistency issues between user message and past knowledge.

        Args:
            user_message: Current user message.
            knowledge_used: Knowledge items retrieved for this turn.
            language: Session language.

        Returns:
            List of detected consistency issues.
        """
        if not knowledge_used:
            return []

        # Build context for LLM to analyze, including fact_id for matching
        knowledge_context = "\n".join([
            f"- [ID:{item.id}] {item.subject}ã¯{item.predicate} ã€Œ{item.object}ã€"
            for item in knowledge_used
        ])
        
        # Create a lookup map for fact_id matching
        knowledge_map = {item.id: item for item in knowledge_used}

        if language.lower() in ("english", "en"):
            system_prompt = """You are a consistency checker. Analyze if the user's current message contradicts or changes any past recorded information.

**CRITICAL: Distinguish between "contradiction" and "change":**
- "contradiction": Impossible to be both true (e.g., "A is responsible" vs "B is responsible for the same task")
- "change": Information has been updated/revised (e.g., deadline moved from date A to date B)

Output ONLY valid JSON array. Each item should have:
- kind: "contradiction" or "change" (use "contradiction" when both cannot be true simultaneously)
- fact_id: The ID from the past record (e.g., "abc-123")
- title: Brief title in 5 words or less
- previous_text: What was recorded before
- current_text: What user says now
- suggested_question: Question to clarify

If no issues found, output empty array: []

Contradiction example:
[{"kind":"contradiction","fact_id":"abc-123","title":"Different person responsible","previous_text":"Tanaka is responsible","current_text":"Yamada is responsible","suggested_question":"Previously Tanaka was mentioned as responsible. Has this changed to Yamada?"}]

Change example:
[{"kind":"change","fact_id":"abc-123","title":"Deadline moved","previous_text":"Deadline is March 31","current_text":"Deadline is February 28","suggested_question":"The deadline was March 31 before. Has it been moved to February 28?"}]"""
        else:
            system_prompt = """ã‚ãªãŸã¯æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéå»ã®è¨˜éŒ²ã¨çŸ›ç›¾ã¾ãŸã¯å¤‰åŒ–ã—ã¦ã„ã‚‹ã‹åˆ†æã—ã¦ãã ã•ã„ã€‚

**é‡è¦ï¼šã€ŒçŸ›ç›¾ã€ã¨ã€Œå¤‰æ›´ã€ã‚’åŒºåˆ¥ã—ã¦ãã ã•ã„ï¼š**
- "contradiction"ï¼ˆçŸ›ç›¾ï¼‰: ä¸¡æ–¹ãŒåŒæ™‚ã«çœŸã§ã‚ã‚‹ã“ã¨ãŒä¸å¯èƒ½ï¼ˆä¾‹ï¼šã€ŒAã•ã‚“ãŒæ‹…å½“ã€vsã€ŒåŒã˜ã‚¿ã‚¹ã‚¯ã‚’Bã•ã‚“ãŒæ‹…å½“ã€ï¼‰
- "change"ï¼ˆå¤‰æ›´ï¼‰: æƒ…å ±ãŒæ›´æ–°ãƒ»ä¿®æ­£ã•ã‚ŒãŸï¼ˆä¾‹ï¼šç· ã‚åˆ‡ã‚ŠãŒæ—¥ä»˜Aã‹ã‚‰æ—¥ä»˜Bã«å¤‰æ›´ï¼‰

æœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„é …ç›®ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¾ã™ï¼š
- kind: "contradiction" ã¾ãŸã¯ "change"ï¼ˆä¸¡æ–¹ãŒåŒæ™‚ã«çœŸã§ã‚ã‚Šå¾—ãªã„å ´åˆã¯ "contradiction"ï¼‰
- fact_id: éå»ã®è¨˜éŒ²ã®IDï¼ˆä¾‹: "abc-123"ï¼‰
- title: 5èªä»¥å†…ã®ç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«
- previous_text: ä»¥å‰è¨˜éŒ²ã•ã‚Œã¦ã„ãŸå†…å®¹
- current_text: ç¾åœ¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€ã£ã¦ã„ã‚‹å†…å®¹
- suggested_question: ç¢ºèªã®ãŸã‚ã®è³ªå•

å•é¡ŒãŒãªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’å‡ºåŠ›: []

çŸ›ç›¾ã®ä¾‹:
[{"kind":"contradiction","fact_id":"abc-123","title":"æ‹…å½“è€…ãŒç•°ãªã‚‹","previous_text":"ç”°ä¸­ã•ã‚“ãŒæ‹…å½“","current_text":"å±±ç”°ã•ã‚“ãŒæ‹…å½“","suggested_question":"ä»¥å‰ã¯ç”°ä¸­ã•ã‚“ãŒæ‹…å½“ã¨ã®ã“ã¨ã§ã—ãŸãŒã€å±±ç”°ã•ã‚“ã«å¤‰æ›´ã«ãªã‚Šã¾ã—ãŸã‹ï¼Ÿ"}]

å¤‰æ›´ã®ä¾‹:
[{"kind":"change","fact_id":"abc-123","title":"ç· ã‚åˆ‡ã‚Šå¤‰æ›´","previous_text":"ç· ã‚åˆ‡ã‚Šã¯3æœˆ31æ—¥","current_text":"ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","suggested_question":"ä»¥å‰ã¯3æœˆ31æ—¥ãŒç· ã‚åˆ‡ã‚Šã§ã—ãŸãŒã€2æœˆ28æ—¥ã«å¤‰æ›´ã«ãªã‚Šã¾ã—ãŸã‹ï¼Ÿ"}]"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"éå»ã®è¨˜éŒ²:\n{knowledge_context}\n\nç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:\n{user_message}"},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            content = response.content or "[]"
            
            # Clean up response (remove markdown code blocks if present)
            content = content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[-1]
            if content.endswith("```"):
                content = content.rsplit("```", 1)[0]
            content = content.strip()
            
            # Parse JSON
            issues_data = json.loads(content)
            if not isinstance(issues_data, list):
                return []

            issues = []
            for item in issues_data:
                try:
                    kind = ConsistencyIssueKind(item.get("kind", "change"))
                    fact_id = item.get("fact_id")
                    
                    # Validate fact_id exists in our knowledge
                    if fact_id and fact_id not in knowledge_map:
                        fact_id = None  # Invalid ID, set to None
                    
                    issues.append(ConsistencyIssue(
                        kind=kind,
                        title=item.get("title", "æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"),
                        fact_id=fact_id,
                        previous_text=item.get("previous_text", ""),
                        previous_source="éå»ã®è¨˜éŒ²",
                        current_text=item.get("current_text", ""),
                        current_source="ç¾åœ¨ã®ä¼šè©±",
                        suggested_question=item.get("suggested_question", ""),
                        confidence=0.7,
                    ))
                except Exception as e:
                    logger.warning(f"Failed to parse consistency issue: {e}")
                    continue

            if issues:
                logger.info(f"Detected {len(issues)} consistency issues")

            return issues

        except Exception as e:
            logger.warning(f"Failed to detect consistency issues: {e}")
            return []

    # ==================== Document Processing ====================

    async def extract_from_document(
        self,
        content: str,
        filename: str,
        language: str = "Japanese",
    ) -> DocumentExtractionResult:
        """Extract structured information from document content using LLM.

        Args:
            content: The parsed text content of the document.
            filename: Original filename for context.
            language: Language for extraction prompts.

        Returns:
            DocumentExtractionResult with summary, facts, topics, entities.
        """
        # Note: Content should already be truncated by caller (process_document_background)
        # This is a safety limit for direct calls - GPT-5.2 can handle 128k+ tokens
        max_chars = 100000
        if len(content) > max_chars:
            # Keep start 20% + end 80% to prioritize recent content
            start_chars = int(max_chars * 0.2)
            end_chars = max_chars - start_chars - 100
            content = (
                content[:start_chars] 
                + f"\n\n[... ä¸­é–“ ç´„{len(content) - start_chars - end_chars:,}æ–‡å­—çœç•¥ ...]\n\n" 
                + content[-end_chars:]
            )

        if language.lower() in ("english", "en"):
            system_prompt = """You are a document analyzer. Extract key information from the document, paying special attention to temporal (date/time) information.

Output ONLY valid JSON with these fields:
- summary: 2-3 sentence summary of the document
- facts: Array of factual statements as objects with:
  - subject: The subject entity
  - predicate: The relationship/attribute
  - object: The value/content
  - source_context: Brief context from document (max 50 chars)
  - event_date: Date when this event occurred (YYYY-MM-DD format, null if unknown)
  - event_date_end: End date for date ranges (YYYY-MM-DD format, null if not a range)
  - date_type: One of "exact", "approximate", "range", "unknown"
- topics: Array of main topics/themes
- entities: Array of mentioned people, organizations, projects, places
- domain: One of "daily_work", "recipe", "postmortem", "creative", "general"

IMPORTANT: Extract ALL date information carefully. For each fact:
- If an exact date is mentioned (e.g., "May 1, 2024"), use date_type: "exact"
- If approximate (e.g., "around May", "early 2024"), use date_type: "approximate"
- If a range (e.g., "May 1-15"), use date_type: "range" and set event_date_end
- If no date context, use date_type: "unknown"

Example:
{"summary":"Project status report showing 80% completion with deadline Feb 28, 2024.","facts":[{"subject":"Project A","predicate":"completion rate","object":"80%","source_context":"According to the report","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"Project A","predicate":"deadline","object":"Feb 28, 2024","source_context":"Confirmed in section 3","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["Project Management","Progress Report"],"entities":["Project A","Tanaka"],"domain":"daily_work"}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æè€…ã§ã™ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ç‰¹ã«æ—¥ä»˜ãƒ»æ™‚é–“æƒ…å ±ã«æ³¨æ„ã‚’æ‰•ã£ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- summary: 2-3æ–‡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ç´„
- facts: äº‹å®Ÿã®é…åˆ—ï¼ˆå„é …ç›®ã¯ä»¥ä¸‹ã®å½¢å¼ï¼‰
  - subject: ä¸»èªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
  - predicate: é–¢ä¿‚ãƒ»å±æ€§
  - object: å€¤ãƒ»å†…å®¹
  - source_context: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®ç°¡æ½”ãªæ–‡è„ˆï¼ˆæœ€å¤§50æ–‡å­—ï¼‰
  - event_date: ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸæ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ã€ä¸æ˜ãªã‚‰nullï¼‰
  - event_date_end: æœŸé–“ã®çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ç¯„å›²ã§ãªã‘ã‚Œã°nullï¼‰
  - date_type: "exact"ï¼ˆæ­£ç¢ºï¼‰, "approximate"ï¼ˆç´„ï¼‰, "range"ï¼ˆæœŸé–“ï¼‰, "unknown"ï¼ˆä¸æ˜ï¼‰ã®ã„ãšã‚Œã‹
- topics: ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ãƒ»ãƒ†ãƒ¼ãƒã®é…åˆ—
- entities: è¨€åŠã•ã‚ŒãŸäººç‰©ã€çµ„ç¹”ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€å ´æ‰€ã®é…åˆ—
- domain: "daily_work", "recipe", "postmortem", "creative", "general" ã®ã„ãšã‚Œã‹

é‡è¦ï¼šã™ã¹ã¦ã®æ—¥ä»˜æƒ…å ±ã‚’æ³¨æ„æ·±ãæŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
- æ­£ç¢ºãªæ—¥ä»˜ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šã€Œ2024å¹´5æœˆ1æ—¥ã€ï¼‰â†’ date_type: "exact"
- æ›–æ˜§ãªæ—¥ä»˜ã®å ´åˆï¼ˆä¾‹ï¼šã€Œ5æœˆé ƒã€ã€Œ2024å¹´åˆã‚ã€ï¼‰â†’ date_type: "approximate"
- æœŸé–“ã®å ´åˆï¼ˆä¾‹ï¼šã€Œ5æœˆ1æ—¥ã€œ15æ—¥ã€ï¼‰â†’ date_type: "range"ã€event_date_endã‚’è¨­å®š
- æ—¥ä»˜ã®æ–‡è„ˆãŒãªã„å ´åˆ â†’ date_type: "unknown"

ä¾‹:
{"summary":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—ãƒ¬ãƒãƒ¼ãƒˆã€‚å®Œäº†ç‡80%ã€ç· ã‚åˆ‡ã‚Šã¯2024å¹´2æœˆ28æ—¥ã€‚","facts":[{"subject":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","predicate":"å®Œäº†ç‡","object":"80%","source_context":"ãƒ¬ãƒãƒ¼ãƒˆã«ã‚ˆã‚‹ã¨","event_date":"2024-01-15","event_date_end":null,"date_type":"exact"},{"subject":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","predicate":"ç· ã‚åˆ‡ã‚Š","object":"2024å¹´2æœˆ28æ—¥","source_context":"ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ç¢ºèª","event_date":"2024-02-28","event_date_end":null,"date_type":"exact"}],"topics":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†","é€²æ—å ±å‘Š"],"entities":["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆA","ç”°ä¸­ã•ã‚“"],"domain":"daily_work"}"""

        user_prompt = f"""ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}

ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã€‘
{content}

ä¸Šè¨˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã€JSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            # Parse facts with date information
            facts = []
            for f in data.get("facts", []):
                if isinstance(f, dict) and "subject" in f and "predicate" in f and "object" in f:
                    # Parse event_date
                    event_date = None
                    if f.get("event_date"):
                        try:
                            from datetime import datetime
                            event_date = datetime.strptime(f["event_date"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse event_date_end
                    event_date_end = None
                    if f.get("event_date_end"):
                        try:
                            from datetime import datetime
                            event_date_end = datetime.strptime(f["event_date_end"], "%Y-%m-%d")
                        except (ValueError, TypeError):
                            pass
                    
                    # Parse date_type
                    date_type_str = f.get("date_type", "unknown")
                    try:
                        date_type = DateType(date_type_str)
                    except ValueError:
                        date_type = DateType.UNKNOWN
                    
                    facts.append(ExtractedFact(
                        subject=f["subject"],
                        predicate=f["predicate"],
                        object=f["object"],
                        source_context=f.get("source_context", ""),
                        event_date=event_date,
                        event_date_end=event_date_end,
                        date_type=date_type,
                    ))

            # Parse domain
            domain_str = data.get("domain", "general")
            try:
                domain = Domain(domain_str)
            except ValueError:
                domain = Domain.GENERAL

            result = DocumentExtractionResult(
                summary=data.get("summary", "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç´„"),
                facts=facts,
                topics=data.get("topics", []),
                entities=data.get("entities", []),
                domain=domain,
            )

            logger.info(f"Extracted {len(facts)} facts from document: {filename}")
            return result

        except Exception as e:
            logger.warning(f"Failed to extract from document: {e}")
            return DocumentExtractionResult(
                summary=f"{filename} ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
                facts=[],
                topics=[],
                entities=[],
                domain=Domain.GENERAL,
            )

    async def generate_topic_summary(
        self,
        topic: str,
        facts: list[KnowledgeItem],
        language: str = "Japanese",
    ) -> TopicSummary:
        """Generate a comprehensive summary for a topic based on accumulated facts.

        Args:
            topic: The topic/entity name.
            facts: List of facts related to the topic.
            language: Language for the summary.

        Returns:
            TopicSummary with comprehensive overview.
        """
        if not facts:
            return TopicSummary(
                topic=topic,
                summary=f"{topic}ã«é–¢ã™ã‚‹è¨˜éŒ²ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚",
                key_points=[],
                related_entities=[],
                fact_count=0,
            )

        # Format facts for LLM
        facts_text = "\n".join([
            f"- {f.subject}ã¯{f.predicate}ã€Œ{f.object}ã€({f.created_at.strftime('%Y-%m-%d')})"
            for f in facts
        ])

        # Extract unique entities
        all_entities = set()
        for f in facts:
            all_entities.add(f.subject)
            all_entities.add(f.object)
        all_entities.discard(topic)

        # Get time range
        dates = [f.created_at for f in facts]
        time_range = f"{min(dates).strftime('%Y-%m-%d')} ï½ {max(dates).strftime('%Y-%m-%d')}"

        if language.lower() in ("english", "en"):
            system_prompt = """You are a knowledge synthesizer. Create a comprehensive summary of the topic based on the facts provided.

Output ONLY valid JSON with these fields:
- summary: 3-5 sentence comprehensive overview
- key_points: Array of the most important points (max 7)
- insights: Array of insights or patterns you noticed
- questions: Array of follow-up questions that might be useful

Example:
{"summary":"Project A has been progressing steadily...","key_points":["Deadline is Feb 28","80% complete","Tanaka leading"],"insights":["Progress accelerated in January","Risk of deadline slip"],"questions":["What are the remaining tasks?","Any blockers?"]}"""
        else:
            system_prompt = """ã‚ãªãŸã¯ãƒŠãƒ¬ãƒƒã‚¸çµ±åˆè€…ã§ã™ã€‚æä¾›ã•ã‚ŒãŸäº‹å®Ÿã«åŸºã¥ã„ã¦ã€ãƒˆãƒ”ãƒƒã‚¯ã®åŒ…æ‹¬çš„ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- summary: 3-5æ–‡ã®åŒ…æ‹¬çš„ãªæ¦‚è¦
- key_points: æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®é…åˆ—ï¼ˆæœ€å¤§7ã¤ï¼‰
- insights: æ°—ã¥ã„ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„æ´å¯Ÿã®é…åˆ—
- questions: æœ‰ç”¨ã¨æ€ã‚ã‚Œã‚‹ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã®é…åˆ—

ä¾‹:
{"summary":"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã¯é †èª¿ã«é€²è¡Œã—ã¦ãŠã‚Š...","key_points":["ç· ã‚åˆ‡ã‚Šã¯2æœˆ28æ—¥","å®Œäº†ç‡80%","ç”°ä¸­ã•ã‚“ãŒãƒªãƒ¼ãƒ‰"],"insights":["1æœˆã«é€²æ—ãŒåŠ é€Ÿ","ç· ã‚åˆ‡ã‚Šé…å»¶ã®ãƒªã‚¹ã‚¯ã‚ã‚Š"],"questions":["æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã¯ä½•ã‹ï¼Ÿ","ãƒ–ãƒ­ãƒƒã‚«ãƒ¼ã¯ã‚ã‚‹ã‹ï¼Ÿ"]}"""

        user_prompt = f"""ãƒˆãƒ”ãƒƒã‚¯: {topic}
é–¢é€£ã™ã‚‹äº‹å®Ÿæ•°: {len(facts)}
æœŸé–“: {time_range}

ã€è¨˜éŒ²ã•ã‚ŒãŸäº‹å®Ÿã€‘
{facts_text}

ä¸Šè¨˜ã®äº‹å®Ÿã‚’çµ±åˆã—ã€{topic}ã«ã¤ã„ã¦ã®åŒ…æ‹¬çš„ãªè¦ç´„ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            import json
            response = await self._llm.chat(messages=messages)  # type: ignore
            result_content = response.content or "{}"

            # Clean up response
            result_content = result_content.strip()
            if result_content.startswith("```"):
                result_content = result_content.split("\n", 1)[-1]
            if result_content.endswith("```"):
                result_content = result_content.rsplit("```", 1)[0]
            result_content = result_content.strip()

            data = json.loads(result_content)

            summary = TopicSummary(
                topic=topic,
                summary=data.get("summary", f"{topic}ã«ã¤ã„ã¦ã®è¦ç´„"),
                key_points=data.get("key_points", []) + data.get("insights", []),
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )

            logger.info(f"Generated summary for topic: {topic}")
            return summary

        except Exception as e:
            logger.warning(f"Failed to generate topic summary: {e}")
            return TopicSummary(
                topic=topic,
                summary=f"{topic}ã«é–¢ã™ã‚‹{len(facts)}ä»¶ã®äº‹å®ŸãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                key_points=[f.object for f in facts[:5]],
                related_entities=list(all_entities)[:10],
                fact_count=len(facts),
                time_range=time_range,
            )
